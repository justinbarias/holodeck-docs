{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HoloDeck - AI Agent Experimentation Platform","text":"<p>HoloDeck is an open-source experimentation platform for building, testing, and deploying AI agents through YAML configuration. Define intelligent agents entirely through configuration\u2014no code required.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>No-Code Agent Definition: Define agents, tools, and evaluations in simple YAML files</li> <li>Multi-Provider Support: OpenAI, Azure OpenAI, Anthropic (add more via MCP)</li> <li>Flexible Tool Integration: Vector stores, custom functions, MCP servers, and AI-powered tools</li> <li>Built-in Testing &amp; Evaluation: Run evaluations with multiple metrics, customize models per metric</li> <li>Production-Ready: Deploy agents as FastAPI endpoints with Docker support</li> <li>Multimodal Test Support: Images, PDFs, Word docs, Excel sheets, and mixed media in test cases</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-install-holodeck","title":"1. Install HoloDeck","text":"<pre><code>pip install holodeck-ai\n</code></pre>"},{"location":"#2-create-a-simple-agent","title":"2. Create a Simple Agent","text":"<p>Create <code>my-agent.yaml</code>:</p> <pre><code>name: \"My First Agent\"\ndescription: \"A helpful AI assistant\"\nmodel:\n  provider: \"openai\"\n  name: \"gpt-4o-mini\"\n  temperature: 0.7\n  max_tokens: 1000\ninstructions:\n  inline: |\n    You are a helpful AI assistant.\n    Answer questions accurately and concisely.\n</code></pre>"},{"location":"#3-load-and-use-the-agent","title":"3. Load and Use the Agent","text":"<pre><code>from holodeck.config.loader import ConfigLoader\n\n# Load agent configuration\nloader = ConfigLoader()\nagent = loader.load_agent_yaml(\"my-agent.yaml\")\n\nprint(f\"Loaded agent: {agent.name}\")\nprint(f\"Model: {agent.model.name}\")\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started - Installation and setup</li> <li>Quickstart Guide - Minimal working example with error handling</li> <li>Agent Configuration - Complete schema reference</li> <li>Tools Guide - All tool types explained with examples</li> <li>Evaluations - Testing and evaluation framework</li> <li>Global Configuration - System-wide settings and precedence rules</li> <li>API Reference - Python API documentation</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>Browse complete examples:</p> <ul> <li><code>basic_agent.yaml</code> - Minimal valid agent</li> <li><code>with_tools.yaml</code> - All tool types</li> <li><code>with_evaluations.yaml</code> - Testing and metrics</li> <li><code>with_global_config.yaml</code> - Configuration precedence</li> </ul>"},{"location":"#features-overview","title":"Features Overview","text":""},{"location":"#define-agents-in-yaml","title":"Define Agents in YAML","text":"<pre><code>name: expert-agent\ndescription: A specialized expert agent with evaluation metrics\nmodel:\n  provider: azure_openai\n  name: gpt-4o\n  temperature: 0.3\n  max_tokens: 1024\n\ninstructions:\n  inline: |\n    You are an expert in your domain.\n    Provide accurate, helpful, and well-reasoned responses.\n\ntest_cases:\n  - name: \"Test Case 1\"\n    input: \"Your question here\"\n    ground_truth: \"Expected answer\"\n    evaluations:\n      - f1_score\n      - bleu\n\nevaluations:\n  model:\n    provider: azure_openai\n    name: gpt-4o\n    temperature: 0.0\n\n  metrics:\n    - metric: f1_score\n      threshold: 0.70\n    - metric: bleu\n      threshold: 0.60\n</code></pre>"},{"location":"#support-multiple-tool-types","title":"Support Multiple Tool Types","text":"<ol> <li>Vector Search - Semantic search over documents and embeddings</li> <li>Functions - Execute custom Python code from files</li> <li>MCP Servers - Standardized integrations (GitHub, filesystem, databases, custom)</li> <li>Prompt Tools - AI-powered semantic functions with templates</li> </ol>"},{"location":"#flexible-model-configuration","title":"Flexible Model Configuration","text":"<p>Configure LLM models at three levels:</p> <ul> <li>Global: Default model for all agents</li> <li>Agent: Override for specific agent</li> <li>Metric: Fine-grained per-evaluation-metric (GPT-4 for critical metrics, GPT-4o-mini for others)</li> </ul>"},{"location":"#multimodal-test-cases","title":"Multimodal Test Cases","text":"<p>Test agents with rich media:</p> <pre><code>test_cases:\n  - input: \"Analyze this image and PDF\"\n    files:\n      - image.png\n      - document.pdf\n    expected_tools: [\"vision_analyzer\"]\n    ground_truth: \"Expected analysis result\"\n</code></pre>"},{"location":"#project-status","title":"Project Status","text":"<p>Version: 0.2.0 (Development)</p> <ul> <li>\u2705 Core configuration schema (Pydantic models)</li> <li>\u2705 YAML parsing and validation</li> <li>\u2705 Environment variable support</li> <li>\u2705 File reference resolution</li> <li>\u2705 CLI interface (holodeck command with init, test, chat, deploy)</li> <li>\u2705 Agent execution engine (LLM provider integration, tool execution, memory)</li> <li>\u2705 Interactive chat with spinner, token tracking, and adaptive status display</li> <li>\u2705 Evaluation framework (AI-powered and NLP metrics with threshold validation)</li> <li>\u23f3 Deployment tools (planned for v0.3)</li> <li>\u23f3 Multi-agent orchestration (planned for v0.3)</li> <li>\u23f3 OpenTelemetry instrumentation (planned for v0.3)</li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub Issues: Report bugs or suggest features</li> <li>Discussions: Ask questions and share ideas</li> <li>Contributing: Read contributing guide to get involved</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - See LICENSE file for details</p> <p>Next Steps:</p> <ul> <li>Get started with installation \u2192</li> <li>Try the quickstart \u2192</li> <li>Explore examples \u2192</li> </ul>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to HoloDeck will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#planned-features","title":"Planned Features","text":"<ul> <li>Deployment Engine: Convert agents to production FastAPI endpoints</li> <li>CLI Commands: <code>holodeck deploy</code></li> <li>Observability: OpenTelemetry integration with GenAI semantic conventions</li> <li>Plugin System: Pre-built plugin packages for common integrations</li> <li>Agent Framework Migration: Plans to migrate the Agents abstraction to Microsoft Agent Framework once it reaches 1.0</li> </ul>"},{"location":"CHANGELOG/#016-2025-11-28","title":"0.1.6 - 2025-11-28","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>MCP Tool Integration: Full Model Context Protocol (MCP) tool support with stdio transport</li> <li>MCP server configuration and connection management</li> <li>Tool discovery and invocation via MCP protocol</li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>Instruction loading issues in agent configuration</li> </ul>"},{"location":"CHANGELOG/#015-2025-11-27","title":"0.1.5 - 2025-11-27","text":""},{"location":"CHANGELOG/#added_1","title":"Added","text":"<ul> <li>Project and User Config Support: Execution config resolution now supports project-level and user-level configuration files</li> </ul>"},{"location":"CHANGELOG/#fixed_1","title":"Fixed","text":"<ul> <li>ChromaDB connection issues</li> </ul>"},{"location":"CHANGELOG/#014-2025-11-27","title":"0.1.4 - 2025-11-27","text":""},{"location":"CHANGELOG/#fixed_2","title":"Fixed","text":"<ul> <li>PyPI release by removing local version identifiers</li> </ul>"},{"location":"CHANGELOG/#013-2025-11-27","title":"0.1.3 - 2025-11-27","text":""},{"location":"CHANGELOG/#added_2","title":"Added","text":"<ul> <li>ChromaDB Support: Explicit ChromaDB vector store integration</li> </ul>"},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Package Manager: Switched from Poetry to uv for faster dependency management</li> </ul>"},{"location":"CHANGELOG/#fixed_3","title":"Fixed","text":"<ul> <li>Test logging improvements</li> <li>RedisVL compatibility issues</li> <li>CLI quiet mode behavior</li> </ul>"},{"location":"CHANGELOG/#012-2025-11-26","title":"0.1.2 - 2025-11-26","text":""},{"location":"CHANGELOG/#added_3","title":"Added","text":"<ul> <li>Ollama Endpoint Support: Local LLM execution via Ollama</li> <li>Vector Stores Setup Guide: Comprehensive Redis vector store documentation</li> <li>Claude Code integration for development assistance</li> </ul>"},{"location":"CHANGELOG/#011-2025-11-25","title":"0.1.1 - 2025-11-25","text":""},{"location":"CHANGELOG/#added_4","title":"Added","text":"<ul> <li>Semantic Kernel Vector Store Abstractions: Support for all vector store providers (Redis, ChromaDB, etc.)</li> <li>Agent config execution settings applied to Semantic Kernel</li> </ul>"},{"location":"CHANGELOG/#010-2025-11-23","title":"0.1.0 - 2025-11-23","text":""},{"location":"CHANGELOG/#added_5","title":"Added","text":"<ul> <li>Chat Models and Validation Pipeline: Scaffold for interactive chat functionality</li> <li>Markdown Report Generation: Comprehensive test result reporting (T123-T127)</li> <li>Progress Display Enhancements: Spinner, ANSI colors, elapsed time display</li> <li>Per-Test Metric Resolution: EvaluationMetric objects for fine-grained metric configuration (T095-T096)</li> <li>File Processing Improvements: Enhanced file input handling</li> </ul>"},{"location":"CHANGELOG/#changed_1","title":"Changed","text":"<ul> <li>Consolidated and refactored tests to parameterized tests for better maintainability</li> <li>Config init command improvements</li> </ul>"},{"location":"CHANGELOG/#0014-2025-11-15","title":"0.0.14 - 2025-11-15","text":""},{"location":"CHANGELOG/#fixed_4","title":"Fixed","text":"<ul> <li>Poetry development dependencies</li> <li>MkDocs build step</li> <li>Poetry version configuration</li> <li>Various Poetry configuration issues</li> </ul>"},{"location":"CHANGELOG/#007-2025-11-08","title":"0.0.7 - 2025-11-08","text":""},{"location":"CHANGELOG/#added_6","title":"Added","text":"<ul> <li>Agent Execution Implementation: Core agent execution engine</li> <li>Evaluators: User Story 1 evaluator implementation</li> <li>Response Format Definition: Phase 4 implementation (T014-T019)</li> <li>Global Settings Configuration: Phase 2 &amp; 3 with TDD approach</li> </ul>"},{"location":"CHANGELOG/#006-2025-10-25","title":"0.0.6 - 2025-10-25","text":""},{"location":"CHANGELOG/#added_7","title":"Added","text":"<ul> <li><code>holodeck init</code> Command: Complete project initialization with templates</li> <li>Phase 8: Polish &amp; QA for init command</li> <li>Phase 7: Project metadata specification (US5)</li> <li>Phase 5: Sample files and examples generation (US3)</li> <li>User Story 2: Project template selection (Phase 4)</li> <li>Core init engine implementation</li> <li>Basic agent creation from templates</li> <li>ConfigLoader returns GlobalConfig rather than dict</li> </ul>"},{"location":"CHANGELOG/#005-2025-10-20","title":"0.0.5 - 2025-10-20","text":""},{"location":"CHANGELOG/#fixed_5","title":"Fixed","text":"<ul> <li>Version tag configuration</li> </ul>"},{"location":"CHANGELOG/#004-2025-10-20","title":"0.0.4 - 2025-10-20","text":""},{"location":"CHANGELOG/#added_8","title":"Added","text":"<ul> <li>GitHub release workflow</li> <li>Automated PyPI publishing</li> </ul>"},{"location":"CHANGELOG/#001-2025-10-19","title":"0.0.1 - 2025-10-19","text":""},{"location":"CHANGELOG/#added-user-story-1-define-agent-configuration","title":"Added - User Story 1: Define Agent Configuration","text":""},{"location":"CHANGELOG/#core-features","title":"Core Features","text":"<ul> <li> <p>Agent Configuration Schema: Complete YAML-based agent configuration with Pydantic validation</p> </li> <li> <p>Agent metadata (name, description)</p> </li> <li>LLM provider configuration (OpenAI, Azure OpenAI, Anthropic)</li> <li>Model parameters (temperature, max_tokens)</li> <li>Instructions (inline or file-based)</li> <li>Tools array with type discrimination</li> <li>Test cases with expected behavior validation</li> <li> <p>Evaluation metrics with flexible model configuration</p> </li> <li> <p>Configuration Loading &amp; Validation (<code>ConfigLoader</code>):</p> </li> <li> <p>Load and parse agent.yaml files</p> </li> <li>Validate against Pydantic schema with user-friendly error messages</li> <li>File path resolution (relative to agent.yaml directory)</li> <li>Environment variable substitution (${VAR_NAME} pattern)</li> <li> <p>Precedence hierarchy: agent.yaml &gt; environment variables &gt; global config</p> </li> <li> <p>Global Configuration Support:</p> </li> <li>Load ~/.holodeck/config.yaml for system-wide settings</li> <li>Provider configurations at global level</li> <li>Tool configurations at global level</li> <li>Configuration merging with proper precedence</li> </ul>"},{"location":"CHANGELOG/#data-models","title":"Data Models","text":"<ul> <li> <p>LLMProvider Model:</p> </li> <li> <p>Multi-provider support (openai, azure_openai, anthropic)</p> </li> <li>Model selection and parameter configuration</li> <li>Temperature range validation (0-2)</li> <li>Max tokens validation (&gt;0)</li> <li> <p>Azure-specific endpoint configuration</p> </li> <li> <p>Tool Models (Discriminated Union):</p> </li> <li> <p>VectorstoreTool: Vector search with source, embedding model, chunk size/overlap</p> </li> <li>FunctionTool: Python function tools with parameters schema</li> <li>MCPTool: Model Context Protocol server integration</li> <li>PromptTool: AI-powered semantic functions with template support</li> <li> <p>Tool type validation and discrimination</p> </li> <li> <p>Evaluation Models:</p> </li> <li> <p>Metric configuration with name, threshold, enabled flag</p> </li> <li>Per-metric model override for flexible configuration</li> <li> <p>AI-powered and NLP metrics support</p> </li> <li> <p>TestCase Model:</p> </li> <li> <p>Test inputs with expected behaviors</p> </li> <li>Ground truth for validation</li> <li>Expected tool usage tracking</li> <li> <p>Evaluation metrics per test</p> </li> <li> <p>Agent Model:</p> </li> <li> <p>Complete agent definition</p> </li> <li>All field validations and constraints</li> <li> <p>Tool and evaluation composition</p> </li> <li> <p>GlobalConfig Model:</p> </li> <li>Provider registry</li> <li>Vectorstore configurations</li> <li>Deployment settings</li> </ul>"},{"location":"CHANGELOG/#error-handling","title":"Error Handling","text":"<ul> <li> <p>Custom Exception Hierarchy:</p> </li> <li> <p><code>HoloDeckError</code>: Base exception</p> </li> <li><code>ConfigError</code>: Configuration-specific errors</li> <li><code>ValidationError</code>: Schema validation errors with field details</li> <li> <p><code>FileNotFoundError</code>: File resolution errors with path suggestions</p> </li> <li> <p>Human-Readable Error Messages:</p> </li> <li>Field names and types in validation errors</li> <li>Actual vs. expected values</li> <li>File paths with suggestions</li> <li>Nested error flattening for complex schemas</li> </ul>"},{"location":"CHANGELOG/#infrastructure-tooling","title":"Infrastructure &amp; Tooling","text":"<ul> <li> <p>Development Setup:</p> </li> <li> <p>Makefile with 30+ development commands</p> </li> <li>Poetry dependency management</li> <li>Pre-commit hooks (black, ruff, mypy, detect-secrets)</li> <li> <p>Python 3.10+ support</p> </li> <li> <p>Testing:</p> </li> <li> <p>Unit test suite with 11 test files covering all models</p> </li> <li>Integration test suite for end-to-end workflows</li> <li>80%+ code coverage requirement</li> <li> <p>Test execution: <code>make test</code>, <code>make test-coverage</code>, <code>make test-parallel</code></p> </li> <li> <p>Code Quality:</p> </li> <li> <p>Black code formatting (88 char line length)</p> </li> <li>Ruff linting (pycodestyle, pyflakes, isort, flake8-bugbear, pyupgrade, pep8-naming, flake8-simplify, bandit)</li> <li>MyPy type checking with strict settings</li> <li>Security scanning (safety, bandit, detect-secrets)</li> <li> <p>Automated pre-commit validation</p> </li> <li> <p>Documentation:</p> </li> <li>MkDocs site configuration with Material theme</li> <li>Getting Started guide (installation, quickstart)</li> <li>Configuration guides (agent config, tools, evaluations, global config, file references)</li> <li>Example agent configurations (basic, with tools, with evaluations, with global config)</li> <li>API reference documentation (ConfigLoader, Pydantic models)</li> <li>Architecture documentation (configuration loading flow)</li> </ul>"},{"location":"CHANGELOG/#features-summary-by-component","title":"Features Summary by Component","text":""},{"location":"CHANGELOG/#configloader-api","title":"ConfigLoader API","text":"<pre><code>loader = ConfigLoader()\nagent = loader.load_agent_yaml(\"agent.yaml\")  # Returns Agent instance\n</code></pre> <ul> <li>Parse YAML to Agent instances</li> <li>Automatic environment variable substitution</li> <li>File reference resolution with validation</li> <li>Configuration precedence handling</li> <li>Comprehensive error reporting</li> </ul>"},{"location":"CHANGELOG/#schema-support","title":"Schema Support","text":"<ul> <li>File References: Instructions and tool definitions can be loaded from files</li> <li>Environment Variables: ${ENV_VAR} patterns supported throughout configs</li> <li>Type Discrimination: Tool types automatically validated and parsed</li> <li>Nested Validation: Complex nested structures validated properly</li> </ul>"},{"location":"CHANGELOG/#testing-coverage","title":"Testing Coverage","text":"<p>Unit Tests (11 files):</p> <ul> <li><code>test_errors.py</code> - Exception handling and messaging</li> <li><code>test_env_loader.py</code> - Environment variable substitution</li> <li><code>test_defaults.py</code> - Default configuration handling</li> <li><code>test_validator.py</code> - Validation utilities</li> <li><code>test_tool_models.py</code> - Tool type validation and discrimination</li> <li><code>test_llm_models.py</code> - LLM provider configuration</li> <li><code>test_evaluation_models.py</code> - Evaluation metric configuration</li> <li><code>test_testcase_models.py</code> - Test case validation</li> <li><code>test_agent_models.py</code> - Agent schema validation</li> <li><code>test_globalconfig_models.py</code> - Global configuration handling</li> <li><code>test_config_loader.py</code> - ConfigLoader functionality</li> </ul> <p>Integration Tests (1 file):</p> <ul> <li><code>test_config_end_to_end.py</code> - Full workflow testing</li> </ul>"},{"location":"CHANGELOG/#known-limitations","title":"Known Limitations","text":""},{"location":"CHANGELOG/#version-001-scope","title":"Version 0.0.1 Scope","text":"<ul> <li>CLI Not Implemented: No command-line interface (planned for User Story 2)</li> <li>No Agent Execution: Agent models are validated but not executed (Phase 2 feature)</li> <li>No Tool Execution: Tools are defined but not executed (Phase 2 feature)</li> <li>No Evaluation Engine: Metrics are configured but not executed (Phase 2 feature)</li> <li>No Deployment: No FastAPI endpoint generation or Docker deployment (Phase 2-3 features)</li> <li>No Observability: OpenTelemetry integration planned for Phase 2</li> <li>No Plugin System: Plugin packages not yet available (Phase 3 feature)</li> </ul>"},{"location":"CHANGELOG/#validation-limitations","title":"Validation Limitations","text":"<ul> <li>File Validation: Only checks file existence, not content validity</li> <li>LLM Provider APIs: No actual API testing (would require credentials)</li> <li>Tool Validation: Type validation only, no runtime validation</li> </ul>"},{"location":"CHANGELOG/#known-issues","title":"Known Issues","text":"<p>None reported in 0.0.1.</p>"},{"location":"CHANGELOG/#how-to-use-this-changelog","title":"How to Use This Changelog","text":"<ul> <li>Unreleased: Features coming in future releases</li> <li>Semantic Versioning: MAJOR.MINOR.PATCH</li> <li>MAJOR: Breaking changes or new architecture</li> <li>MINOR: New features and functionality</li> <li>PATCH: Bug fixes and improvements</li> <li>Categories: Added (new features), Changed (modifications), Fixed (bug fixes), Deprecated (to be removed), Removed (deprecated features deleted), Security (security fixes)</li> </ul>"},{"location":"CHANGELOG/#roadmap","title":"Roadmap","text":"<ul> <li> v0.1 - Core agent engine + CLI</li> <li> v0.2 - Evaluation framework</li> <li> v0.3 - API deployment</li> <li> v0.4 - Multi-agent orchestration &amp; workflows</li> <li> v0.5 - Web UI (no-code editor)</li> <li> v0.6 - Enterprise features (SSO, audit logs, RBAC)</li> <li> v1.0 - Production-ready release</li> </ul>"},{"location":"CHANGELOG/#previous-versions","title":"Previous Versions","text":""},{"location":"CHANGELOG/#development-versions","title":"Development Versions","text":"<ul> <li>Pre-0.0.1: Architecture planning and vision definition</li> <li>Project vision (VISION.md)</li> <li>Architecture documentation</li> <li>Specification and planning</li> </ul>"},{"location":"CHANGELOG/#contributing","title":"Contributing","text":"<p>See CONTRIBUTING.md for guidelines on:</p> <ul> <li>Development setup</li> <li>Running tests</li> <li>Code style requirements</li> <li>Submitting pull requests</li> </ul>"},{"location":"CHANGELOG/#license","title":"License","text":"<p>HoloDeck is released under the MIT License. See LICENSE file for details.</p>"},{"location":"CHANGELOG/#changelog-format","title":"Changelog Format","text":"<p>We follow Keep a Changelog format:</p> <ul> <li>Added: New features</li> <li>Changed: Changes to existing functionality</li> <li>Deprecated: Features to be removed in future versions</li> <li>Removed: Features that have been removed</li> <li>Fixed: Bug fixes</li> <li>Security: Security-related changes</li> </ul>"},{"location":"CHANGELOG/#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started</li> <li>Configuration Guide</li> <li>API Reference</li> <li>Contributing Guide</li> </ul>"},{"location":"contributing/","title":"Contributing to HoloDeck","text":"<p>Thank you for your interest in contributing to HoloDeck! This guide will help you get started with development, testing, and submitting your changes.</p>"},{"location":"contributing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Development Setup</li> <li>Running Tests</li> <li>Code Style Guide</li> <li>Commit Message Format</li> <li>Pull Request Workflow</li> <li>Pre-commit Hooks</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>Git</li> <li>Virtual environment manager (venv)</li> </ul>"},{"location":"contributing/#initial-setup","title":"Initial Setup","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/yourusername/holodeck.git\ncd holodeck\n</code></pre> <ol> <li>Initialize the project:</li> </ol> <pre><code>make init\n</code></pre> <p>This command will:</p> <ul> <li>Create a Python virtual environment (<code>.venv</code>)</li> <li>Install all development dependencies</li> <li> <p>Set up pre-commit hooks</p> </li> <li> <p>Activate the virtual environment (if not already done by <code>make init</code>):</p> </li> </ul> <pre><code>source .venv/bin/activate  # On macOS/Linux\n# or\n.venv\\Scripts\\activate  # On Windows\n</code></pre>"},{"location":"contributing/#manual-setup-if-make-init-doesnt-work","title":"Manual Setup (if <code>make init</code> doesn't work)","text":"<pre><code># Create virtual environment\npython3 -m venv .venv\n\n# Activate it\nsource .venv/bin/activate\n\n# Install development dependencies\nmake install-dev\n\n# Install pre-commit hooks\nmake install-hooks\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":""},{"location":"contributing/#run-all-tests","title":"Run All Tests","text":"<pre><code>make test\n</code></pre>"},{"location":"contributing/#run-unit-tests-only","title":"Run Unit Tests Only","text":"<pre><code>make test-unit\n</code></pre>"},{"location":"contributing/#run-integration-tests-only","title":"Run Integration Tests Only","text":"<pre><code>make test-integration\n</code></pre>"},{"location":"contributing/#run-tests-with-coverage-report","title":"Run Tests with Coverage Report","text":"<pre><code>make test-coverage\n</code></pre> <p>This generates:</p> <ul> <li>Terminal summary</li> <li>HTML report: <code>htmlcov/index.html</code></li> <li>XML report: <code>coverage.xml</code></li> </ul> <p>Coverage Requirements: Minimum 80% coverage on all modules is enforced.</p>"},{"location":"contributing/#run-failed-tests-only","title":"Run Failed Tests Only","text":"<p>Quickly re-run tests that failed in the last run:</p> <pre><code>make test-failed\n</code></pre>"},{"location":"contributing/#run-tests-in-parallel","title":"Run Tests in Parallel","text":"<p>For faster test execution (requires pytest-xdist):</p> <pre><code>make test-parallel\n</code></pre>"},{"location":"contributing/#code-style-guide","title":"Code Style Guide","text":"<p>HoloDeck follows the Google Python Style Guide with tooling enforcement:</p>"},{"location":"contributing/#formatting","title":"Formatting","text":"<p>Code is automatically formatted using Black (88 character line length):</p> <pre><code>make format\n</code></pre>"},{"location":"contributing/#check-formatting-without-changes","title":"Check Formatting Without Changes","text":"<p>Verify code is properly formatted without modifying files:</p> <pre><code>make format-check\n</code></pre>"},{"location":"contributing/#linting","title":"Linting","text":"<p>Code quality is enforced using Ruff:</p> <pre><code>make lint\n</code></pre> <p>To auto-fix linting issues:</p> <pre><code>make lint-fix\n</code></pre> <p>Ruff includes:</p> <ul> <li>pycodestyle (E, W)</li> <li>pyflakes (F)</li> <li>isort (I) - import sorting</li> <li>flake8-bugbear (B) - bug detection</li> <li>pyupgrade (UP) - Python syntax modernization</li> <li>pep8-naming (N) - naming convention checks</li> <li>flake8-simplify (SIM) - code simplification</li> <li>flake8-bandit (S) - security checks</li> </ul>"},{"location":"contributing/#type-checking","title":"Type Checking","text":"<p>Type hints are required. Validate with MyPy:</p> <pre><code>make type-check\n</code></pre> <p>Type checking rules:</p> <ul> <li>Full type coverage required (disallow untyped defs)</li> <li>No <code>Any</code> types where avoidable</li> <li>Strict optional checking enabled</li> <li>Strict equality checking enabled</li> </ul>"},{"location":"contributing/#security-checks","title":"Security Checks","text":"<p>Run security scanning before committing:</p> <pre><code>make security\n</code></pre> <p>This includes:</p> <ul> <li>Safety: Known vulnerability detection</li> <li>Bandit: Security issue scanning</li> <li>detect-secrets: Hardcoded secret detection</li> </ul>"},{"location":"contributing/#code-style-checklist","title":"Code Style Checklist","text":"<p>Before committing, ensure:</p> <ul> <li> Code is formatted: <code>make format</code></li> <li> Linting passes: <code>make lint</code></li> <li> Type checking passes: <code>make type-check</code></li> <li> Tests pass: <code>make test-coverage</code></li> <li> Security checks pass: <code>make security</code></li> </ul>"},{"location":"contributing/#commit-message-format","title":"Commit Message Format","text":"<p>Follow this format for commit messages:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n\n&lt;body&gt;\n\n&lt;footer&gt;\n</code></pre>"},{"location":"contributing/#type","title":"Type","text":"<p>Must be one of:</p> <ul> <li>feat: A new feature</li> <li>fix: A bug fix</li> <li>docs: Documentation only changes</li> <li>style: Code formatting changes (no functional changes)</li> <li>refactor: Code refactoring without feature changes</li> <li>perf: Performance improvements</li> <li>test: Test additions or modifications</li> <li>chore: Build, dependency, or tooling changes</li> <li>ci: CI/CD configuration changes</li> </ul>"},{"location":"contributing/#scope","title":"Scope","text":"<p>Optional. Scope of the change:</p> <ul> <li><code>config</code>: Configuration loading and validation</li> <li><code>models</code>: Pydantic data models</li> <li><code>cli</code>: Command-line interface</li> <li><code>loader</code>: YAML configuration loader</li> <li><code>tests</code>: Test infrastructure</li> <li><code>docs</code>: Documentation</li> <li><code>core</code>: Core engine functionality</li> </ul>"},{"location":"contributing/#subject","title":"Subject","text":"<ul> <li>Use imperative mood: \"add\" not \"added\" or \"adds\"</li> <li>Don't capitalize first letter</li> <li>No period at the end</li> <li>Limit to 50 characters</li> </ul>"},{"location":"contributing/#body","title":"Body","text":"<ul> <li>Optional. Explain what and why, not how.</li> <li>Wrap at 72 characters</li> <li>Separate from subject with blank line</li> <li>Use bullet points for multiple changes</li> </ul>"},{"location":"contributing/#footer","title":"Footer","text":"<ul> <li>Optional. Reference issues: <code>Closes #123</code></li> <li>Reference related PRs: <code>Refs #456</code></li> </ul>"},{"location":"contributing/#examples","title":"Examples","text":"<pre><code>feat(config): add support for environment variable substitution\n\nSupport ${VAR_NAME} pattern in YAML configurations.\nVariables are substituted before schema validation.\n\nCloses #42\n</code></pre> <pre><code>fix(models): correct validation for vector_field XOR vector_fields\n\nPreviously allowed both vector_field and vector_fields to be\nspecified simultaneously, violating the constraint.\n\nFixes #89\n</code></pre> <pre><code>docs: update installation instructions for Python 3.10\n</code></pre>"},{"location":"contributing/#pull-request-workflow","title":"Pull Request Workflow","text":""},{"location":"contributing/#before-creating-a-pr","title":"Before Creating a PR","text":"<ol> <li>Create a feature branch from <code>main</code>:</li> </ol> <pre><code>git checkout -b feat/your-feature-name\n</code></pre> <ol> <li> <p>Implement your changes:</p> </li> <li> <p>Write tests first (TDD approach)</p> </li> <li>Implement feature</li> <li> <p>Ensure all tests pass</p> </li> <li> <p>Run full validation:</p> </li> </ol> <pre><code>make ci\n</code></pre> <p>This runs:</p> <ul> <li>Code formatting checks</li> <li>Linting</li> <li>Type checking</li> <li>Test suite with coverage</li> <li> <p>Security scanning</p> </li> <li> <p>Commit with proper messages:</p> </li> </ul> <pre><code>git add .\ngit commit -m \"feat(scope): descriptive message\"\n</code></pre> <ol> <li>Push to your fork:</li> </ol> <pre><code>git push origin feat/your-feature-name\n</code></pre>"},{"location":"contributing/#creating-a-pr","title":"Creating a PR","text":"<ol> <li>Visit the repository on GitHub</li> <li>Click \"New Pull Request\"</li> <li>Select your branch as the source</li> <li>Fill in the PR template:</li> </ol> <pre><code>## Description\n\nBrief description of changes\n\n## Type of Change\n\n- [ ] New feature\n- [ ] Bug fix\n- [ ] Breaking change\n- [ ] Documentation update\n\n## Testing\n\nDescribe how you tested this change\n\n## Checklist\n\n- [ ] Tests pass locally (`make test`)\n- [ ] Code formatted (`make format`)\n- [ ] Type checking passes (`make type-check`)\n- [ ] Coverage \u226580%\n- [ ] Updated relevant documentation\n</code></pre>"},{"location":"contributing/#pr-requirements","title":"PR Requirements","text":"<p>Before a PR can be merged:</p> <p>\u2705 All checks must pass:</p> <ul> <li>Formatting check</li> <li>Linting</li> <li>Type checking</li> <li>Test coverage (\u226580%)</li> <li>Security scanning</li> </ul> <p>\u2705 At least one review approval</p> <p>\u2705 No merge conflicts</p>"},{"location":"contributing/#code-review-process","title":"Code Review Process","text":"<ul> <li> <p>Reviewers will check for:</p> </li> <li> <p>Code quality and style consistency</p> </li> <li>Test coverage</li> <li>Performance implications</li> <li>Documentation accuracy</li> <li> <p>Security considerations</p> </li> <li> <p>Address feedback and push updates (do not force-push)</p> </li> <li>Re-request review after changes</li> <li>We aim to review within 1-2 business days</li> </ul>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Pre-commit hooks automatically run before each commit to catch issues early.</p>"},{"location":"contributing/#install-pre-commit-hooks","title":"Install Pre-commit Hooks","text":"<pre><code>make install-hooks\n</code></pre>"},{"location":"contributing/#hooks-included","title":"Hooks Included","text":"<ul> <li>black: Code formatting</li> <li>ruff: Linting and import sorting</li> <li>mypy: Type checking</li> <li>detect-secrets: Secret detection</li> <li>trailing-whitespace: Remove trailing whitespace</li> <li>end-of-file-fixer: Ensure newline at EOF</li> </ul>"},{"location":"contributing/#bypass-pre-commit-use-carefully","title":"Bypass Pre-commit (Use Carefully!)","text":"<pre><code>git commit --no-verify\n</code></pre> <p>Only use this in exceptional circumstances and ensure you run the checks manually.</p>"},{"location":"contributing/#project-architecture","title":"Project Architecture","text":"<p>For understanding the codebase structure, see:</p> <ul> <li><code>docs/architecture/</code> - Architecture documentation</li> <li><code>docs/api/models.md</code> - Data model documentation</li> <li><code>docs/api/config-loader.md</code> - ConfigLoader API reference</li> </ul>"},{"location":"contributing/#common-issues","title":"Common Issues","text":""},{"location":"contributing/#virtual-environment-not-activated","title":"Virtual Environment Not Activated","text":"<p>Problem: <code>command not found: pytest</code></p> <p>Solution:</p> <pre><code>source .venv/bin/activate\n</code></pre>"},{"location":"contributing/#type-checking-fails","title":"Type Checking Fails","text":"<p>Problem: MyPy reports type errors</p> <p>Solution:</p> <ol> <li>Add type hints to function parameters and returns</li> <li>For unavoidable types: <code># type: ignore</code> (use sparingly)</li> <li>Check <code>Any</code> usage - prefer specific types</li> </ol>"},{"location":"contributing/#tests-fail-after-changes","title":"Tests Fail After Changes","text":"<p>Problem: Tests pass locally but fail in CI</p> <p>Solution:</p> <ol> <li>Run full test suite: <code>make test</code></li> <li>Run with coverage: <code>make test-coverage</code></li> <li>Check for system-specific issues (paths, line endings)</li> </ol>"},{"location":"contributing/#security-scan-fails","title":"Security Scan Fails","text":"<p>Problem: Security checks report issues</p> <p>Solution:</p> <ol> <li>Run locally: <code>make security</code></li> <li>Fix hardcoded secrets or security vulnerabilities</li> <li>Update <code>.secrets.baseline</code> if necessary (carefully!)</li> </ol>"},{"location":"contributing/#development-tips","title":"Development Tips","text":""},{"location":"contributing/#running-specific-tests","title":"Running Specific Tests","text":"<pre><code># Run specific test file\npytest tests/unit/test_config_loader.py -v\n\n# Run specific test function\npytest tests/unit/test_config_loader.py::test_load_agent_yaml -v\n\n# Run tests matching pattern\npytest -k \"validation\" -v\n</code></pre>"},{"location":"contributing/#debugging","title":"Debugging","text":"<p>Enable verbose output:</p> <pre><code>pytest -vv --tb=long tests/unit/test_config_loader.py\n</code></pre> <p>Use Python debugger:</p> <pre><code>pytest --pdb tests/unit/test_config_loader.py\n</code></pre>"},{"location":"contributing/#local-documentation","title":"Local Documentation","text":"<p>Build and serve documentation locally:</p> <pre><code>mkdocs serve\n</code></pre> <p>Visit <code>http://localhost:8000</code> to view.</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Issues: Open a GitHub issue with details</li> <li>Discussions: Use GitHub Discussions for questions</li> <li>Documentation: Check <code>docs/</code> for answers</li> <li>Code examples: See <code>docs/examples/</code> for configuration examples</li> </ul>"},{"location":"contributing/#thank-you","title":"Thank You","text":"<p>Thank you for contributing to HoloDeck! Your efforts help make this project better for everyone.</p>"},{"location":"api/cli/","title":"CLI API Reference","text":"<p>HoloDeck provides a command-line interface for project initialization, agent testing, and configuration management. This section documents the programmatic CLI API.</p>"},{"location":"api/cli/#main-cli","title":"Main CLI","text":"<p>Entry point for the HoloDeck CLI application using Click.</p>"},{"location":"api/cli/#holodeck.cli.main.main","title":"<code>main(ctx)</code>","text":"<p>HoloDeck - Experimentation platform for AI agents.</p> Commands <p>init   Initialize a new agent project test   Run agent test cases chat   Interactive chat session with an agent</p> <p>Initialize and manage AI agent projects with YAML configuration.</p> Source code in <code>src/holodeck/cli/main.py</code> <pre><code>@click.group(invoke_without_command=True)\n@click.version_option(version=__version__, prog_name=\"holodeck\")\n@click.pass_context\ndef main(ctx: click.Context) -&gt; None:\n    \"\"\"HoloDeck - Experimentation platform for AI agents.\n\n    Commands:\n        init   Initialize a new agent project\n        test   Run agent test cases\n        chat   Interactive chat session with an agent\n\n    Initialize and manage AI agent projects with YAML configuration.\n    \"\"\"\n    # Show help if no command is provided\n    if ctx.invoked_subcommand is None:\n        click.echo(ctx.get_help())\n</code></pre>"},{"location":"api/cli/#cli-commands","title":"CLI Commands","text":""},{"location":"api/cli/#init-command","title":"Init Command","text":"<p>Initialize a new HoloDeck project with bundled templates.</p>"},{"location":"api/cli/#holodeck.cli.commands.init.init","title":"<code>init(project_name, template, description, author, force)</code>","text":"<p>Initialize a new HoloDeck agent project.</p> <p>Creates a new project directory with all required configuration files, example instructions, tools templates, test cases, and data files.</p> <p>The generated project includes agent.yaml (main configuration), instructions/ (system prompts), tools/ (custom function templates), data/ (sample datasets), and tests/ (evaluation test cases).</p> <p>TEMPLATES:</p> <pre><code>conversational  - General-purpose conversational agent (default)\nresearch        - Research/analysis agent with vector search examples\ncustomer-support - Customer support agent with function tools\n</code></pre> <p>EXAMPLES:</p> <pre><code>Basic project with default (conversational) template:\n\n    holodeck init my-chatbot\n\nResearch-focused agent with metadata:\n\n    holodeck init research-agent --template research \\\n        --description \"Research paper analysis and summarization\" \\\n        --author \"Data Team\"\n\nCustomer support agent:\n\n    holodeck init support-bot --template customer-support \\\n        --description \"Intelligent customer support chatbot\" \\\n        --author \"Support Team\"\n\nOverwrite existing project:\n\n    holodeck init my-agent --force\n</code></pre> <p>For more information, see: https://useholodeck.ai/docs/getting-started</p> Source code in <code>src/holodeck/cli/commands/init.py</code> <pre><code>@click.command(name=\"init\")\n@click.argument(\"project_name\")\n@click.option(\n    \"--template\",\n    default=\"conversational\",\n    type=str,\n    callback=validate_template,\n    help=\"Project template: conversational (default), research, or customer-support\",\n)\n@click.option(\n    \"--description\",\n    default=None,\n    help=\"Brief description of what the agent does\",\n)\n@click.option(\n    \"--author\",\n    default=None,\n    help=\"Name of the project creator or organization\",\n)\n@click.option(\n    \"--force\",\n    is_flag=True,\n    help=\"Overwrite existing project directory without prompting\",\n)\ndef init(\n    project_name: str,\n    template: str,\n    description: str | None,\n    author: str | None,\n    force: bool,\n) -&gt; None:\n    \"\"\"Initialize a new HoloDeck agent project.\n\n    Creates a new project directory with all required configuration files,\n    example instructions, tools templates, test cases, and data files.\n\n    The generated project includes agent.yaml (main configuration), instructions/\n    (system prompts), tools/ (custom function templates), data/ (sample datasets),\n    and tests/ (evaluation test cases).\n\n    TEMPLATES:\n\n        conversational  - General-purpose conversational agent (default)\n        research        - Research/analysis agent with vector search examples\n        customer-support - Customer support agent with function tools\n\n    EXAMPLES:\n\n        Basic project with default (conversational) template:\n\n            holodeck init my-chatbot\n\n        Research-focused agent with metadata:\n\n            holodeck init research-agent --template research \\\\\n                --description \"Research paper analysis and summarization\" \\\\\n                --author \"Data Team\"\n\n        Customer support agent:\n\n            holodeck init support-bot --template customer-support \\\\\n                --description \"Intelligent customer support chatbot\" \\\\\n                --author \"Support Team\"\n\n        Overwrite existing project:\n\n            holodeck init my-agent --force\n\n    For more information, see: https://useholodeck.ai/docs/getting-started\n    \"\"\"\n    try:\n        # Get current working directory as output directory\n        output_dir = Path.cwd()\n\n        # Check if project directory already exists (unless force)\n        project_dir = output_dir / project_name\n        if project_dir.exists() and not force:\n            # Prompt user for confirmation\n            if click.confirm(\n                f\"Project directory '{project_name}' already exists. \"\n                \"Do you want to overwrite it?\",\n                default=False,\n            ):\n                force = True\n            else:\n                click.echo(\"Initialization cancelled.\")\n                return\n\n        # Create project initialization input\n        init_input = ProjectInitInput(\n            project_name=project_name,\n            template=template,\n            description=description,\n            author=author,\n            output_dir=str(output_dir),\n            overwrite=force,\n        )\n\n        # Initialize project\n        initializer = ProjectInitializer()\n        result = initializer.initialize(init_input)\n\n        # Handle result\n        if result.success:\n            # Display success message\n            click.echo()  # Blank line for readability\n            click.secho(\"\u2713 Project initialized successfully!\", fg=\"green\", bold=True)\n            click.echo()\n            click.echo(f\"Project: {result.project_name}\")\n            click.echo(f\"Location: {result.project_path}\")\n            click.echo(f\"Template: {result.template_used}\")\n            click.echo(f\"Time: {result.duration_seconds:.2f}s\")\n\n            # Show created files (first 10, then summary)\n            if result.files_created:\n                click.echo()\n                click.echo(\"Files created:\")\n                # Show key files first (config, instructions, tools, data)\n                key_files = [\n                    f\n                    for f in result.files_created\n                    if \"agent.yaml\" in f\n                    or \"system-prompt\" in f\n                    or \"tools\" in f\n                    or \"data\" in f\n                ]\n                for file_path in key_files[:5]:\n                    click.echo(f\"  \u2022 {file_path}\")\n                if len(result.files_created) &gt; 5:\n                    remaining = len(result.files_created) - 5\n                    click.echo(f\"  ... and {remaining} more file(s)\")\n\n            click.echo()\n            click.echo(\"Next steps:\")\n            click.echo(f\"  1. cd {result.project_name}\")\n            click.echo(\"  2. Edit agent.yaml to configure your agent\")\n            click.echo(\"  3. Edit instructions/system-prompt.md to customize behavior\")\n            click.echo(\"  4. Add tools in tools/ directory\")\n            click.echo(\"  5. Update test_cases in agent.yaml\")\n            click.echo(\"  6. Run tests with: holodeck test agent.yaml\")\n            click.echo()\n        else:\n            # Display error message\n            click.secho(\"\u2717 Project initialization failed\", fg=\"red\", bold=True)\n            click.echo()\n            for error in result.errors:\n                click.secho(f\"Error: {error}\", fg=\"red\")\n            click.echo()\n            raise click.Abort()\n\n    except KeyboardInterrupt as e:\n        # Handle Ctrl+C gracefully with cleanup\n        click.echo()\n        click.secho(\"Initialization cancelled by user.\", fg=\"yellow\")\n        raise click.Abort() from e\n\n    except (ValidationError, InitError) as e:\n        # Handle known errors\n        click.secho(f\"Error: {str(e)}\", fg=\"red\")\n        raise click.Abort() from e\n\n    except Exception as e:\n        # Handle unexpected errors\n        click.secho(f\"Unexpected error: {str(e)}\", fg=\"red\")\n        raise click.Abort() from e\n</code></pre>"},{"location":"api/cli/#test-command","title":"Test Command","text":"<p>Run tests for a HoloDeck agent with evaluation and reporting.</p>"},{"location":"api/cli/#holodeck.cli.commands.test.test","title":"<code>test(agent_config, output, format, verbose, quiet, timeout, force_ingest)</code>","text":"<p>Execute agent test cases with evaluation metrics.</p> <p>Runs test cases defined in the agent configuration file and displays pass/fail status with evaluation metric scores.</p> <p>AGENT_CONFIG is the path to the agent.yaml configuration file.</p> Source code in <code>src/holodeck/cli/commands/test.py</code> <pre><code>@click.command()\n@click.argument(\"agent_config\", type=click.Path(exists=True))\n@click.option(\n    \"--output\",\n    type=click.Path(),\n    default=None,\n    help=\"Path to save test report file (JSON or Markdown)\",\n)\n@click.option(\n    \"--format\",\n    type=click.Choice([\"json\", \"markdown\"]),\n    default=None,\n    help=\"Report format (auto-detect from extension if not specified)\",\n)\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Enable verbose output with debug information\",\n)\n@click.option(\n    \"--quiet\",\n    \"-q\",\n    is_flag=True,\n    help=\"Suppress progress output (summary still shown)\",\n)\n@click.option(\n    \"--timeout\",\n    type=int,\n    default=None,\n    help=\"LLM execution timeout in seconds\",\n)\n@click.option(\n    \"--force-ingest\",\n    \"-f\",\n    is_flag=True,\n    help=\"Force re-ingestion of all vector store source files\",\n)\ndef test(\n    agent_config: str,\n    output: str | None,\n    format: str | None,\n    verbose: bool,\n    quiet: bool,\n    timeout: int | None,\n    force_ingest: bool,\n) -&gt; None:\n    \"\"\"Execute agent test cases with evaluation metrics.\n\n    Runs test cases defined in the agent configuration file and displays\n    pass/fail status with evaluation metric scores.\n\n    AGENT_CONFIG is the path to the agent.yaml configuration file.\n    \"\"\"\n    # Reconfigure logging based on CLI flags\n    # If verbose is enabled, it overrides quiet mode\n    effective_quiet = quiet and not verbose\n    setup_logging(verbose=verbose, quiet=effective_quiet)\n\n    logger.info(\n        f\"Test command invoked: config={agent_config}, \"\n        f\"verbose={verbose}, quiet={quiet}, timeout={timeout}, \"\n        f\"force_ingest={force_ingest}\"\n    )\n\n    start_time = time.time()\n\n    try:\n        # Create execution config from CLI options\n        cli_config = None\n        if timeout is not None or verbose or quiet:\n            cli_config = ExecutionConfig(\n                llm_timeout=timeout,\n                file_timeout=None,\n                download_timeout=None,\n                cache_enabled=None,\n                cache_dir=None,\n                verbose=verbose or None,\n                quiet=quiet or None,\n            )\n\n        # Load agent config to get test count for progress indicator\n        from holodeck.config.context import agent_base_dir\n        from holodeck.config.defaults import DEFAULT_EXECUTION_CONFIG\n        from holodeck.config.loader import ConfigLoader\n\n        logger.debug(f\"Loading agent configuration from {agent_config}\")\n        loader = ConfigLoader()\n        agent = loader.load_agent_yaml(agent_config)\n        logger.info(f\"Agent configuration loaded successfully: {agent.name}\")\n\n        # Set the base directory context for resolving relative paths in tools\n        agent_dir = str(Path(agent_config).parent.resolve())\n        agent_base_dir.set(agent_dir)\n        logger.debug(f\"Set agent_base_dir context: {agent_base_dir.get()}\")\n\n        # Resolve execution config once (CLI &gt; agent.yaml &gt; project &gt; user &gt; defaults)\n        project_config = loader.load_project_config(agent_dir)\n        project_execution = project_config.execution if project_config else None\n        user_config = loader.load_global_config()\n        user_execution = user_config.execution if user_config else None\n\n        resolved_config = loader.resolve_execution_config(\n            cli_config=cli_config,\n            yaml_config=agent.execution,\n            project_config=project_execution,\n            user_config=user_execution,\n            defaults=DEFAULT_EXECUTION_CONFIG,\n        )\n        logger.debug(\n            f\"Resolved execution config: verbose={resolved_config.verbose}, \"\n            f\"quiet={resolved_config.quiet}, llm_timeout={resolved_config.llm_timeout}\"\n        )\n\n        # Get total test count\n        total_tests = len(agent.test_cases) if agent.test_cases else 0\n        logger.info(f\"Found {total_tests} test cases to execute\")\n\n        # Initialize progress indicator\n        progress = ProgressIndicator(\n            total_tests=total_tests, quiet=quiet, verbose=verbose\n        )\n\n        # Initialize spinner thread\n        spinner_thread = SpinnerThread(progress)\n\n        # Define callbacks\n        def on_test_start(test_case: TestCaseModel) -&gt; None:\n            \"\"\"Start spinner for new test.\"\"\"\n            progress.start_test(test_case.name or \"Test\")\n            if not spinner_thread.is_alive() and not quiet and sys.stdout.isatty():\n                # Start thread if not running (it handles its own loop)\n                # Note: We can't restart a thread, so we might need a new instance\n                # or just keep it running and use events.\n                # Simpler approach: Start it once before execution and use flags.\n                pass\n\n        def progress_callback(result: TestResult) -&gt; None:\n            \"\"\"Update progress indicator and display progress line.\"\"\"\n            # Temporarily stop spinner to print result\n            # In a real implementation with threading, we might need a lock\n            # But here we just clear the line in the main thread (via update)\n\n            # Update progress state\n            progress.update(result)\n\n            # Get formatted result line\n            progress_line = progress.get_progress_line()\n\n            if progress_line:  # Only print if not empty (respects quiet mode)\n                # Clear current line (handled by \\r in spinner)\n                sys.stdout.write(\"\\r\" + \" \" * 60 + \"\\r\")\n                click.echo(progress_line)\n\n        # Initialize executor with pre-loaded configs to avoid duplicate I/O\n        logger.debug(\"Initializing test executor\")\n        executor = TestExecutor(\n            agent_config_path=agent_config,\n            progress_callback=progress_callback,\n            on_test_start=on_test_start,\n            force_ingest=force_ingest,\n            agent_config=agent,\n            resolved_execution_config=resolved_config,\n        )\n\n        # Run tests asynchronously\n        logger.info(\"Starting test execution\")\n\n        # Start spinner thread\n        if not quiet and sys.stdout.isatty():\n            spinner_thread.start()\n\n        async def run_tests_with_cleanup() -&gt; TestReport:\n            \"\"\"Execute tests and ensure proper cleanup of MCP plugins.\"\"\"\n            try:\n                return await executor.execute_tests()\n            finally:\n                await executor.shutdown()\n\n        try:\n            report = asyncio.run(run_tests_with_cleanup())\n        finally:\n            # Ensure spinner stops\n            if spinner_thread.is_alive():\n                spinner_thread.stop()\n                spinner_thread.join()\n\n        elapsed_time = time.time() - start_time\n        logger.info(\n            f\"Test execution completed in {elapsed_time:.2f}s - \"\n            f\"{report.summary.passed} passed, {report.summary.failed} failed\"\n        )\n\n        # Display summary (always shown, even in quiet mode)\n        summary_text = progress.get_summary()\n        click.echo(summary_text)\n\n        # Save report if output specified\n        if output:\n            logger.debug(f\"Saving report to {output} (format={format})\")\n            _save_report(report, output, format)\n            logger.info(f\"Report saved successfully to {output}\")\n\n        # Exit with appropriate code\n        if report.summary.failed &gt; 0:\n            logger.info(\"Exiting with failure status (failed tests)\")\n            sys.exit(1)\n        else:\n            logger.info(\"Exiting with success status (all tests passed)\")\n            sys.exit(0)\n\n    except ConfigError as e:\n        logger.error(f\"Configuration error: {e}\", exc_info=True)\n        click.echo(f\"Configuration Error: {e}\", err=True)\n        sys.exit(2)\n    except ExecutionError as e:\n        logger.error(f\"Execution error: {e}\", exc_info=True)\n        click.echo(f\"Execution Error: {e}\", err=True)\n        sys.exit(3)\n    except EvaluationError as e:\n        logger.error(f\"Evaluation error: {e}\", exc_info=True)\n        click.echo(f\"Evaluation Error: {e}\", err=True)\n        sys.exit(4)\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\", exc_info=True)\n        click.echo(f\"Error: {str(e)}\", err=True)\n        sys.exit(3)\n</code></pre>"},{"location":"api/cli/#cli-utilities","title":"CLI Utilities","text":"<p>Project initialization and scaffolding utilities.</p>"},{"location":"api/cli/#holodeck.cli.utils.project_init.ProjectInitializer","title":"<code>ProjectInitializer()</code>","text":"<p>Handles project initialization logic.</p> <p>Provides methods to: - Validate user inputs (project name, template, permissions) - Load and validate template manifests - Initialize new agent projects with all required files</p> <p>Initialize the ProjectInitializer.</p> Source code in <code>src/holodeck/cli/utils/project_init.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the ProjectInitializer.\"\"\"\n    self.template_renderer = TemplateRenderer()\n    # Get available templates from discovery function\n    self.available_templates = set(TemplateRenderer.list_available_templates())\n</code></pre>"},{"location":"api/cli/#holodeck.cli.utils.project_init.ProjectInitializer.initialize","title":"<code>initialize(input_data)</code>","text":"<p>Initialize a new agent project.</p> <p>Creates a new project directory with all required files and templates. Follows all-or-nothing semantics: either the entire project is created successfully, or no files are created and the directory is cleaned up.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>ProjectInitInput</code> <p>ProjectInitInput with validated user inputs</p> required <p>Returns:</p> Name Type Description <code>ProjectInitResult</code> <code>ProjectInitResult</code> <p>Result of initialization with status and metadata</p> <p>Raises:</p> Type Description <code>InitError</code> <p>If initialization fails (will attempt cleanup)</p> Source code in <code>src/holodeck/cli/utils/project_init.py</code> <pre><code>def initialize(self, input_data: ProjectInitInput) -&gt; ProjectInitResult:\n    \"\"\"Initialize a new agent project.\n\n    Creates a new project directory with all required files and templates.\n    Follows all-or-nothing semantics: either the entire project is created\n    successfully, or no files are created and the directory is cleaned up.\n\n    Args:\n        input_data: ProjectInitInput with validated user inputs\n\n    Returns:\n        ProjectInitResult: Result of initialization with status and metadata\n\n    Raises:\n        InitError: If initialization fails (will attempt cleanup)\n    \"\"\"\n    start_time = time.time()\n    project_name = input_data.project_name.strip()\n    output_dir = Path(input_data.output_dir)\n    project_dir = output_dir / project_name\n\n    files_created = []\n\n    try:\n        # Validate inputs first\n        self.validate_inputs(input_data)\n\n        # Load template manifest\n        template = self.load_template(input_data.template)\n\n        # Create project directory\n        if project_dir.exists() and input_data.overwrite:\n            # Remove existing directory if force flag is set\n            shutil.rmtree(project_dir)\n\n        project_dir.mkdir(parents=True, exist_ok=False)\n        files_created.append(str(project_dir))\n\n        # Prepare template variables\n        template_vars = {\n            \"project_name\": project_name,\n            \"description\": input_data.description or \"TODO: Add agent description\",\n            \"author\": input_data.author or \"\",\n        }\n\n        # Add template-specific defaults from manifest\n        if template.defaults:\n            template_vars.update(template.defaults)\n\n        # Create files from template\n        template_dir = (\n            Path(__file__).parent.parent.parent / \"templates\" / input_data.template\n        )\n\n        # Process each file in the template manifest\n        if template.files:\n            for file_spec in template.files.values():\n                if not file_spec.required:\n                    continue\n\n                file_path = project_dir / file_spec.path\n                file_path.parent.mkdir(parents=True, exist_ok=True)\n\n                if file_spec.template:\n                    # Render Jinja2 template\n                    template_file = template_dir / f\"{file_spec.path}.j2\"\n                    if not template_file.exists():\n                        # Try without .j2 extension\n                        template_file = template_dir / file_spec.path\n\n                    if file_path.suffix == \".yaml\" or file_path.suffix == \".yml\":\n                        # Validate YAML files against schema\n                        content = self.template_renderer.render_and_validate(\n                            str(template_file), template_vars\n                        )\n                    else:\n                        # Render non-YAML files normally\n                        content = self.template_renderer.render_template(\n                            str(template_file), template_vars\n                        )\n\n                    file_path.write_text(content)\n                else:\n                    # Copy static files directly\n                    source_file = template_dir / file_spec.path\n                    if source_file.exists():\n                        shutil.copy2(source_file, file_path)\n\n                files_created.append(str(file_path.relative_to(output_dir)))\n\n        # Also copy .gitignore if it exists\n        gitignore_src = template_dir / \".gitignore\"\n        if gitignore_src.exists():\n            gitignore_dst = project_dir / \".gitignore\"\n            shutil.copy2(gitignore_src, gitignore_dst)\n            files_created.append(str(gitignore_dst.relative_to(output_dir)))\n\n        duration = time.time() - start_time\n\n        return ProjectInitResult(\n            success=True,\n            project_name=project_name,\n            project_path=str(project_dir),\n            template_used=input_data.template,\n            files_created=files_created,\n            warnings=[],\n            errors=[],\n            duration_seconds=duration,\n        )\n\n    except (ValidationError, InitError) as e:\n        # Clean up partial directory on error\n        if project_dir.exists():\n            with contextlib.suppress(Exception):\n                shutil.rmtree(project_dir)\n\n        duration = time.time() - start_time\n\n        return ProjectInitResult(\n            success=False,\n            project_name=project_name,\n            project_path=str(project_dir),\n            template_used=input_data.template,\n            files_created=[],\n            warnings=[],\n            errors=[str(e)],\n            duration_seconds=duration,\n        )\n\n    except Exception as e:\n        # Clean up partial directory on unexpected error\n        if project_dir.exists():\n            with contextlib.suppress(Exception):\n                shutil.rmtree(project_dir)\n\n        duration = time.time() - start_time\n\n        return ProjectInitResult(\n            success=False,\n            project_name=project_name,\n            project_path=str(project_dir),\n            template_used=input_data.template,\n            files_created=[],\n            warnings=[],\n            errors=[f\"Unexpected error: {str(e)}\"],\n            duration_seconds=duration,\n        )\n</code></pre>"},{"location":"api/cli/#holodeck.cli.utils.project_init.ProjectInitializer.load_template","title":"<code>load_template(template_name)</code>","text":"<p>Load and validate a template manifest.</p> <p>Loads the manifest.yaml file from a template directory and validates it against the TemplateManifest schema.</p> <p>Parameters:</p> Name Type Description Default <code>template_name</code> <code>str</code> <p>Name of the template (e.g., 'conversational')</p> required <p>Returns:</p> Name Type Description <code>TemplateManifest</code> <code>TemplateManifest</code> <p>Parsed and validated template manifest</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If template or manifest file not found</p> <code>InitError</code> <p>If manifest cannot be parsed or validated</p> Source code in <code>src/holodeck/cli/utils/project_init.py</code> <pre><code>def load_template(self, template_name: str) -&gt; TemplateManifest:\n    \"\"\"Load and validate a template manifest.\n\n    Loads the manifest.yaml file from a template directory and validates\n    it against the TemplateManifest schema.\n\n    Args:\n        template_name: Name of the template (e.g., 'conversational')\n\n    Returns:\n        TemplateManifest: Parsed and validated template manifest\n\n    Raises:\n        FileNotFoundError: If template or manifest file not found\n        InitError: If manifest cannot be parsed or validated\n    \"\"\"\n    # Get template directory\n    # Templates are bundled in src/holodeck/templates/\n    template_dir = Path(__file__).parent.parent.parent / \"templates\" / template_name\n\n    if not template_dir.exists():\n        raise FileNotFoundError(f\"Template directory not found: {template_dir}\")\n\n    manifest_path = template_dir / \"manifest.yaml\"\n\n    if not manifest_path.exists():\n        raise FileNotFoundError(f\"Template manifest not found: {manifest_path}\")\n\n    try:\n        with open(manifest_path) as f:\n            manifest_data = yaml.safe_load(f)\n\n        if not manifest_data:\n            raise InitError(f\"Template manifest is empty: {manifest_path}\")\n\n        # Validate against TemplateManifest schema\n        manifest = TemplateManifest.model_validate(manifest_data)\n        return manifest\n\n    except yaml.YAMLError as e:\n        raise InitError(f\"Template manifest contains invalid YAML: {e}\") from e\n    except Exception as e:\n        if isinstance(e, ValidationError | InitError):\n            raise\n        raise InitError(f\"Failed to load template manifest: {e}\") from e\n</code></pre>"},{"location":"api/cli/#holodeck.cli.utils.project_init.ProjectInitializer.validate_inputs","title":"<code>validate_inputs(input_data)</code>","text":"<p>Validate user inputs for project initialization.</p> <p>Checks: - Project name format (alphanumeric, hyphens, underscores, no leading digits) - Project name is not empty and within length limits - Template exists in available templates - Output directory is writable - Project directory doesn't already exist (unless overwrite is True)</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>ProjectInitInput</code> <p>ProjectInitInput with user-provided values</p> required <p>Raises:</p> Type Description <code>ValidationError</code> <p>If any validation checks fail</p> Source code in <code>src/holodeck/cli/utils/project_init.py</code> <pre><code>def validate_inputs(self, input_data: ProjectInitInput) -&gt; None:\n    \"\"\"Validate user inputs for project initialization.\n\n    Checks:\n    - Project name format (alphanumeric, hyphens, underscores, no leading digits)\n    - Project name is not empty and within length limits\n    - Template exists in available templates\n    - Output directory is writable\n    - Project directory doesn't already exist (unless overwrite is True)\n\n    Args:\n        input_data: ProjectInitInput with user-provided values\n\n    Raises:\n        ValidationError: If any validation checks fail\n    \"\"\"\n    project_name = input_data.project_name.strip()\n\n    # Check project name is not empty\n    if not project_name:\n        raise ValidationError(\"Project name cannot be empty\")\n\n    # Check project name length\n    if len(project_name) &gt; self.MAX_PROJECT_NAME_LENGTH:\n        raise ValidationError(\n            f\"Project name cannot exceed {self.MAX_PROJECT_NAME_LENGTH} characters\"\n        )\n\n    # Check project name format\n    if not re.match(self.PROJECT_NAME_PATTERN, project_name):\n        raise ValidationError(\n            f\"Invalid project name: '{project_name}'. \"\n            \"Project names must start with a letter or underscore, \"\n            \"and contain only alphanumeric characters, hyphens, and underscores.\"\n        )\n\n    # Check template exists\n    if input_data.template not in self.available_templates:\n        templates_list = \", \".join(sorted(self.available_templates))\n        raise ValidationError(\n            f\"Unknown template: '{input_data.template}'. \"\n            f\"Available templates: {templates_list}\"\n        )\n\n    # Check output directory is writable\n    output_dir = Path(input_data.output_dir)\n    if not output_dir.exists():\n        raise ValidationError(f\"Output directory does not exist: {output_dir}\")\n\n    if not output_dir.is_dir():\n        raise ValidationError(f\"Output path is not a directory: {output_dir}\")\n\n    try:\n        # Test write permissions by attempting to check access\n        if not os.access(str(output_dir), os.W_OK):\n            raise ValidationError(f\"Output directory is not writable: {output_dir}\")\n    except OSError as e:\n        raise ValidationError(f\"Cannot access output directory: {e}\") from e\n\n    # Check project directory doesn't already exist (unless force)\n    project_dir = output_dir / project_name\n    if project_dir.exists() and not input_data.overwrite:\n        raise ValidationError(\n            f\"Project directory already exists: {project_dir}. \"\n            \"Use --force to overwrite.\"\n        )\n</code></pre>"},{"location":"api/cli/#cli-exceptions","title":"CLI Exceptions","text":"<p>CLI-specific exception handling.</p>"},{"location":"api/cli/#holodeck.cli.exceptions.CLIError","title":"<code>CLIError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all CLI errors.</p> <p>This is the parent class for all exceptions raised by the CLI module. Users can catch this to handle any CLI error generically.</p>"},{"location":"api/cli/#holodeck.cli.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>CLIError</code></p> <p>Raised when user input validation fails.</p> <p>This exception is raised when: - Project name is invalid (special characters, leading digits, etc.) - Template choice doesn't exist - Directory permissions are insufficient - Input constraints are violated</p> <p>Attributes:</p> Name Type Description <code>message</code> <p>Description of the validation failure</p>"},{"location":"api/cli/#holodeck.cli.exceptions.InitError","title":"<code>InitError</code>","text":"<p>               Bases: <code>CLIError</code></p> <p>Raised when project initialization fails.</p> <p>This exception is raised when: - Directory creation fails - File writing fails - Cleanup fails after partial creation - Unexpected errors occur during initialization</p> <p>Attributes:</p> Name Type Description <code>message</code> <p>Description of the initialization failure</p>"},{"location":"api/cli/#holodeck.cli.exceptions.TemplateError","title":"<code>TemplateError</code>","text":"<p>               Bases: <code>CLIError</code></p> <p>Raised when template processing fails.</p> <p>This exception is raised when: - Template manifest is malformed or missing - Jinja2 rendering fails - Generated YAML doesn't validate against schema - Template variables are missing or invalid</p> <p>Attributes:</p> Name Type Description <code>message</code> <p>Description of the template failure</p>"},{"location":"api/cli/#usage-from-python","title":"Usage from Python","text":"<p>You can invoke CLI commands programmatically:</p> <pre><code>from holodeck.cli.main import main\nfrom click.testing import CliRunner\n\nrunner = CliRunner()\n\n# Initialize a new project\nresult = runner.invoke(main, ['init', '--template', 'conversational', '--name', 'my-agent'])\nprint(result.output)\n\n# Run tests\nresult = runner.invoke(main, ['test', 'path/to/agent.yaml'])\nprint(result.output)\n</code></pre>"},{"location":"api/cli/#cli-entry-point","title":"CLI Entry Point","text":"<p>The CLI is registered as the <code>holodeck</code> command via <code>pyproject.toml</code>:</p> <pre><code>[project.scripts]\nholodeck = \"holodeck.cli.main:main\"\n</code></pre> <p>After installation, use from terminal:</p> <pre><code>holodeck init --template conversational --name my-agent\nholodeck test agent.yaml\n</code></pre>"},{"location":"api/cli/#related-documentation","title":"Related Documentation","text":"<ul> <li>Project Templates: Available templates</li> <li>Configuration Loading: Configuration system</li> <li>Test Runner: Test execution</li> </ul>"},{"location":"api/config-loader/","title":"Configuration Loading and Management API","text":"<p>This section documents the HoloDeck configuration system, including YAML loading, validation, environment variable substitution, and default configuration management.</p>"},{"location":"api/config-loader/#overview","title":"Overview","text":"<p>The configuration system is built on three pillars:</p> <ol> <li>Loading: Parse YAML agent configuration files</li> <li>Validation: Validate against Pydantic models with detailed error messages</li> <li>Merging: Combine default settings, user config, and environment overrides</li> </ol>"},{"location":"api/config-loader/#configloader","title":"ConfigLoader","text":"<p>The main entry point for loading HoloDeck agent configurations.</p>"},{"location":"api/config-loader/#holodeck.config.loader.ConfigLoader","title":"<code>ConfigLoader()</code>","text":"<p>Loads and validates agent configuration from YAML files.</p> <p>This class handles: - Parsing YAML files into Python dictionaries - Loading global configuration from ~/.holodeck/config.yaml - Merging configurations with proper precedence - Resolving file references (instructions, tools) - Converting validation errors into human-readable messages - Environment variable substitution</p> <p>Initialize the ConfigLoader.</p> Source code in <code>src/holodeck/config/loader.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the ConfigLoader.\"\"\"\n    pass\n</code></pre>"},{"location":"api/config-loader/#environment-variable-utilities","title":"Environment Variable Utilities","text":"<p>Support for dynamic configuration using environment variables with <code>${VAR_NAME}</code> pattern.</p>"},{"location":"api/config-loader/#holodeck.config.env_loader.substitute_env_vars","title":"<code>substitute_env_vars(text)</code>","text":"<p>Substitute environment variables in text using ${VAR_NAME} pattern.</p> <p>Replaces all occurrences of ${VAR_NAME} with the corresponding environment variable value. Raises ConfigError if a referenced variable does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text potentially containing ${VAR_NAME} patterns</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with all environment variables substituted</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If a referenced environment variable does not exist</p> Example <p>import os os.environ[\"API_KEY\"] = \"secret123\" substitute_env_vars(\"key: ${API_KEY}\") 'key: secret123'</p> Source code in <code>src/holodeck/config/env_loader.py</code> <pre><code>def substitute_env_vars(text: str) -&gt; str:\n    \"\"\"Substitute environment variables in text using ${VAR_NAME} pattern.\n\n    Replaces all occurrences of ${VAR_NAME} with the corresponding environment\n    variable value. Raises ConfigError if a referenced variable does not exist.\n\n    Args:\n        text: Text potentially containing ${VAR_NAME} patterns\n\n    Returns:\n        Text with all environment variables substituted\n\n    Raises:\n        ConfigError: If a referenced environment variable does not exist\n\n    Example:\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; os.environ[\"API_KEY\"] = \"secret123\"\n        &gt;&gt;&gt; substitute_env_vars(\"key: ${API_KEY}\")\n        'key: secret123'\n    \"\"\"\n    # Pattern to match ${VAR_NAME} - captures alphanumeric, underscore\n    pattern = r\"\\$\\{([A-Za-z_][A-Za-z0-9_]*)\\}\"\n\n    def replace_var(match: re.Match[str]) -&gt; str:\n        \"\"\"Replace a single ${VAR_NAME} pattern with env value.\n\n        Args:\n            match: Regex match object for ${VAR_NAME}\n\n        Returns:\n            Environment variable value\n\n        Raises:\n            ConfigError: If variable does not exist\n        \"\"\"\n        var_name = match.group(1)\n        if var_name not in os.environ:\n            raise ConfigError(\n                var_name,\n                f\"Environment variable '{var_name}' not found. \"\n                f\"Please set it and try again.\",\n            )\n        return os.environ[var_name]\n\n    return re.sub(pattern, replace_var, text)\n</code></pre>"},{"location":"api/config-loader/#holodeck.config.env_loader.get_env_var","title":"<code>get_env_var(key, default=None)</code>","text":"<p>Get environment variable with optional default.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Environment variable name</p> required <code>default</code> <code>Any</code> <p>Default value if variable not set</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Environment variable value or default</p> Source code in <code>src/holodeck/config/env_loader.py</code> <pre><code>def get_env_var(key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get environment variable with optional default.\n\n    Args:\n        key: Environment variable name\n        default: Default value if variable not set\n\n    Returns:\n        Environment variable value or default\n    \"\"\"\n    return os.environ.get(key, default)\n</code></pre>"},{"location":"api/config-loader/#holodeck.config.env_loader.load_env_file","title":"<code>load_env_file(path)</code>","text":"<p>Load environment variables from a .env file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to .env file</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary of loaded environment variables</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If file cannot be read</p> Source code in <code>src/holodeck/config/env_loader.py</code> <pre><code>def load_env_file(path: str) -&gt; dict[str, str]:\n    \"\"\"Load environment variables from a .env file.\n\n    Args:\n        path: Path to .env file\n\n    Returns:\n        Dictionary of loaded environment variables\n\n    Raises:\n        ConfigError: If file cannot be read\n    \"\"\"\n    try:\n        env_vars = {}\n        with open(path) as f:\n            for line in f:\n                line = line.strip()\n                if not line or line.startswith(\"#\"):\n                    continue\n                if \"=\" in line:\n                    key, value = line.split(\"=\", 1)\n                    env_vars[key.strip()] = value.strip()\n        return env_vars\n    except OSError as e:\n        raise ConfigError(\"env_file\", f\"Cannot read environment file: {e}\") from e\n</code></pre>"},{"location":"api/config-loader/#configuration-validation","title":"Configuration Validation","text":"<p>Schema validation and error normalization utilities for configuration validation.</p>"},{"location":"api/config-loader/#holodeck.config.validator.normalize_errors","title":"<code>normalize_errors(errors)</code>","text":"<p>Convert raw error messages to human-readable format.</p> <p>Processes error messages to be more user-friendly and actionable, removing technical jargon where possible.</p> <p>Parameters:</p> Name Type Description Default <code>errors</code> <code>list[str]</code> <p>List of error message strings</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of normalized, human-readable error messages</p> Source code in <code>src/holodeck/config/validator.py</code> <pre><code>def normalize_errors(errors: list[str]) -&gt; list[str]:\n    \"\"\"Convert raw error messages to human-readable format.\n\n    Processes error messages to be more user-friendly and actionable,\n    removing technical jargon where possible.\n\n    Args:\n        errors: List of error message strings\n\n    Returns:\n        List of normalized, human-readable error messages\n    \"\"\"\n    normalized: list[str] = []\n\n    for error in errors:\n        # Remove common technical prefixes\n        msg = error\n        if msg.startswith(\"value_error\"):\n            msg = msg.replace(\"value_error\", \"\").strip()\n        if msg.startswith(\"type_error\"):\n            msg = msg.replace(\"type_error\", \"\").strip()\n\n        # Improve message readability\n        if msg:\n            normalized.append(msg)\n\n    return normalized if normalized else [\"An unknown validation error occurred\"]\n</code></pre>"},{"location":"api/config-loader/#holodeck.config.validator.flatten_pydantic_errors","title":"<code>flatten_pydantic_errors(exc)</code>","text":"<p>Flatten Pydantic ValidationError into human-readable messages.</p> <p>Converts Pydantic's nested error structure into a flat list of user-friendly error messages that include field names and descriptions.</p> <p>Parameters:</p> Name Type Description Default <code>exc</code> <code>ValidationError</code> <p>Pydantic ValidationError exception</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of human-readable error messages, one per field error</p> Example <p>from pydantic import BaseModel, ValidationError class Model(BaseModel): ...     name: str try: ...     Model(name=123) ... except ValidationError as e: ...     msgs = flatten_pydantic_errors(e) ...     # msgs contains human-readable descriptions</p> Source code in <code>src/holodeck/config/validator.py</code> <pre><code>def flatten_pydantic_errors(exc: PydanticValidationError) -&gt; list[str]:\n    \"\"\"Flatten Pydantic ValidationError into human-readable messages.\n\n    Converts Pydantic's nested error structure into a flat list of\n    user-friendly error messages that include field names and descriptions.\n\n    Args:\n        exc: Pydantic ValidationError exception\n\n    Returns:\n        List of human-readable error messages, one per field error\n\n    Example:\n        &gt;&gt;&gt; from pydantic import BaseModel, ValidationError\n        &gt;&gt;&gt; class Model(BaseModel):\n        ...     name: str\n        &gt;&gt;&gt; try:\n        ...     Model(name=123)\n        ... except ValidationError as e:\n        ...     msgs = flatten_pydantic_errors(e)\n        ...     # msgs contains human-readable descriptions\n    \"\"\"\n    errors: list[str] = []\n\n    for error in exc.errors():\n        # Extract location (field path)\n        loc = error.get(\"loc\", ())\n        field_path = \".\".join(str(item) for item in loc) if loc else \"unknown\"\n\n        # Extract error message\n        msg = error.get(\"msg\", \"Unknown error\")\n        error_type = error.get(\"type\", \"\")\n\n        # Format the error message\n        if error_type == \"value_error\":\n            # For value errors, include what was provided\n            input_val = error.get(\"input\")\n            formatted = f\"Field '{field_path}': {msg} (received: {input_val!r})\"\n        else:\n            formatted = f\"Field '{field_path}': {msg}\"\n\n        errors.append(formatted)\n\n    return errors if errors else [\"Validation failed with unknown error\"]\n</code></pre>"},{"location":"api/config-loader/#holodeck.config.validator.validate_field_exists","title":"<code>validate_field_exists(data, field, field_type)</code>","text":"<p>Validate that a required field exists and has correct type.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary to validate</p> required <code>field</code> <code>str</code> <p>Field name to check</p> required <code>field_type</code> <code>type</code> <p>Expected type for the field</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If field is missing or has wrong type</p> Source code in <code>src/holodeck/config/validator.py</code> <pre><code>def validate_field_exists(data: dict[str, Any], field: str, field_type: type) -&gt; None:\n    \"\"\"Validate that a required field exists and has correct type.\n\n    Args:\n        data: Dictionary to validate\n        field: Field name to check\n        field_type: Expected type for the field\n\n    Raises:\n        ValueError: If field is missing or has wrong type\n    \"\"\"\n    if field not in data:\n        raise ValueError(f\"Required field '{field}' is missing\")\n    if not isinstance(data[field], field_type):\n        raise ValueError(\n            f\"Field '{field}' must be {field_type.__name__}, \"\n            f\"got {type(data[field]).__name__}\"\n        )\n</code></pre>"},{"location":"api/config-loader/#holodeck.config.validator.validate_mutually_exclusive","title":"<code>validate_mutually_exclusive(data, fields)</code>","text":"<p>Validate that exactly one of the given fields is present.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary to validate</p> required <code>fields</code> <code>list[str]</code> <p>List of mutually exclusive field names</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If not exactly one field is present</p> Source code in <code>src/holodeck/config/validator.py</code> <pre><code>def validate_mutually_exclusive(data: dict[str, Any], fields: list[str]) -&gt; None:\n    \"\"\"Validate that exactly one of the given fields is present.\n\n    Args:\n        data: Dictionary to validate\n        fields: List of mutually exclusive field names\n\n    Raises:\n        ValueError: If not exactly one field is present\n    \"\"\"\n    present = [f for f in fields if f in data and data[f] is not None]\n    if len(present) == 0:\n        raise ValueError(f\"Exactly one of {fields} must be provided\")\n    if len(present) &gt; 1:\n        raise ValueError(f\"Only one of {fields} can be provided, got {present}\")\n</code></pre>"},{"location":"api/config-loader/#holodeck.config.validator.validate_range","title":"<code>validate_range(value, min_val, max_val, name='value')</code>","text":"<p>Validate that a numeric value is within a range.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>Value to validate</p> required <code>min_val</code> <code>float</code> <p>Minimum allowed value (inclusive)</p> required <code>max_val</code> <code>float</code> <p>Maximum allowed value (inclusive)</p> required <code>name</code> <code>str</code> <p>Name of the field for error messages</p> <code>'value'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If value is outside the range</p> Source code in <code>src/holodeck/config/validator.py</code> <pre><code>def validate_range(\n    value: float, min_val: float, max_val: float, name: str = \"value\"\n) -&gt; None:\n    \"\"\"Validate that a numeric value is within a range.\n\n    Args:\n        value: Value to validate\n        min_val: Minimum allowed value (inclusive)\n        max_val: Maximum allowed value (inclusive)\n        name: Name of the field for error messages\n\n    Raises:\n        ValueError: If value is outside the range\n    \"\"\"\n    if not (min_val &lt;= value &lt;= max_val):\n        raise ValueError(f\"{name} must be between {min_val} and {max_val}, got {value}\")\n</code></pre>"},{"location":"api/config-loader/#holodeck.config.validator.validate_enum","title":"<code>validate_enum(value, allowed, name='value')</code>","text":"<p>Validate that a string is one of allowed values.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Value to validate</p> required <code>allowed</code> <code>list[str]</code> <p>List of allowed values</p> required <code>name</code> <code>str</code> <p>Name of the field for error messages</p> <code>'value'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If value is not in allowed list</p> Source code in <code>src/holodeck/config/validator.py</code> <pre><code>def validate_enum(value: str, allowed: list[str], name: str = \"value\") -&gt; None:\n    \"\"\"Validate that a string is one of allowed values.\n\n    Args:\n        value: Value to validate\n        allowed: List of allowed values\n        name: Name of the field for error messages\n\n    Raises:\n        ValueError: If value is not in allowed list\n    \"\"\"\n    if value not in allowed:\n        raise ValueError(f\"{name} must be one of {allowed}, got '{value}'\")\n</code></pre>"},{"location":"api/config-loader/#holodeck.config.validator.validate_path_exists","title":"<code>validate_path_exists(path, description='file')</code>","text":"<p>Validate that a file or directory exists.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to validate</p> required <code>description</code> <code>str</code> <p>Description of path for error messages</p> <code>'file'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If path does not exist</p> Source code in <code>src/holodeck/config/validator.py</code> <pre><code>def validate_path_exists(path: str, description: str = \"file\") -&gt; None:\n    \"\"\"Validate that a file or directory exists.\n\n    Args:\n        path: Path to validate\n        description: Description of path for error messages\n\n    Raises:\n        ValueError: If path does not exist\n    \"\"\"\n    from pathlib import Path\n\n    if not Path(path).exists():\n        raise ValueError(f\"Path does not exist: {path}\")\n</code></pre>"},{"location":"api/config-loader/#default-configuration","title":"Default Configuration","text":"<p>Utilities for generating default configuration templates for common components.</p>"},{"location":"api/config-loader/#holodeck.config.defaults.get_default_model_config","title":"<code>get_default_model_config(provider='openai')</code>","text":"<p>Get default model configuration for a provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>LLM provider name (openai, azure_openai, anthropic, ollama)</p> <code>'openai'</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with default model configuration</p> Source code in <code>src/holodeck/config/defaults.py</code> <pre><code>def get_default_model_config(provider: str = \"openai\") -&gt; dict[str, Any]:\n    \"\"\"Get default model configuration for a provider.\n\n    Args:\n        provider: LLM provider name (openai, azure_openai, anthropic, ollama)\n\n    Returns:\n        Dictionary with default model configuration\n    \"\"\"\n    defaults = {\n        \"openai\": {\n            \"provider\": \"openai\",\n            \"name\": \"gpt-4o-mini\",\n            \"temperature\": 0.7,\n            \"max_tokens\": 2048,\n        },\n        \"azure_openai\": {\n            \"provider\": \"azure_openai\",\n            \"name\": \"gpt-4o\",\n            \"temperature\": 0.7,\n            \"max_tokens\": 2048,\n        },\n        \"anthropic\": {\n            \"provider\": \"anthropic\",\n            \"name\": \"claude-3-haiku-20240307\",\n            \"temperature\": 0.7,\n            \"max_tokens\": 2048,\n        },\n        \"ollama\": {\n            \"provider\": \"ollama\",\n            \"endpoint\": \"http://localhost:11434\",\n            \"temperature\": 0.3,\n            \"max_tokens\": 1000,\n            \"top_p\": None,\n            \"api_key\": None,\n        },\n    }\n    return defaults.get(provider, defaults[\"openai\"])\n</code></pre>"},{"location":"api/config-loader/#holodeck.config.defaults.get_default_tool_config","title":"<code>get_default_tool_config(tool_type=None)</code>","text":"<p>Get default configuration template for a tool type.</p> <p>Parameters:</p> Name Type Description Default <code>tool_type</code> <code>str | None</code> <p>Tool type (vectorstore, function, mcp, prompt). If None, returns generic.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with default tool configuration</p> Source code in <code>src/holodeck/config/defaults.py</code> <pre><code>def get_default_tool_config(tool_type: str | None = None) -&gt; dict[str, Any]:\n    \"\"\"Get default configuration template for a tool type.\n\n    Args:\n        tool_type: Tool type (vectorstore, function, mcp, prompt).\n            If None, returns generic.\n\n    Returns:\n        Dictionary with default tool configuration\n    \"\"\"\n    if tool_type is None:\n        return {\"type\": \"function\"}\n\n    defaults: dict[str, dict[str, Any]] = {\n        \"vectorstore\": {\n            \"type\": \"vectorstore\",\n            \"source\": \"\",\n            \"embedding_model\": \"text-embedding-3-small\",\n        },\n        \"function\": {\n            \"type\": \"function\",\n            \"file\": \"\",\n            \"function\": \"\",\n        },\n        \"mcp\": {\n            \"type\": \"mcp\",\n            \"server\": \"\",\n        },\n        \"prompt\": {\n            \"type\": \"prompt\",\n            \"template\": \"\",\n            \"parameters\": {},\n        },\n    }\n    return defaults.get(tool_type, {})\n</code></pre>"},{"location":"api/config-loader/#holodeck.config.defaults.get_default_evaluation_config","title":"<code>get_default_evaluation_config(metric_name=None)</code>","text":"<p>Get default evaluation configuration.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str | None</code> <p>Specific metric name. If None, returns generic structure.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with default evaluation configuration</p> Source code in <code>src/holodeck/config/defaults.py</code> <pre><code>def get_default_evaluation_config(metric_name: str | None = None) -&gt; dict[str, Any]:\n    \"\"\"Get default evaluation configuration.\n\n    Args:\n        metric_name: Specific metric name. If None, returns generic structure.\n\n    Returns:\n        Dictionary with default evaluation configuration\n    \"\"\"\n    # Default per-metric configs\n    metric_defaults = {\n        \"groundedness\": {\n            \"metric\": \"groundedness\",\n            \"threshold\": 4.0,\n            \"enabled\": True,\n            \"scale\": 5,\n        },\n        \"relevance\": {\n            \"metric\": \"relevance\",\n            \"threshold\": 4.0,\n            \"enabled\": True,\n            \"scale\": 5,\n        },\n        \"coherence\": {\n            \"metric\": \"coherence\",\n            \"threshold\": 3.5,\n            \"enabled\": True,\n            \"scale\": 5,\n        },\n        \"safety\": {\n            \"metric\": \"safety\",\n            \"threshold\": 4.0,\n            \"enabled\": True,\n            \"scale\": 5,\n        },\n        \"f1_score\": {\n            \"metric\": \"f1_score\",\n            \"threshold\": 0.85,\n            \"enabled\": True,\n        },\n        \"bleu\": {\n            \"metric\": \"bleu\",\n            \"threshold\": 0.7,\n            \"enabled\": True,\n        },\n        \"rouge\": {\n            \"metric\": \"rouge\",\n            \"threshold\": 0.7,\n            \"enabled\": True,\n        },\n    }\n    if metric_name is None:\n        return {\n            \"metrics\": [\n                {\"metric\": \"groundedness\", \"threshold\": 4.0},\n                {\"metric\": \"relevance\", \"threshold\": 4.0},\n            ]\n        }\n    return metric_defaults.get(metric_name, {})\n</code></pre>"},{"location":"api/config-loader/#configuration-merging","title":"Configuration Merging","text":""},{"location":"api/config-loader/#holodeck.config.merge.ConfigMerger","title":"<code>ConfigMerger</code>","text":"<p>Merges configurations with proper precedence and inheritance rules.</p>"},{"location":"api/config-loader/#holodeck.config.merge.ConfigMerger.merge_agent_with_global","title":"<code>merge_agent_with_global(agent_config, global_config)</code>  <code>staticmethod</code>","text":"<p>Merge agent configuration with global configuration.</p> <p>Agent-level settings take precedence. When inherit_global is False, only agent settings are used.</p> <p>Parameters:</p> Name Type Description Default <code>agent_config</code> <code>dict[str, Any]</code> <p>Agent configuration from agent.yaml</p> required <code>global_config</code> <code>GlobalConfig | None</code> <p>Merged global configuration (user + project level)</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Merged configuration dict with agent settings taking precedence</p> Source code in <code>src/holodeck/config/merge.py</code> <pre><code>@staticmethod\ndef merge_agent_with_global(\n    agent_config: dict[str, Any], global_config: GlobalConfig | None\n) -&gt; dict[str, Any]:\n    \"\"\"Merge agent configuration with global configuration.\n\n    Agent-level settings take precedence. When inherit_global is False,\n    only agent settings are used.\n\n    Args:\n        agent_config: Agent configuration from agent.yaml\n        global_config: Merged global configuration (user + project level)\n\n    Returns:\n        Merged configuration dict with agent settings taking precedence\n    \"\"\"\n    # If inherit_global is explicitly false, return agent config as-is\n    if agent_config.get(\"inherit_global\") is False:\n        logger.info(\"inherit_global set to false; using only agent configuration\")\n        # Remove the inherit_global flag from the config\n        merged = dict(agent_config)\n        merged.pop(\"inherit_global\", None)\n        return merged\n\n    # If no global config or agent has explicit config, use agent config\n    if global_config is None:\n        merged = dict(agent_config)\n        merged.pop(\"inherit_global\", None)\n        return merged\n\n    # Merge global config (base) with agent config (override)\n    global_dict = global_config.model_dump()\n    agent_dict = dict(agent_config)\n    agent_dict.pop(\"inherit_global\", None)\n\n    # For agent-level properties (response_format, tools, etc),\n    # agent config completely overrides global\n    merged_dict = ConfigMerger._deep_merge_dicts(global_dict, agent_dict)\n\n    return merged_dict\n</code></pre>"},{"location":"api/config-loader/#holodeck.config.merge.ConfigMerger.merge_global_configs","title":"<code>merge_global_configs(user_config, project_config)</code>  <code>staticmethod</code>","text":"<p>Merge user-level and project-level global configurations.</p> <p>Project-level config overrides user-level config when both are present.</p> <p>Parameters:</p> Name Type Description Default <code>user_config</code> <code>GlobalConfig | None</code> <p>Global configuration from ~/.holodeck/config.yml|yaml</p> required <code>project_config</code> <code>GlobalConfig | None</code> <p>Global configuration from project root config.yml|yaml</p> required <p>Returns:</p> Type Description <code>GlobalConfig | None</code> <p>Merged GlobalConfig instance, or None if neither config exists</p> Source code in <code>src/holodeck/config/merge.py</code> <pre><code>@staticmethod\ndef merge_global_configs(\n    user_config: GlobalConfig | None, project_config: GlobalConfig | None\n) -&gt; GlobalConfig | None:\n    \"\"\"Merge user-level and project-level global configurations.\n\n    Project-level config overrides user-level config when both are present.\n\n    Args:\n        user_config: Global configuration from ~/.holodeck/config.yml|yaml\n        project_config: Global configuration from project root config.yml|yaml\n\n    Returns:\n        Merged GlobalConfig instance, or None if neither config exists\n    \"\"\"\n    if user_config is None and project_config is None:\n        return None\n\n    if user_config is None:\n        return project_config\n\n    if project_config is None:\n        return user_config\n\n    # Merge project config (override) into user config (base)\n    user_dict = user_config.model_dump()\n    project_dict = project_config.model_dump()\n\n    merged_dict = ConfigMerger._deep_merge_dicts(user_dict, project_dict)\n    return GlobalConfig(**merged_dict)\n</code></pre>"},{"location":"api/config-loader/#related-documentation","title":"Related Documentation","text":"<ul> <li>Data Models: Configuration model definitions</li> <li>CLI Commands: CLI API reference</li> <li>YAML Schema: Agent configuration YAML format</li> </ul>"},{"location":"api/evaluators/","title":"Evaluation Framework API","text":"<p>HoloDeck provides a flexible evaluation framework for measuring agent response quality. The framework supports three tiers of metrics:</p> <ol> <li>DeepEval Metrics (Recommended) - LLM-as-a-judge with GEval and RAG metrics</li> <li>NLP Metrics (Standard) - Algorithmic text comparison</li> <li>Legacy AI Metrics (Deprecated) - Azure AI-based metrics</li> </ol>"},{"location":"api/evaluators/#evaluation-configuration-models","title":"Evaluation Configuration Models","text":""},{"location":"api/evaluators/#holodeck.models.evaluation.EvaluationConfig","title":"<code>EvaluationConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Evaluation framework configuration.</p> <p>Container for evaluation metrics with optional default model configuration. Supports standard EvaluationMetric, GEvalMetric (custom criteria), and RAGMetric (RAG pipeline evaluation).</p>"},{"location":"api/evaluators/#holodeck.models.evaluation.EvaluationConfig.validate_metrics","title":"<code>validate_metrics(v)</code>  <code>classmethod</code>","text":"<p>Validate metrics list is not empty.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"metrics\")\n@classmethod\ndef validate_metrics(\n    cls, v: list[EvaluationMetric | GEvalMetric | RAGMetric]\n) -&gt; list[EvaluationMetric | GEvalMetric | RAGMetric]:\n    \"\"\"Validate metrics list is not empty.\"\"\"\n    if not v:\n        raise ValueError(\"metrics must have at least one metric\")\n    return v\n</code></pre>"},{"location":"api/evaluators/#holodeck.models.evaluation.MetricType","title":"<code>MetricType = Annotated[EvaluationMetric | GEvalMetric | RAGMetric, Field(discriminator='type')]</code>  <code>module-attribute</code>","text":""},{"location":"api/evaluators/#holodeck.models.evaluation.GEvalMetric","title":"<code>GEvalMetric</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>G-Eval custom criteria metric configuration.</p> <p>Uses discriminator pattern with type=\"geval\" to distinguish from standard EvaluationMetric instances in a discriminated union.</p> <p>G-Eval enables custom evaluation criteria defined in natural language, using chain-of-thought prompting with LLM-based scoring.</p> Example <p>metric = GEvalMetric( ...     name=\"Professionalism\", ...     criteria=\"Evaluate if the response uses professional language\", ...     threshold=0.7 ... )</p>"},{"location":"api/evaluators/#holodeck.models.evaluation.GEvalMetric.validate_criteria","title":"<code>validate_criteria(v)</code>  <code>classmethod</code>","text":"<p>Validate criteria is not empty.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"criteria\")\n@classmethod\ndef validate_criteria(cls, v: str) -&gt; str:\n    \"\"\"Validate criteria is not empty.\"\"\"\n    if not v or not v.strip():\n        raise ValueError(\"criteria must be a non-empty string\")\n    return v\n</code></pre>"},{"location":"api/evaluators/#holodeck.models.evaluation.GEvalMetric.validate_evaluation_params","title":"<code>validate_evaluation_params(v)</code>  <code>classmethod</code>","text":"<p>Validate evaluation_params contains valid values.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"evaluation_params\")\n@classmethod\ndef validate_evaluation_params(cls, v: list[str]) -&gt; list[str]:\n    \"\"\"Validate evaluation_params contains valid values.\"\"\"\n    if not v:\n        raise ValueError(\"evaluation_params must not be empty\")\n    invalid_params = set(v) - VALID_EVALUATION_PARAMS\n    if invalid_params:\n        raise ValueError(\n            f\"Invalid evaluation_params: {sorted(invalid_params)}. \"\n            f\"Valid options: {sorted(VALID_EVALUATION_PARAMS)}\"\n        )\n    return v\n</code></pre>"},{"location":"api/evaluators/#holodeck.models.evaluation.GEvalMetric.validate_name","title":"<code>validate_name(v)</code>  <code>classmethod</code>","text":"<p>Validate name is not empty.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"name\")\n@classmethod\ndef validate_name(cls, v: str) -&gt; str:\n    \"\"\"Validate name is not empty.\"\"\"\n    if not v or not v.strip():\n        raise ValueError(\"name must be a non-empty string\")\n    return v\n</code></pre>"},{"location":"api/evaluators/#holodeck.models.evaluation.GEvalMetric.validate_threshold","title":"<code>validate_threshold(v)</code>  <code>classmethod</code>","text":"<p>Validate threshold is in valid range.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"threshold\")\n@classmethod\ndef validate_threshold(cls, v: float | None) -&gt; float | None:\n    \"\"\"Validate threshold is in valid range.\"\"\"\n    if v is not None and (v &lt; 0.0 or v &gt; 1.0):\n        raise ValueError(\"threshold must be between 0.0 and 1.0\")\n    return v\n</code></pre>"},{"location":"api/evaluators/#holodeck.models.evaluation.RAGMetric","title":"<code>RAGMetric</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>RAG pipeline evaluation metric configuration.</p> <p>Uses discriminator pattern with type=\"rag\" to distinguish from standard EvaluationMetric and GEvalMetric instances in a discriminated union.</p> <p>RAG metrics evaluate the quality of retrieval-augmented generation pipelines: - Faithfulness: Detects hallucinations by comparing response to context - ContextualRelevancy: Measures relevance of retrieved chunks to query - ContextualPrecision: Evaluates ranking quality of retrieved chunks - ContextualRecall: Measures retrieval completeness against expected output</p> Example <p>metric = RAGMetric( ...     metric_type=RAGMetricType.FAITHFULNESS, ...     threshold=0.8 ... )</p>"},{"location":"api/evaluators/#holodeck.models.evaluation.RAGMetric.validate_threshold","title":"<code>validate_threshold(v)</code>  <code>classmethod</code>","text":"<p>Validate threshold is in valid range.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"threshold\")\n@classmethod\ndef validate_threshold(cls, v: float) -&gt; float:\n    \"\"\"Validate threshold is in valid range.\"\"\"\n    if v &lt; 0.0 or v &gt; 1.0:\n        raise ValueError(\"threshold must be between 0.0 and 1.0\")\n    return v\n</code></pre>"},{"location":"api/evaluators/#holodeck.models.evaluation.EvaluationMetric","title":"<code>EvaluationMetric</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Evaluation metric configuration.</p> <p>Represents a single evaluation metric with flexible model configuration, including per-metric LLM model overrides.</p>"},{"location":"api/evaluators/#holodeck.models.evaluation.EvaluationMetric.validate_custom_prompt","title":"<code>validate_custom_prompt(v)</code>  <code>classmethod</code>","text":"<p>Validate custom_prompt is not empty if provided.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"custom_prompt\")\n@classmethod\ndef validate_custom_prompt(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate custom_prompt is not empty if provided.\"\"\"\n    if v is not None and (not v or not v.strip()):\n        raise ValueError(\"custom_prompt must be non-empty if provided\")\n    return v\n</code></pre>"},{"location":"api/evaluators/#holodeck.models.evaluation.EvaluationMetric.validate_enabled","title":"<code>validate_enabled(v)</code>  <code>classmethod</code>","text":"<p>Validate enabled is boolean.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"enabled\")\n@classmethod\ndef validate_enabled(cls, v: bool) -&gt; bool:\n    \"\"\"Validate enabled is boolean.\"\"\"\n    if not isinstance(v, bool):\n        raise ValueError(\"enabled must be boolean\")\n    return v\n</code></pre>"},{"location":"api/evaluators/#holodeck.models.evaluation.EvaluationMetric.validate_fail_on_error","title":"<code>validate_fail_on_error(v)</code>  <code>classmethod</code>","text":"<p>Validate fail_on_error is boolean.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"fail_on_error\")\n@classmethod\ndef validate_fail_on_error(cls, v: bool) -&gt; bool:\n    \"\"\"Validate fail_on_error is boolean.\"\"\"\n    if not isinstance(v, bool):\n        raise ValueError(\"fail_on_error must be boolean\")\n    return v\n</code></pre>"},{"location":"api/evaluators/#holodeck.models.evaluation.EvaluationMetric.validate_metric","title":"<code>validate_metric(v)</code>  <code>classmethod</code>","text":"<p>Validate metric is not empty.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"metric\")\n@classmethod\ndef validate_metric(cls, v: str) -&gt; str:\n    \"\"\"Validate metric is not empty.\"\"\"\n    if not v or not v.strip():\n        raise ValueError(\"metric must be a non-empty string\")\n    return v\n</code></pre>"},{"location":"api/evaluators/#holodeck.models.evaluation.EvaluationMetric.validate_retry_on_failure","title":"<code>validate_retry_on_failure(v)</code>  <code>classmethod</code>","text":"<p>Validate retry_on_failure is in valid range.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"retry_on_failure\")\n@classmethod\ndef validate_retry_on_failure(cls, v: int | None) -&gt; int | None:\n    \"\"\"Validate retry_on_failure is in valid range.\"\"\"\n    if v is not None and (v &lt; 1 or v &gt; 3):\n        raise ValueError(\"retry_on_failure must be between 1 and 3\")\n    return v\n</code></pre>"},{"location":"api/evaluators/#holodeck.models.evaluation.EvaluationMetric.validate_scale","title":"<code>validate_scale(v)</code>  <code>classmethod</code>","text":"<p>Validate scale is positive.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"scale\")\n@classmethod\ndef validate_scale(cls, v: int | None) -&gt; int | None:\n    \"\"\"Validate scale is positive.\"\"\"\n    if v is not None and v &lt;= 0:\n        raise ValueError(\"scale must be positive\")\n    return v\n</code></pre>"},{"location":"api/evaluators/#holodeck.models.evaluation.EvaluationMetric.validate_threshold","title":"<code>validate_threshold(v)</code>  <code>classmethod</code>","text":"<p>Validate threshold is numeric if provided.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"threshold\")\n@classmethod\ndef validate_threshold(cls, v: float | None) -&gt; float | None:\n    \"\"\"Validate threshold is numeric if provided.\"\"\"\n    if v is not None and not isinstance(v, int | float):\n        raise ValueError(\"threshold must be numeric\")\n    return v\n</code></pre>"},{"location":"api/evaluators/#holodeck.models.evaluation.EvaluationMetric.validate_timeout_ms","title":"<code>validate_timeout_ms(v)</code>  <code>classmethod</code>","text":"<p>Validate timeout_ms is positive.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"timeout_ms\")\n@classmethod\ndef validate_timeout_ms(cls, v: int | None) -&gt; int | None:\n    \"\"\"Validate timeout_ms is positive.\"\"\"\n    if v is not None and v &lt;= 0:\n        raise ValueError(\"timeout_ms must be positive\")\n    return v\n</code></pre>"},{"location":"api/evaluators/#deepeval-evaluators-recommended","title":"DeepEval Evaluators (Recommended)","text":"<p>DeepEval provides powerful LLM-as-a-judge evaluation using the DeepEval library.</p>"},{"location":"api/evaluators/#base-classes","title":"Base Classes","text":""},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.DeepEvalBaseEvaluator","title":"<code>DeepEvalBaseEvaluator(model_config=None, threshold=0.5, timeout=60.0, retry_config=None)</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Abstract base class for DeepEval-based evaluators.</p> <p>This class extends BaseEvaluator to provide DeepEval-specific functionality: - Model configuration and initialization - LLMTestCase construction from evaluation inputs - Result normalization and logging</p> <p>Subclasses must implement _create_metric() to return the specific DeepEval metric instance.</p> <p>Note: DeepEval uses different parameter names than Azure AI/NLP: - input (not query) - actual_output (not response) - expected_output (not ground_truth)</p> <p>Attributes:</p> Name Type Description <code>model_config</code> <p>Configuration for the evaluation LLM</p> <code>threshold</code> <p>Score threshold for pass/fail determination</p> <code>model</code> <p>The initialized DeepEval model instance</p> Example <p>class MyMetricEvaluator(DeepEvalBaseEvaluator): ...     def _create_metric(self): ...         return SomeDeepEvalMetric( ...             threshold=self._threshold, ...             model=self._model ...         )</p> <p>evaluator = MyMetricEvaluator(threshold=0.7) result = await evaluator.evaluate( ...     input=\"What is Python?\", ...     actual_output=\"Python is a programming language.\" ... )</p> <p>Initialize DeepEval base evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>DeepEvalModelConfig | None</code> <p>Configuration for the evaluation model.          Defaults to Ollama with gpt-oss:20b.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Score threshold for pass/fail (0.0-1.0, default: 0.5)</p> <code>0.5</code> <code>timeout</code> <code>float | None</code> <p>Evaluation timeout in seconds (default: 60.0)</p> <code>60.0</code> <code>retry_config</code> <code>RetryConfig | None</code> <p>Retry configuration for transient failures</p> <code>None</code> Source code in <code>src/holodeck/lib/evaluators/deepeval/base.py</code> <pre><code>def __init__(\n    self,\n    model_config: DeepEvalModelConfig | None = None,\n    threshold: float = 0.5,\n    timeout: float | None = 60.0,\n    retry_config: RetryConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize DeepEval base evaluator.\n\n    Args:\n        model_config: Configuration for the evaluation model.\n                     Defaults to Ollama with gpt-oss:20b.\n        threshold: Score threshold for pass/fail (0.0-1.0, default: 0.5)\n        timeout: Evaluation timeout in seconds (default: 60.0)\n        retry_config: Retry configuration for transient failures\n    \"\"\"\n    super().__init__(timeout=timeout, retry_config=retry_config)\n    self._model_config = model_config or DeepEvalModelConfig()\n    self._model = self._model_config.to_deepeval_model()\n    self._threshold = threshold\n\n    logger.debug(\n        f\"DeepEval evaluator initialized: {self.name}, \"\n        f\"provider={self._model_config.provider.value}, \"\n        f\"model={self._model_config.model_name}, \"\n        f\"threshold={threshold}\"\n    )\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.DeepEvalBaseEvaluator.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return evaluator name (class name by default).</p>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.DeepEvalBaseEvaluator.evaluate","title":"<code>evaluate(**kwargs)</code>  <code>async</code>","text":"<p>Evaluate with timeout and retry logic.</p> <p>This is the main public interface for evaluation. It wraps the implementation with timeout and retry handling.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Evaluation parameters (query, response, context, ground_truth, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Evaluation result dictionary</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If evaluation exceeds timeout</p> <code>EvaluationError</code> <p>If evaluation fails after retries</p> Example <p>evaluator = MyEvaluator(timeout=30.0) result = await evaluator.evaluate( ...     query=\"What is the capital of France?\", ...     response=\"The capital of France is Paris.\", ...     context=\"France is a country in Europe.\", ...     ground_truth=\"Paris\" ... ) print(result[\"score\"]) 0.95</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>async def evaluate(self, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Evaluate with timeout and retry logic.\n\n    This is the main public interface for evaluation. It wraps the\n    implementation with timeout and retry handling.\n\n    Args:\n        **kwargs: Evaluation parameters\n            (query, response, context, ground_truth, etc.)\n\n    Returns:\n        Evaluation result dictionary\n\n    Raises:\n        asyncio.TimeoutError: If evaluation exceeds timeout\n        EvaluationError: If evaluation fails after retries\n\n    Example:\n        &gt;&gt;&gt; evaluator = MyEvaluator(timeout=30.0)\n        &gt;&gt;&gt; result = await evaluator.evaluate(\n        ...     query=\"What is the capital of France?\",\n        ...     response=\"The capital of France is Paris.\",\n        ...     context=\"France is a country in Europe.\",\n        ...     ground_truth=\"Paris\"\n        ... )\n        &gt;&gt;&gt; print(result[\"score\"])\n        0.95\n    \"\"\"\n    logger.debug(f\"Starting evaluation: {self.name} (timeout={self.timeout}s)\")\n\n    if self.timeout is None:\n        # No timeout - evaluate directly with retry\n        logger.debug(f\"Evaluation {self.name}: no timeout\")\n        return await self._evaluate_with_retry(**kwargs)\n\n    # Apply timeout using asyncio.wait_for\n    try:\n        logger.debug(f\"Evaluation {self.name}: applying timeout of {self.timeout}s\")\n        return await asyncio.wait_for(\n            self._evaluate_with_retry(**kwargs), timeout=self.timeout\n        )\n    except TimeoutError:\n        logger.error(f\"Evaluation {self.name} exceeded timeout of {self.timeout}s\")\n        raise  # Re-raise timeout error as-is\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.DeepEvalBaseEvaluator.get_param_spec","title":"<code>get_param_spec()</code>  <code>classmethod</code>","text":"<p>Get the parameter specification for this evaluator.</p> <p>Returns:</p> Type Description <code>ParamSpec</code> <p>ParamSpec declaring required/optional parameters and context flags.</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>@classmethod\ndef get_param_spec(cls) -&gt; ParamSpec:\n    \"\"\"Get the parameter specification for this evaluator.\n\n    Returns:\n        ParamSpec declaring required/optional parameters and context flags.\n    \"\"\"\n    return cls.PARAM_SPEC\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.DeepEvalModelConfig","title":"<code>DeepEvalModelConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration adapter for DeepEval model classes.</p> <p>This class bridges HoloDeck's LLMProvider configuration to DeepEval's native model classes (GPTModel, AzureOpenAIModel, AnthropicModel, OllamaModel).</p> <p>The default configuration uses Ollama with gpt-oss:20b for local evaluation without requiring API keys.</p> <p>Attributes:</p> Name Type Description <code>provider</code> <code>ProviderEnum</code> <p>LLM provider to use (defaults to Ollama)</p> <code>model_name</code> <code>str</code> <p>Name of the model (defaults to gpt-oss:20b)</p> <code>api_key</code> <code>str | None</code> <p>API key for cloud providers (not required for Ollama)</p> <code>endpoint</code> <code>str | None</code> <p>API endpoint URL (required for Azure, optional for Ollama)</p> <code>api_version</code> <code>str | None</code> <p>Azure OpenAI API version</p> <code>deployment_name</code> <code>str | None</code> <p>Azure OpenAI deployment name</p> <code>temperature</code> <code>float</code> <p>Temperature for generation (defaults to 0.0 for determinism)</p> API Key Behavior <ul> <li>OpenAI: API key can be provided via <code>api_key</code> field or the   <code>OPENAI_API_KEY</code> environment variable. If neither is set, DeepEval's   GPTModel will raise an error at runtime.</li> <li>Anthropic: API key can be provided via <code>api_key</code> field or the   <code>ANTHROPIC_API_KEY</code> environment variable. If neither is set,   DeepEval's AnthropicModel will raise an error at runtime.</li> <li>Azure OpenAI: The <code>api_key</code> field is required and validated   at configuration time (no environment variable fallback).</li> <li>Ollama: No API key required (local inference).</li> </ul> Example <p>config = DeepEvalModelConfig()  # Default Ollama model = config.to_deepeval_model()</p> <p>openai_config = DeepEvalModelConfig( ...     provider=ProviderEnum.OPENAI, ...     model_name=\"gpt-4o\", ...     api_key=\"sk-...\"  # Or set OPENAI_API_KEY env var ... )</p>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.DeepEvalModelConfig.to_deepeval_model","title":"<code>to_deepeval_model()</code>","text":"<p>Convert configuration to native DeepEval model class.</p> <p>Returns the appropriate DeepEval model class instance based on the configured provider.</p> <p>Returns:</p> Type Description <code>DeepEvalModel</code> <p>DeepEval model instance (GPTModel, AzureOpenAIModel,</p> <code>DeepEvalModel</code> <p>AnthropicModel, or OllamaModel)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider is not supported</p> Source code in <code>src/holodeck/lib/evaluators/deepeval/config.py</code> <pre><code>def to_deepeval_model(self) -&gt; DeepEvalModel:\n    \"\"\"Convert configuration to native DeepEval model class.\n\n    Returns the appropriate DeepEval model class instance based on\n    the configured provider.\n\n    Returns:\n        DeepEval model instance (GPTModel, AzureOpenAIModel,\n        AnthropicModel, or OllamaModel)\n\n    Raises:\n        ValueError: If provider is not supported\n    \"\"\"\n    if self.provider == ProviderEnum.OPENAI:\n        from deepeval.models import GPTModel\n\n        kwargs: dict[str, Any] = {\n            \"model\": self.model_name,\n            \"temperature\": self.temperature,\n        }\n        if self.api_key:\n            kwargs[\"api_key\"] = self.api_key\n        return GPTModel(**kwargs)\n\n    elif self.provider == ProviderEnum.AZURE_OPENAI:\n        from deepeval.models import AzureOpenAIModel\n\n        return AzureOpenAIModel(\n            model_name=self.model_name,\n            deployment_name=self.deployment_name,\n            azure_endpoint=self.endpoint,\n            openai_api_version=self.api_version,\n            azure_openai_api_key=self.api_key,\n            temperature=1.0,  # reasoning models require temperature=1.0\n        )\n\n    elif self.provider == ProviderEnum.ANTHROPIC:\n        from deepeval.models import AnthropicModel\n\n        kwargs = {\n            \"model\": self.model_name,\n            \"temperature\": self.temperature,\n        }\n        if self.api_key:\n            kwargs[\"api_key\"] = self.api_key\n        return AnthropicModel(**kwargs)\n\n    elif self.provider == ProviderEnum.OLLAMA:\n        from deepeval.models import OllamaModel\n\n        return OllamaModel(\n            model=self.model_name,\n            base_url=self.endpoint or \"http://localhost:11434\",\n            temperature=self.temperature,\n        )\n\n    else:\n        raise ValueError(f\"Unsupported provider: {self.provider}\")\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.DeepEvalModelConfig.validate_provider_requirements","title":"<code>validate_provider_requirements()</code>","text":"<p>Validate that required fields are present for each provider.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields are missing for the provider</p> Source code in <code>src/holodeck/lib/evaluators/deepeval/config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_provider_requirements(self) -&gt; \"DeepEvalModelConfig\":\n    \"\"\"Validate that required fields are present for each provider.\n\n    Raises:\n        ValueError: If required fields are missing for the provider\n    \"\"\"\n    if self.provider == ProviderEnum.AZURE_OPENAI:\n        if not self.endpoint:\n            raise ValueError(\"endpoint is required for Azure OpenAI provider\")\n        if not self.deployment_name:\n            raise ValueError(\n                \"deployment_name is required for Azure OpenAI provider\"\n            )\n        if not self.api_key:\n            raise ValueError(\"api_key is required for Azure OpenAI provider\")\n    return self\n</code></pre>"},{"location":"api/evaluators/#geval-evaluator","title":"GEval Evaluator","text":"<p>The GEval evaluator uses the G-Eval algorithm with chain-of-thought prompting for custom criteria evaluation.</p>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.GEvalEvaluator","title":"<code>GEvalEvaluator(name, criteria, evaluation_params=None, evaluation_steps=None, model_config=None, threshold=0.5, strict_mode=False, timeout=60.0, retry_config=None)</code>","text":"<p>               Bases: <code>DeepEvalBaseEvaluator</code></p> <p>G-Eval custom criteria evaluator.</p> <p>Evaluates LLM outputs against user-defined criteria using the G-Eval algorithm, which combines chain-of-thought prompting with token probability scoring.</p> <p>G-Eval works in two phases: 1. Step Generation: Auto-generates evaluation steps from the criteria 2. Scoring: Uses the steps to score the test case on a 1-5 scale (normalized to 0-1)</p> <p>Attributes:</p> Name Type Description <code>_metric_name</code> <p>Custom name for this evaluation metric</p> <code>_criteria</code> <p>Natural language criteria for evaluation</p> <code>_evaluation_params</code> <p>Test case fields to include in evaluation</p> <code>_evaluation_steps</code> <p>Optional explicit evaluation steps</p> <code>_strict_mode</code> <p>Whether to use binary scoring (1.0 or 0.0)</p> Example <p>evaluator = GEvalEvaluator( ...     name=\"Professionalism\", ...     criteria=\"Evaluate if the response uses professional language\", ...     threshold=0.7 ... ) result = await evaluator.evaluate( ...     input=\"Write me an email\", ...     actual_output=\"Dear Sir/Madam, ...\" ... ) print(result[\"score\"])  # 0.85 print(result[\"passed\"])  # True</p> <p>Initialize G-Eval evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Metric identifier (e.g., \"Correctness\", \"Helpfulness\")</p> required <code>criteria</code> <code>str</code> <p>Natural language evaluation criteria</p> required <code>evaluation_params</code> <code>list[str] | None</code> <p>Test case fields to include in evaluation. Valid options: [\"input\", \"actual_output\", \"expected_output\",               \"context\", \"retrieval_context\"] Default: [\"actual_output\"]</p> <code>None</code> <code>evaluation_steps</code> <code>list[str] | None</code> <p>Explicit evaluation steps. If None, G-Eval auto-generates steps from the criteria.</p> <code>None</code> <code>model_config</code> <code>DeepEvalModelConfig | None</code> <p>LLM judge configuration. Defaults to Ollama gpt-oss:20b.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Pass/fail score threshold (0.0-1.0). Default: 0.5.</p> <code>0.5</code> <code>strict_mode</code> <code>bool</code> <p>If True, scores are binary (1.0 or 0.0). Default: False.</p> <code>False</code> <code>timeout</code> <code>float | None</code> <p>Evaluation timeout in seconds. Default: 60.0.</p> <code>60.0</code> <code>retry_config</code> <code>RetryConfig | None</code> <p>Retry configuration for transient failures.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid evaluation_params are provided.</p> Source code in <code>src/holodeck/lib/evaluators/deepeval/geval.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    criteria: str,\n    evaluation_params: list[str] | None = None,\n    evaluation_steps: list[str] | None = None,\n    model_config: DeepEvalModelConfig | None = None,\n    threshold: float = 0.5,\n    strict_mode: bool = False,\n    timeout: float | None = 60.0,\n    retry_config: RetryConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize G-Eval evaluator.\n\n    Args:\n        name: Metric identifier (e.g., \"Correctness\", \"Helpfulness\")\n        criteria: Natural language evaluation criteria\n        evaluation_params: Test case fields to include in evaluation.\n            Valid options: [\"input\", \"actual_output\", \"expected_output\",\n                          \"context\", \"retrieval_context\"]\n            Default: [\"actual_output\"]\n        evaluation_steps: Explicit evaluation steps. If None, G-Eval\n            auto-generates steps from the criteria.\n        model_config: LLM judge configuration. Defaults to Ollama gpt-oss:20b.\n        threshold: Pass/fail score threshold (0.0-1.0). Default: 0.5.\n        strict_mode: If True, scores are binary (1.0 or 0.0). Default: False.\n        timeout: Evaluation timeout in seconds. Default: 60.0.\n        retry_config: Retry configuration for transient failures.\n\n    Raises:\n        ValueError: If invalid evaluation_params are provided.\n    \"\"\"\n    # Validate and set evaluation params before calling super().__init__\n    if evaluation_params is None:\n        evaluation_params = [\"actual_output\"]\n\n    # Validate evaluation params\n    for param in evaluation_params:\n        if param not in VALID_EVALUATION_PARAMS:\n            raise ValueError(\n                f\"Invalid evaluation_param: '{param}'. \"\n                f\"Valid options: {sorted(VALID_EVALUATION_PARAMS)}\"\n            )\n\n    self._metric_name = name\n    self._criteria = criteria\n    self._evaluation_params = evaluation_params\n    self._evaluation_steps = evaluation_steps\n    self._strict_mode = strict_mode\n\n    super().__init__(\n        model_config=model_config,\n        threshold=threshold,\n        timeout=timeout,\n        retry_config=retry_config,\n    )\n\n    logger.debug(\n        f\"GEvalEvaluator initialized: name={name}, \"\n        f\"criteria_len={len(criteria)}, \"\n        f\"evaluation_params={evaluation_params}, \"\n        f\"strict_mode={strict_mode}\"\n    )\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.GEvalEvaluator.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the custom metric name.</p>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.GEvalEvaluator.evaluate","title":"<code>evaluate(**kwargs)</code>  <code>async</code>","text":"<p>Evaluate with timeout and retry logic.</p> <p>This is the main public interface for evaluation. It wraps the implementation with timeout and retry handling.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Evaluation parameters (query, response, context, ground_truth, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Evaluation result dictionary</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If evaluation exceeds timeout</p> <code>EvaluationError</code> <p>If evaluation fails after retries</p> Example <p>evaluator = MyEvaluator(timeout=30.0) result = await evaluator.evaluate( ...     query=\"What is the capital of France?\", ...     response=\"The capital of France is Paris.\", ...     context=\"France is a country in Europe.\", ...     ground_truth=\"Paris\" ... ) print(result[\"score\"]) 0.95</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>async def evaluate(self, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Evaluate with timeout and retry logic.\n\n    This is the main public interface for evaluation. It wraps the\n    implementation with timeout and retry handling.\n\n    Args:\n        **kwargs: Evaluation parameters\n            (query, response, context, ground_truth, etc.)\n\n    Returns:\n        Evaluation result dictionary\n\n    Raises:\n        asyncio.TimeoutError: If evaluation exceeds timeout\n        EvaluationError: If evaluation fails after retries\n\n    Example:\n        &gt;&gt;&gt; evaluator = MyEvaluator(timeout=30.0)\n        &gt;&gt;&gt; result = await evaluator.evaluate(\n        ...     query=\"What is the capital of France?\",\n        ...     response=\"The capital of France is Paris.\",\n        ...     context=\"France is a country in Europe.\",\n        ...     ground_truth=\"Paris\"\n        ... )\n        &gt;&gt;&gt; print(result[\"score\"])\n        0.95\n    \"\"\"\n    logger.debug(f\"Starting evaluation: {self.name} (timeout={self.timeout}s)\")\n\n    if self.timeout is None:\n        # No timeout - evaluate directly with retry\n        logger.debug(f\"Evaluation {self.name}: no timeout\")\n        return await self._evaluate_with_retry(**kwargs)\n\n    # Apply timeout using asyncio.wait_for\n    try:\n        logger.debug(f\"Evaluation {self.name}: applying timeout of {self.timeout}s\")\n        return await asyncio.wait_for(\n            self._evaluate_with_retry(**kwargs), timeout=self.timeout\n        )\n    except TimeoutError:\n        logger.error(f\"Evaluation {self.name} exceeded timeout of {self.timeout}s\")\n        raise  # Re-raise timeout error as-is\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.GEvalEvaluator.get_param_spec","title":"<code>get_param_spec()</code>  <code>classmethod</code>","text":"<p>Get the parameter specification for this evaluator.</p> <p>Returns:</p> Type Description <code>ParamSpec</code> <p>ParamSpec declaring required/optional parameters and context flags.</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>@classmethod\ndef get_param_spec(cls) -&gt; ParamSpec:\n    \"\"\"Get the parameter specification for this evaluator.\n\n    Returns:\n        ParamSpec declaring required/optional parameters and context flags.\n    \"\"\"\n    return cls.PARAM_SPEC\n</code></pre>"},{"location":"api/evaluators/#rag-evaluators","title":"RAG Evaluators","text":"<p>RAG evaluators measure retrieval-augmented generation pipeline quality.</p>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.FaithfulnessEvaluator","title":"<code>FaithfulnessEvaluator(model_config=None, threshold=0.5, include_reason=True, timeout=60.0, retry_config=None)</code>","text":"<p>               Bases: <code>DeepEvalBaseEvaluator</code></p> <p>Faithfulness evaluator for detecting hallucinations.</p> <p>Detects hallucinations by comparing agent response to retrieval context. Returns a low score if the response contains information not found in the retrieval context (hallucination detected).</p> Required inputs <ul> <li>input: User query</li> <li>actual_output: Agent response</li> <li>retrieval_context: List of retrieved text chunks</li> </ul> Example <p>evaluator = FaithfulnessEvaluator(threshold=0.8) result = await evaluator.evaluate( ...     input=\"What are the store hours?\", ...     actual_output=\"Store is open 24/7.\", ...     retrieval_context=[\"Store hours: Mon-Fri 9am-5pm\"] ... ) print(result[\"score\"])  # Low score (hallucination detected)</p> <p>Attributes:</p> Name Type Description <code>_include_reason</code> <p>Whether to include reasoning in results.</p> <p>Initialize Faithfulness evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>DeepEvalModelConfig | None</code> <p>LLM judge configuration. Defaults to Ollama gpt-oss:20b.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Pass/fail score threshold (0.0-1.0). Default: 0.5.</p> <code>0.5</code> <code>include_reason</code> <code>bool</code> <p>Whether to include reasoning in results. Default: True.</p> <code>True</code> <code>timeout</code> <code>float | None</code> <p>Evaluation timeout in seconds. Default: 60.0.</p> <code>60.0</code> <code>retry_config</code> <code>RetryConfig | None</code> <p>Retry configuration for transient failures.</p> <code>None</code> Source code in <code>src/holodeck/lib/evaluators/deepeval/faithfulness.py</code> <pre><code>def __init__(\n    self,\n    model_config: DeepEvalModelConfig | None = None,\n    threshold: float = 0.5,\n    include_reason: bool = True,\n    timeout: float | None = 60.0,\n    retry_config: RetryConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize Faithfulness evaluator.\n\n    Args:\n        model_config: LLM judge configuration. Defaults to Ollama gpt-oss:20b.\n        threshold: Pass/fail score threshold (0.0-1.0). Default: 0.5.\n        include_reason: Whether to include reasoning in results. Default: True.\n        timeout: Evaluation timeout in seconds. Default: 60.0.\n        retry_config: Retry configuration for transient failures.\n    \"\"\"\n    self._include_reason = include_reason\n\n    super().__init__(\n        model_config=model_config,\n        threshold=threshold,\n        timeout=timeout,\n        retry_config=retry_config,\n    )\n\n    logger.debug(\n        f\"FaithfulnessEvaluator initialized: \"\n        f\"provider={self._model_config.provider.value}, \"\n        f\"model={self._model_config.model_name}, \"\n        f\"threshold={threshold}, include_reason={include_reason}\"\n    )\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.FaithfulnessEvaluator.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the metric name.</p>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.FaithfulnessEvaluator.evaluate","title":"<code>evaluate(**kwargs)</code>  <code>async</code>","text":"<p>Evaluate with timeout and retry logic.</p> <p>This is the main public interface for evaluation. It wraps the implementation with timeout and retry handling.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Evaluation parameters (query, response, context, ground_truth, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Evaluation result dictionary</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If evaluation exceeds timeout</p> <code>EvaluationError</code> <p>If evaluation fails after retries</p> Example <p>evaluator = MyEvaluator(timeout=30.0) result = await evaluator.evaluate( ...     query=\"What is the capital of France?\", ...     response=\"The capital of France is Paris.\", ...     context=\"France is a country in Europe.\", ...     ground_truth=\"Paris\" ... ) print(result[\"score\"]) 0.95</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>async def evaluate(self, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Evaluate with timeout and retry logic.\n\n    This is the main public interface for evaluation. It wraps the\n    implementation with timeout and retry handling.\n\n    Args:\n        **kwargs: Evaluation parameters\n            (query, response, context, ground_truth, etc.)\n\n    Returns:\n        Evaluation result dictionary\n\n    Raises:\n        asyncio.TimeoutError: If evaluation exceeds timeout\n        EvaluationError: If evaluation fails after retries\n\n    Example:\n        &gt;&gt;&gt; evaluator = MyEvaluator(timeout=30.0)\n        &gt;&gt;&gt; result = await evaluator.evaluate(\n        ...     query=\"What is the capital of France?\",\n        ...     response=\"The capital of France is Paris.\",\n        ...     context=\"France is a country in Europe.\",\n        ...     ground_truth=\"Paris\"\n        ... )\n        &gt;&gt;&gt; print(result[\"score\"])\n        0.95\n    \"\"\"\n    logger.debug(f\"Starting evaluation: {self.name} (timeout={self.timeout}s)\")\n\n    if self.timeout is None:\n        # No timeout - evaluate directly with retry\n        logger.debug(f\"Evaluation {self.name}: no timeout\")\n        return await self._evaluate_with_retry(**kwargs)\n\n    # Apply timeout using asyncio.wait_for\n    try:\n        logger.debug(f\"Evaluation {self.name}: applying timeout of {self.timeout}s\")\n        return await asyncio.wait_for(\n            self._evaluate_with_retry(**kwargs), timeout=self.timeout\n        )\n    except TimeoutError:\n        logger.error(f\"Evaluation {self.name} exceeded timeout of {self.timeout}s\")\n        raise  # Re-raise timeout error as-is\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.FaithfulnessEvaluator.get_param_spec","title":"<code>get_param_spec()</code>  <code>classmethod</code>","text":"<p>Get the parameter specification for this evaluator.</p> <p>Returns:</p> Type Description <code>ParamSpec</code> <p>ParamSpec declaring required/optional parameters and context flags.</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>@classmethod\ndef get_param_spec(cls) -&gt; ParamSpec:\n    \"\"\"Get the parameter specification for this evaluator.\n\n    Returns:\n        ParamSpec declaring required/optional parameters and context flags.\n    \"\"\"\n    return cls.PARAM_SPEC\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.AnswerRelevancyEvaluator","title":"<code>AnswerRelevancyEvaluator(model_config=None, threshold=0.5, include_reason=True, strict_mode=False, timeout=60.0, retry_config=None)</code>","text":"<p>               Bases: <code>DeepEvalBaseEvaluator</code></p> <p>Answer Relevancy evaluator - measures statement relevance to input.</p> <p>Evaluates how relevant the response statements are to the input query. Unlike other RAG metrics, this does NOT require retrieval_context.</p> Required inputs <ul> <li>input: User query</li> <li>actual_output: Agent response</li> </ul> Example <p>evaluator = AnswerRelevancyEvaluator(threshold=0.7) result = await evaluator.evaluate( ...     input=\"What is the return policy?\", ...     actual_output=\"We offer a 30-day full refund at no extra cost.\" ... ) print(result[\"score\"])  # High score if relevant</p> <p>Attributes:</p> Name Type Description <code>_include_reason</code> <p>Whether to include reasoning in results.</p> <code>_strict_mode</code> <p>Whether to use binary scoring (1.0 or 0.0).</p> <p>Initialize Answer Relevancy evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>DeepEvalModelConfig | None</code> <p>LLM judge configuration. Defaults to Ollama gpt-oss:20b.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Pass/fail score threshold (0.0-1.0). Default: 0.5.</p> <code>0.5</code> <code>include_reason</code> <code>bool</code> <p>Whether to include reasoning in results. Default: True.</p> <code>True</code> <code>strict_mode</code> <code>bool</code> <p>Binary scoring mode (1.0 or 0.0 only). Default: False.</p> <code>False</code> <code>timeout</code> <code>float | None</code> <p>Evaluation timeout in seconds. Default: 60.0.</p> <code>60.0</code> <code>retry_config</code> <code>RetryConfig | None</code> <p>Retry configuration for transient failures.</p> <code>None</code> Source code in <code>src/holodeck/lib/evaluators/deepeval/answer_relevancy.py</code> <pre><code>def __init__(\n    self,\n    model_config: DeepEvalModelConfig | None = None,\n    threshold: float = 0.5,\n    include_reason: bool = True,\n    strict_mode: bool = False,\n    timeout: float | None = 60.0,\n    retry_config: RetryConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize Answer Relevancy evaluator.\n\n    Args:\n        model_config: LLM judge configuration. Defaults to Ollama gpt-oss:20b.\n        threshold: Pass/fail score threshold (0.0-1.0). Default: 0.5.\n        include_reason: Whether to include reasoning in results. Default: True.\n        strict_mode: Binary scoring mode (1.0 or 0.0 only). Default: False.\n        timeout: Evaluation timeout in seconds. Default: 60.0.\n        retry_config: Retry configuration for transient failures.\n    \"\"\"\n    self._include_reason = include_reason\n    self._strict_mode = strict_mode\n\n    super().__init__(\n        model_config=model_config,\n        threshold=threshold,\n        timeout=timeout,\n        retry_config=retry_config,\n    )\n\n    logger.debug(\n        f\"AnswerRelevancyEvaluator initialized: \"\n        f\"provider={self._model_config.provider.value}, \"\n        f\"model={self._model_config.model_name}, \"\n        f\"threshold={threshold}, include_reason={include_reason}, \"\n        f\"strict_mode={strict_mode}\"\n    )\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.AnswerRelevancyEvaluator.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the metric name.</p>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.AnswerRelevancyEvaluator.evaluate","title":"<code>evaluate(**kwargs)</code>  <code>async</code>","text":"<p>Evaluate with timeout and retry logic.</p> <p>This is the main public interface for evaluation. It wraps the implementation with timeout and retry handling.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Evaluation parameters (query, response, context, ground_truth, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Evaluation result dictionary</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If evaluation exceeds timeout</p> <code>EvaluationError</code> <p>If evaluation fails after retries</p> Example <p>evaluator = MyEvaluator(timeout=30.0) result = await evaluator.evaluate( ...     query=\"What is the capital of France?\", ...     response=\"The capital of France is Paris.\", ...     context=\"France is a country in Europe.\", ...     ground_truth=\"Paris\" ... ) print(result[\"score\"]) 0.95</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>async def evaluate(self, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Evaluate with timeout and retry logic.\n\n    This is the main public interface for evaluation. It wraps the\n    implementation with timeout and retry handling.\n\n    Args:\n        **kwargs: Evaluation parameters\n            (query, response, context, ground_truth, etc.)\n\n    Returns:\n        Evaluation result dictionary\n\n    Raises:\n        asyncio.TimeoutError: If evaluation exceeds timeout\n        EvaluationError: If evaluation fails after retries\n\n    Example:\n        &gt;&gt;&gt; evaluator = MyEvaluator(timeout=30.0)\n        &gt;&gt;&gt; result = await evaluator.evaluate(\n        ...     query=\"What is the capital of France?\",\n        ...     response=\"The capital of France is Paris.\",\n        ...     context=\"France is a country in Europe.\",\n        ...     ground_truth=\"Paris\"\n        ... )\n        &gt;&gt;&gt; print(result[\"score\"])\n        0.95\n    \"\"\"\n    logger.debug(f\"Starting evaluation: {self.name} (timeout={self.timeout}s)\")\n\n    if self.timeout is None:\n        # No timeout - evaluate directly with retry\n        logger.debug(f\"Evaluation {self.name}: no timeout\")\n        return await self._evaluate_with_retry(**kwargs)\n\n    # Apply timeout using asyncio.wait_for\n    try:\n        logger.debug(f\"Evaluation {self.name}: applying timeout of {self.timeout}s\")\n        return await asyncio.wait_for(\n            self._evaluate_with_retry(**kwargs), timeout=self.timeout\n        )\n    except TimeoutError:\n        logger.error(f\"Evaluation {self.name} exceeded timeout of {self.timeout}s\")\n        raise  # Re-raise timeout error as-is\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.AnswerRelevancyEvaluator.get_param_spec","title":"<code>get_param_spec()</code>  <code>classmethod</code>","text":"<p>Get the parameter specification for this evaluator.</p> <p>Returns:</p> Type Description <code>ParamSpec</code> <p>ParamSpec declaring required/optional parameters and context flags.</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>@classmethod\ndef get_param_spec(cls) -&gt; ParamSpec:\n    \"\"\"Get the parameter specification for this evaluator.\n\n    Returns:\n        ParamSpec declaring required/optional parameters and context flags.\n    \"\"\"\n    return cls.PARAM_SPEC\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.ContextualRelevancyEvaluator","title":"<code>ContextualRelevancyEvaluator(model_config=None, threshold=0.5, include_reason=True, timeout=60.0, retry_config=None)</code>","text":"<p>               Bases: <code>DeepEvalBaseEvaluator</code></p> <p>Contextual Relevancy evaluator for RAG pipelines.</p> <p>Measures the relevance of retrieved context to the user query. Returns the proportion of chunks that are relevant to the query.</p> Required inputs <ul> <li>input: User query</li> <li>actual_output: Agent response</li> <li>retrieval_context: List of retrieved text chunks</li> </ul> Example <p>evaluator = ContextualRelevancyEvaluator(threshold=0.6) result = await evaluator.evaluate( ...     input=\"What is the pricing?\", ...     actual_output=\"Basic plan is $10/month.\", ...     retrieval_context=[ ...         \"Pricing: Basic $10, Pro $25\",  # Relevant ...         \"Company founded in 2020\",       # Irrelevant ...     ] ... ) print(result[\"score\"])  # 0.5 (1 of 2 chunks relevant)</p> <p>Attributes:</p> Name Type Description <code>_include_reason</code> <p>Whether to include reasoning in results.</p> <p>Initialize Contextual Relevancy evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>DeepEvalModelConfig | None</code> <p>LLM judge configuration. Defaults to Ollama gpt-oss:20b.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Pass/fail score threshold (0.0-1.0). Default: 0.5.</p> <code>0.5</code> <code>include_reason</code> <code>bool</code> <p>Whether to include reasoning in results. Default: True.</p> <code>True</code> <code>timeout</code> <code>float | None</code> <p>Evaluation timeout in seconds. Default: 60.0.</p> <code>60.0</code> <code>retry_config</code> <code>RetryConfig | None</code> <p>Retry configuration for transient failures.</p> <code>None</code> Source code in <code>src/holodeck/lib/evaluators/deepeval/contextual_relevancy.py</code> <pre><code>def __init__(\n    self,\n    model_config: DeepEvalModelConfig | None = None,\n    threshold: float = 0.5,\n    include_reason: bool = True,\n    timeout: float | None = 60.0,\n    retry_config: RetryConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize Contextual Relevancy evaluator.\n\n    Args:\n        model_config: LLM judge configuration. Defaults to Ollama gpt-oss:20b.\n        threshold: Pass/fail score threshold (0.0-1.0). Default: 0.5.\n        include_reason: Whether to include reasoning in results. Default: True.\n        timeout: Evaluation timeout in seconds. Default: 60.0.\n        retry_config: Retry configuration for transient failures.\n    \"\"\"\n    self._include_reason = include_reason\n\n    super().__init__(\n        model_config=model_config,\n        threshold=threshold,\n        timeout=timeout,\n        retry_config=retry_config,\n    )\n\n    logger.debug(\n        f\"ContextualRelevancyEvaluator initialized: \"\n        f\"provider={self._model_config.provider.value}, \"\n        f\"model={self._model_config.model_name}, \"\n        f\"threshold={threshold}, include_reason={include_reason}\"\n    )\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.ContextualRelevancyEvaluator.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the metric name.</p>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.ContextualRelevancyEvaluator.evaluate","title":"<code>evaluate(**kwargs)</code>  <code>async</code>","text":"<p>Evaluate with timeout and retry logic.</p> <p>This is the main public interface for evaluation. It wraps the implementation with timeout and retry handling.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Evaluation parameters (query, response, context, ground_truth, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Evaluation result dictionary</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If evaluation exceeds timeout</p> <code>EvaluationError</code> <p>If evaluation fails after retries</p> Example <p>evaluator = MyEvaluator(timeout=30.0) result = await evaluator.evaluate( ...     query=\"What is the capital of France?\", ...     response=\"The capital of France is Paris.\", ...     context=\"France is a country in Europe.\", ...     ground_truth=\"Paris\" ... ) print(result[\"score\"]) 0.95</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>async def evaluate(self, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Evaluate with timeout and retry logic.\n\n    This is the main public interface for evaluation. It wraps the\n    implementation with timeout and retry handling.\n\n    Args:\n        **kwargs: Evaluation parameters\n            (query, response, context, ground_truth, etc.)\n\n    Returns:\n        Evaluation result dictionary\n\n    Raises:\n        asyncio.TimeoutError: If evaluation exceeds timeout\n        EvaluationError: If evaluation fails after retries\n\n    Example:\n        &gt;&gt;&gt; evaluator = MyEvaluator(timeout=30.0)\n        &gt;&gt;&gt; result = await evaluator.evaluate(\n        ...     query=\"What is the capital of France?\",\n        ...     response=\"The capital of France is Paris.\",\n        ...     context=\"France is a country in Europe.\",\n        ...     ground_truth=\"Paris\"\n        ... )\n        &gt;&gt;&gt; print(result[\"score\"])\n        0.95\n    \"\"\"\n    logger.debug(f\"Starting evaluation: {self.name} (timeout={self.timeout}s)\")\n\n    if self.timeout is None:\n        # No timeout - evaluate directly with retry\n        logger.debug(f\"Evaluation {self.name}: no timeout\")\n        return await self._evaluate_with_retry(**kwargs)\n\n    # Apply timeout using asyncio.wait_for\n    try:\n        logger.debug(f\"Evaluation {self.name}: applying timeout of {self.timeout}s\")\n        return await asyncio.wait_for(\n            self._evaluate_with_retry(**kwargs), timeout=self.timeout\n        )\n    except TimeoutError:\n        logger.error(f\"Evaluation {self.name} exceeded timeout of {self.timeout}s\")\n        raise  # Re-raise timeout error as-is\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.ContextualRelevancyEvaluator.get_param_spec","title":"<code>get_param_spec()</code>  <code>classmethod</code>","text":"<p>Get the parameter specification for this evaluator.</p> <p>Returns:</p> Type Description <code>ParamSpec</code> <p>ParamSpec declaring required/optional parameters and context flags.</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>@classmethod\ndef get_param_spec(cls) -&gt; ParamSpec:\n    \"\"\"Get the parameter specification for this evaluator.\n\n    Returns:\n        ParamSpec declaring required/optional parameters and context flags.\n    \"\"\"\n    return cls.PARAM_SPEC\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.ContextualPrecisionEvaluator","title":"<code>ContextualPrecisionEvaluator(model_config=None, threshold=0.5, include_reason=True, timeout=60.0, retry_config=None)</code>","text":"<p>               Bases: <code>DeepEvalBaseEvaluator</code></p> <p>Contextual Precision evaluator for RAG pipelines.</p> <p>Evaluates the ranking quality of retrieved chunks. Measures whether relevant chunks appear before irrelevant ones.</p> Required inputs <ul> <li>input: User query</li> <li>actual_output: Agent response</li> <li>expected_output: Ground truth answer</li> <li>retrieval_context: List of retrieved text chunks (order matters)</li> </ul> Example <p>evaluator = ContextualPrecisionEvaluator(threshold=0.7) result = await evaluator.evaluate( ...     input=\"What is X?\", ...     actual_output=\"X is...\", ...     expected_output=\"X is the correct definition.\", ...     retrieval_context=[ ...         \"Irrelevant info\",      # Bad: irrelevant first ...         \"X is the definition\",  # Good: relevant ...     ] ... ) print(result[\"score\"])  # Lower due to poor ranking</p> <p>Attributes:</p> Name Type Description <code>_include_reason</code> <p>Whether to include reasoning in results.</p> <p>Initialize Contextual Precision evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>DeepEvalModelConfig | None</code> <p>LLM judge configuration. Defaults to Ollama gpt-oss:20b.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Pass/fail score threshold (0.0-1.0). Default: 0.5.</p> <code>0.5</code> <code>include_reason</code> <code>bool</code> <p>Whether to include reasoning in results. Default: True.</p> <code>True</code> <code>timeout</code> <code>float | None</code> <p>Evaluation timeout in seconds. Default: 60.0.</p> <code>60.0</code> <code>retry_config</code> <code>RetryConfig | None</code> <p>Retry configuration for transient failures.</p> <code>None</code> Source code in <code>src/holodeck/lib/evaluators/deepeval/contextual_precision.py</code> <pre><code>def __init__(\n    self,\n    model_config: DeepEvalModelConfig | None = None,\n    threshold: float = 0.5,\n    include_reason: bool = True,\n    timeout: float | None = 60.0,\n    retry_config: RetryConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize Contextual Precision evaluator.\n\n    Args:\n        model_config: LLM judge configuration. Defaults to Ollama gpt-oss:20b.\n        threshold: Pass/fail score threshold (0.0-1.0). Default: 0.5.\n        include_reason: Whether to include reasoning in results. Default: True.\n        timeout: Evaluation timeout in seconds. Default: 60.0.\n        retry_config: Retry configuration for transient failures.\n    \"\"\"\n    self._include_reason = include_reason\n\n    super().__init__(\n        model_config=model_config,\n        threshold=threshold,\n        timeout=timeout,\n        retry_config=retry_config,\n    )\n\n    logger.debug(\n        f\"ContextualPrecisionEvaluator initialized: \"\n        f\"provider={self._model_config.provider.value}, \"\n        f\"model={self._model_config.model_name}, \"\n        f\"threshold={threshold}, include_reason={include_reason}\"\n    )\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.ContextualPrecisionEvaluator.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the metric name.</p>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.ContextualPrecisionEvaluator.evaluate","title":"<code>evaluate(**kwargs)</code>  <code>async</code>","text":"<p>Evaluate with timeout and retry logic.</p> <p>This is the main public interface for evaluation. It wraps the implementation with timeout and retry handling.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Evaluation parameters (query, response, context, ground_truth, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Evaluation result dictionary</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If evaluation exceeds timeout</p> <code>EvaluationError</code> <p>If evaluation fails after retries</p> Example <p>evaluator = MyEvaluator(timeout=30.0) result = await evaluator.evaluate( ...     query=\"What is the capital of France?\", ...     response=\"The capital of France is Paris.\", ...     context=\"France is a country in Europe.\", ...     ground_truth=\"Paris\" ... ) print(result[\"score\"]) 0.95</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>async def evaluate(self, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Evaluate with timeout and retry logic.\n\n    This is the main public interface for evaluation. It wraps the\n    implementation with timeout and retry handling.\n\n    Args:\n        **kwargs: Evaluation parameters\n            (query, response, context, ground_truth, etc.)\n\n    Returns:\n        Evaluation result dictionary\n\n    Raises:\n        asyncio.TimeoutError: If evaluation exceeds timeout\n        EvaluationError: If evaluation fails after retries\n\n    Example:\n        &gt;&gt;&gt; evaluator = MyEvaluator(timeout=30.0)\n        &gt;&gt;&gt; result = await evaluator.evaluate(\n        ...     query=\"What is the capital of France?\",\n        ...     response=\"The capital of France is Paris.\",\n        ...     context=\"France is a country in Europe.\",\n        ...     ground_truth=\"Paris\"\n        ... )\n        &gt;&gt;&gt; print(result[\"score\"])\n        0.95\n    \"\"\"\n    logger.debug(f\"Starting evaluation: {self.name} (timeout={self.timeout}s)\")\n\n    if self.timeout is None:\n        # No timeout - evaluate directly with retry\n        logger.debug(f\"Evaluation {self.name}: no timeout\")\n        return await self._evaluate_with_retry(**kwargs)\n\n    # Apply timeout using asyncio.wait_for\n    try:\n        logger.debug(f\"Evaluation {self.name}: applying timeout of {self.timeout}s\")\n        return await asyncio.wait_for(\n            self._evaluate_with_retry(**kwargs), timeout=self.timeout\n        )\n    except TimeoutError:\n        logger.error(f\"Evaluation {self.name} exceeded timeout of {self.timeout}s\")\n        raise  # Re-raise timeout error as-is\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.ContextualPrecisionEvaluator.get_param_spec","title":"<code>get_param_spec()</code>  <code>classmethod</code>","text":"<p>Get the parameter specification for this evaluator.</p> <p>Returns:</p> Type Description <code>ParamSpec</code> <p>ParamSpec declaring required/optional parameters and context flags.</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>@classmethod\ndef get_param_spec(cls) -&gt; ParamSpec:\n    \"\"\"Get the parameter specification for this evaluator.\n\n    Returns:\n        ParamSpec declaring required/optional parameters and context flags.\n    \"\"\"\n    return cls.PARAM_SPEC\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.ContextualRecallEvaluator","title":"<code>ContextualRecallEvaluator(model_config=None, threshold=0.5, include_reason=True, timeout=60.0, retry_config=None)</code>","text":"<p>               Bases: <code>DeepEvalBaseEvaluator</code></p> <p>Contextual Recall evaluator for RAG pipelines.</p> <p>Measures retrieval completeness against expected output. Evaluates whether retrieval context contains all facts needed to produce the expected output.</p> Required inputs <ul> <li>input: User query</li> <li>actual_output: Agent response</li> <li>expected_output: Ground truth answer</li> <li>retrieval_context: List of retrieved text chunks</li> </ul> Example <p>evaluator = ContextualRecallEvaluator(threshold=0.8) result = await evaluator.evaluate( ...     input=\"List all features\", ...     actual_output=\"Features are A and B\", ...     expected_output=\"Features are A, B, and C\", ...     retrieval_context=[\"Feature A: ...\", \"Feature B: ...\"] ... ) print(result[\"score\"])  # ~0.67 (missing Feature C)</p> <p>Attributes:</p> Name Type Description <code>_include_reason</code> <p>Whether to include reasoning in results.</p> <p>Initialize Contextual Recall evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>DeepEvalModelConfig | None</code> <p>LLM judge configuration. Defaults to Ollama gpt-oss:20b.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Pass/fail score threshold (0.0-1.0). Default: 0.5.</p> <code>0.5</code> <code>include_reason</code> <code>bool</code> <p>Whether to include reasoning in results. Default: True.</p> <code>True</code> <code>timeout</code> <code>float | None</code> <p>Evaluation timeout in seconds. Default: 60.0.</p> <code>60.0</code> <code>retry_config</code> <code>RetryConfig | None</code> <p>Retry configuration for transient failures.</p> <code>None</code> Source code in <code>src/holodeck/lib/evaluators/deepeval/contextual_recall.py</code> <pre><code>def __init__(\n    self,\n    model_config: DeepEvalModelConfig | None = None,\n    threshold: float = 0.5,\n    include_reason: bool = True,\n    timeout: float | None = 60.0,\n    retry_config: RetryConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize Contextual Recall evaluator.\n\n    Args:\n        model_config: LLM judge configuration. Defaults to Ollama gpt-oss:20b.\n        threshold: Pass/fail score threshold (0.0-1.0). Default: 0.5.\n        include_reason: Whether to include reasoning in results. Default: True.\n        timeout: Evaluation timeout in seconds. Default: 60.0.\n        retry_config: Retry configuration for transient failures.\n    \"\"\"\n    self._include_reason = include_reason\n\n    super().__init__(\n        model_config=model_config,\n        threshold=threshold,\n        timeout=timeout,\n        retry_config=retry_config,\n    )\n\n    logger.debug(\n        f\"ContextualRecallEvaluator initialized: \"\n        f\"provider={self._model_config.provider.value}, \"\n        f\"model={self._model_config.model_name}, \"\n        f\"threshold={threshold}, include_reason={include_reason}\"\n    )\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.ContextualRecallEvaluator.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the metric name.</p>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.ContextualRecallEvaluator.evaluate","title":"<code>evaluate(**kwargs)</code>  <code>async</code>","text":"<p>Evaluate with timeout and retry logic.</p> <p>This is the main public interface for evaluation. It wraps the implementation with timeout and retry handling.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Evaluation parameters (query, response, context, ground_truth, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Evaluation result dictionary</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If evaluation exceeds timeout</p> <code>EvaluationError</code> <p>If evaluation fails after retries</p> Example <p>evaluator = MyEvaluator(timeout=30.0) result = await evaluator.evaluate( ...     query=\"What is the capital of France?\", ...     response=\"The capital of France is Paris.\", ...     context=\"France is a country in Europe.\", ...     ground_truth=\"Paris\" ... ) print(result[\"score\"]) 0.95</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>async def evaluate(self, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"Evaluate with timeout and retry logic.\n\n    This is the main public interface for evaluation. It wraps the\n    implementation with timeout and retry handling.\n\n    Args:\n        **kwargs: Evaluation parameters\n            (query, response, context, ground_truth, etc.)\n\n    Returns:\n        Evaluation result dictionary\n\n    Raises:\n        asyncio.TimeoutError: If evaluation exceeds timeout\n        EvaluationError: If evaluation fails after retries\n\n    Example:\n        &gt;&gt;&gt; evaluator = MyEvaluator(timeout=30.0)\n        &gt;&gt;&gt; result = await evaluator.evaluate(\n        ...     query=\"What is the capital of France?\",\n        ...     response=\"The capital of France is Paris.\",\n        ...     context=\"France is a country in Europe.\",\n        ...     ground_truth=\"Paris\"\n        ... )\n        &gt;&gt;&gt; print(result[\"score\"])\n        0.95\n    \"\"\"\n    logger.debug(f\"Starting evaluation: {self.name} (timeout={self.timeout}s)\")\n\n    if self.timeout is None:\n        # No timeout - evaluate directly with retry\n        logger.debug(f\"Evaluation {self.name}: no timeout\")\n        return await self._evaluate_with_retry(**kwargs)\n\n    # Apply timeout using asyncio.wait_for\n    try:\n        logger.debug(f\"Evaluation {self.name}: applying timeout of {self.timeout}s\")\n        return await asyncio.wait_for(\n            self._evaluate_with_retry(**kwargs), timeout=self.timeout\n        )\n    except TimeoutError:\n        logger.error(f\"Evaluation {self.name} exceeded timeout of {self.timeout}s\")\n        raise  # Re-raise timeout error as-is\n</code></pre>"},{"location":"api/evaluators/#holodeck.lib.evaluators.deepeval.ContextualRecallEvaluator.get_param_spec","title":"<code>get_param_spec()</code>  <code>classmethod</code>","text":"<p>Get the parameter specification for this evaluator.</p> <p>Returns:</p> Type Description <code>ParamSpec</code> <p>ParamSpec declaring required/optional parameters and context flags.</p> Source code in <code>src/holodeck/lib/evaluators/base.py</code> <pre><code>@classmethod\ndef get_param_spec(cls) -&gt; ParamSpec:\n    \"\"\"Get the parameter specification for this evaluator.\n\n    Returns:\n        ParamSpec declaring required/optional parameters and context flags.\n    \"\"\"\n    return cls.PARAM_SPEC\n</code></pre>"},{"location":"api/evaluators/#usage-examples","title":"Usage Examples","text":""},{"location":"api/evaluators/#deepeval-geval-metrics","title":"DeepEval GEval Metrics","text":"<pre><code>from holodeck.lib.evaluators.deepeval import GEvalEvaluator, DeepEvalModelConfig\nfrom holodeck.models.llm import LLMProvider\n\n# Configure model\nmodel_config = DeepEvalModelConfig(\n    provider=LLMProvider.OLLAMA,\n    name=\"llama3.2:latest\",\n    temperature=0.0\n)\n\n# Create evaluator with custom criteria\nevaluator = GEvalEvaluator(\n    name=\"Coherence\",\n    criteria=\"Evaluate whether the response is clear and well-structured.\",\n    evaluation_steps=[\n        \"Check if the response uses clear language.\",\n        \"Assess if the explanation is easy to follow.\"\n    ],\n    evaluation_params=[\"actual_output\"],\n    model_config=model_config,\n    threshold=0.7\n)\n\n# Evaluate\nresult = await evaluator.evaluate(\n    actual_output=\"The password can be reset by clicking 'Forgot Password' on the login page.\",\n    input=\"How do I reset my password?\"\n)\n\nprint(f\"Score: {result.score}\")\nprint(f\"Passed: {result.passed}\")\nprint(f\"Reason: {result.reason}\")\n</code></pre>"},{"location":"api/evaluators/#deepeval-rag-metrics","title":"DeepEval RAG Metrics","text":"<pre><code>from holodeck.lib.evaluators.deepeval import (\n    FaithfulnessEvaluator,\n    AnswerRelevancyEvaluator,\n    DeepEvalModelConfig\n)\nfrom holodeck.models.llm import LLMProvider\n\n# Configure model\nmodel_config = DeepEvalModelConfig(\n    provider=LLMProvider.OLLAMA,\n    name=\"llama3.2:latest\",\n    temperature=0.0\n)\n\n# Faithfulness - detect hallucinations\nfaithfulness = FaithfulnessEvaluator(\n    model_config=model_config,\n    threshold=0.8,\n    include_reason=True\n)\n\nresult = await faithfulness.evaluate(\n    input=\"What is our return policy?\",\n    actual_output=\"You can return items within 30 days for a full refund.\",\n    retrieval_context=[\n        \"Our return policy allows returns within 30 days of purchase.\",\n        \"Full refunds are provided for items in original condition.\"\n    ]\n)\n\n# Answer Relevancy - check response addresses query\nrelevancy = AnswerRelevancyEvaluator(\n    model_config=model_config,\n    threshold=0.7\n)\n\nresult = await relevancy.evaluate(\n    input=\"How do I reset my password?\",\n    actual_output=\"Click 'Forgot Password' on the login page and follow the email instructions.\"\n)\n</code></pre>"},{"location":"api/evaluators/#nlp-metrics","title":"NLP Metrics","text":"<pre><code>from holodeck.lib.evaluators.nlp_metrics import compute_f1_score, compute_rouge\n\n# Compute F1 score\nprediction = \"the cat is on the mat\"\nreference = \"a cat is on the mat\"\nf1 = compute_f1_score(prediction, reference)\nprint(f\"F1 Score: {f1}\")\n\n# Compute ROUGE scores\nscores = compute_rouge(prediction, reference)\nprint(f\"ROUGE-1: {scores['rouge1']}\")\nprint(f\"ROUGE-2: {scores['rouge2']}\")\nprint(f\"ROUGE-L: {scores['rougeL']}\")\n</code></pre>"},{"location":"api/evaluators/#metric-configuration-in-yaml","title":"Metric Configuration in YAML","text":""},{"location":"api/evaluators/#deepeval-geval-metric","title":"DeepEval GEval Metric","text":"<pre><code>evaluations:\n  model:\n    provider: ollama\n    name: llama3.2:latest\n    temperature: 0.0\n\n  metrics:\n    - type: geval\n      name: \"Coherence\"\n      criteria: \"Evaluate whether the response is clear and well-structured.\"\n      evaluation_steps:\n        - \"Check if the response uses clear language.\"\n        - \"Assess if the explanation is easy to follow.\"\n      evaluation_params:\n        - actual_output\n        - input\n      threshold: 0.7\n      enabled: true\n      fail_on_error: false\n</code></pre>"},{"location":"api/evaluators/#deepeval-rag-metrics_1","title":"DeepEval RAG Metrics","text":"<pre><code>evaluations:\n  model:\n    provider: ollama\n    name: llama3.2:latest\n    temperature: 0.0\n\n  metrics:\n    # Faithfulness - hallucination detection\n    - type: rag\n      metric_type: faithfulness\n      threshold: 0.8\n      include_reason: true\n\n    # Answer Relevancy\n    - type: rag\n      metric_type: answer_relevancy\n      threshold: 0.7\n\n    # Contextual Relevancy\n    - type: rag\n      metric_type: contextual_relevancy\n      threshold: 0.75\n\n    # Contextual Precision\n    - type: rag\n      metric_type: contextual_precision\n      threshold: 0.8\n\n    # Contextual Recall\n    - type: rag\n      metric_type: contextual_recall\n      threshold: 0.7\n</code></pre>"},{"location":"api/evaluators/#nlp-metrics_1","title":"NLP Metrics","text":"<pre><code>evaluations:\n  metrics:\n    - type: standard\n      metric: f1_score\n      threshold: 0.8\n\n    - type: standard\n      metric: bleu\n      threshold: 0.6\n\n    - type: standard\n      metric: rouge\n      threshold: 0.7\n\n    - type: standard\n      metric: meteor\n      threshold: 0.65\n</code></pre>"},{"location":"api/evaluators/#per-metric-model-override","title":"Per-Metric Model Override","text":"<pre><code>evaluations:\n  model:\n    provider: ollama\n    name: llama3.2:latest  # Default: free, local\n\n  metrics:\n    - type: rag\n      metric_type: faithfulness\n      threshold: 0.9\n      model:  # Override for critical metric\n        provider: openai\n        name: gpt-4\n</code></pre>"},{"location":"api/evaluators/#legacy-ai-metrics-deprecated","title":"Legacy AI Metrics (Deprecated)","text":"<p>DEPRECATED: Azure AI-based metrics are deprecated and will be removed in a future version. Migrate to DeepEval metrics for better flexibility and local model support.</p>"},{"location":"api/evaluators/#migration-guide","title":"Migration Guide","text":"Legacy Metric Recommended Replacement <code>groundedness</code> <code>type: rag</code>, <code>metric_type: faithfulness</code> <code>relevance</code> <code>type: rag</code>, <code>metric_type: answer_relevancy</code> <code>coherence</code> <code>type: geval</code> with custom criteria <code>safety</code> <code>type: geval</code> with custom criteria"},{"location":"api/evaluators/#legacy-usage-not-recommended","title":"Legacy Usage (Not Recommended)","text":"<pre><code># DEPRECATED - Use DeepEval evaluators instead\nfrom holodeck.lib.evaluators.azure_ai import AzureAIEvaluator\n\nevaluator = AzureAIEvaluator(model=\"gpt-4\", api_key=\"your-key\")\n\nresult = await evaluator.evaluate_groundedness(\n    response=\"Paris is the capital of France\",\n    context=\"France's capital city is known for the Eiffel Tower\",\n)\n</code></pre>"},{"location":"api/evaluators/#legacy-yaml-configuration-not-recommended","title":"Legacy YAML Configuration (Not Recommended)","text":"<pre><code># DEPRECATED - Use type: geval or type: rag instead\nevaluations:\n  metrics:\n    - type: standard\n      metric: groundedness  # Deprecated\n      threshold: 0.8\n\n    - type: standard\n      metric: relevance     # Deprecated\n      threshold: 0.75\n\n    - type: standard\n      metric: coherence     # Deprecated\n      threshold: 0.7\n\n    - type: standard\n      metric: safety        # Deprecated\n      threshold: 0.9\n</code></pre>"},{"location":"api/evaluators/#integration-with-test-runner","title":"Integration with Test Runner","text":"<p>The test runner automatically:</p> <ol> <li>Loads evaluation configuration from agent YAML</li> <li>Creates appropriate evaluators based on metric type</li> <li>Invokes evaluators on test outputs</li> <li>Extracts retrieval_context from tool results (for RAG metrics)</li> <li>Collects metric scores</li> <li>Compares against thresholds</li> <li>Includes results in test report</li> </ol>"},{"location":"api/evaluators/#test-runner-evaluator-creation","title":"Test Runner Evaluator Creation","text":"<pre><code># Internal test runner logic (simplified)\ndef _create_evaluators(self, metrics: list[MetricType]) -&gt; dict:\n    evaluators = {}\n\n    for metric in metrics:\n        if metric.type == \"geval\":\n            evaluators[metric.name] = GEvalEvaluator(\n                name=metric.name,\n                criteria=metric.criteria,\n                evaluation_steps=metric.evaluation_steps,\n                evaluation_params=metric.evaluation_params,\n                model_config=self._get_model_config(metric),\n                threshold=metric.threshold,\n                strict_mode=metric.strict_mode\n            )\n        elif metric.type == \"rag\":\n            evaluator_class = RAG_EVALUATOR_MAP[metric.metric_type]\n            evaluators[metric.metric_type] = evaluator_class(\n                model_config=self._get_model_config(metric),\n                threshold=metric.threshold,\n                include_reason=metric.include_reason\n            )\n        elif metric.type == \"standard\":\n            # NLP or legacy metrics\n            evaluators[metric.metric] = self._create_standard_evaluator(metric)\n\n    return evaluators\n</code></pre>"},{"location":"api/evaluators/#error-handling","title":"Error Handling","text":""},{"location":"api/evaluators/#deepeval-errors","title":"DeepEval Errors","text":"<pre><code>from holodeck.lib.evaluators.deepeval.errors import (\n    DeepEvalError,\n    ProviderNotSupportedError\n)\n\ntry:\n    result = await evaluator.evaluate(actual_output=\"...\")\nexcept DeepEvalError as e:\n    print(f\"Evaluation failed: {e.message}\")\n    print(f\"Metric: {e.metric_name}\")\n    print(f\"Test case: {e.test_case_summary}\")\nexcept ProviderNotSupportedError as e:\n    print(f\"Provider not supported: {e}\")\n</code></pre>"},{"location":"api/evaluators/#soft-vs-hard-failures","title":"Soft vs Hard Failures","text":"<pre><code>metrics:\n  # Soft failure - continues on error\n  - type: geval\n    name: \"Quality\"\n    criteria: \"...\"\n    fail_on_error: false  # Default\n\n  # Hard failure - stops test on error\n  - type: rag\n    metric_type: faithfulness\n    fail_on_error: true\n</code></pre>"},{"location":"api/evaluators/#related-documentation","title":"Related Documentation","text":"<ul> <li>Evaluations Guide: Configuration and usage guide</li> <li>Test Runner: Test execution framework</li> <li>Data Models: EvaluationConfig and MetricConfig models</li> <li>Configuration Loading: Loading evaluation configs</li> </ul>"},{"location":"api/models/","title":"Data Models API Reference","text":"<p>HoloDeck uses Pydantic models for all configuration validation. This section documents the complete data model hierarchy used throughout the platform.</p>"},{"location":"api/models/#agent-models","title":"Agent Models","text":"<p>Core agent configuration and instruction models.</p>"},{"location":"api/models/#holodeck.models.agent.Agent","title":"<code>Agent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Agent configuration entity.</p> <p>Root configuration for a single AI agent instance, defining model, instructions, tools, evaluations, and test cases.</p>"},{"location":"api/models/#holodeck.models.agent.Agent.name","title":"<code>name = Field(default=..., description='Agent identifier')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/models/#holodeck.models.agent.Agent.description","title":"<code>description = Field(default=None, description='Human-readable description')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/models/#holodeck.models.agent.Agent.instructions","title":"<code>instructions = Field(default=..., description='System instructions (file or inline)')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/models/#holodeck.models.agent.Agent.model","title":"<code>model = Field(default=..., description='LLM provider configuration')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/models/#holodeck.models.agent.Agent.tools","title":"<code>tools = Field(default=None, description='Agent tools (vectorstore, function, mcp, prompt)')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/models/#holodeck.models.agent.Agent.evaluations","title":"<code>evaluations = Field(default=None, description='Evaluation configuration')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/models/#holodeck.models.agent.Agent.test_cases","title":"<code>test_cases = Field(default=None, description='Test scenarios')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/models/#holodeck.models.agent.Instructions","title":"<code>Instructions</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>System instructions specification (file or inline).</p> <p>Represents the system prompt for an agent, supporting both file references and inline text.</p>"},{"location":"api/models/#holodeck.models.agent.Instructions.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Validate file and inline mutual exclusivity.</p> Source code in <code>src/holodeck/models/agent.py</code> <pre><code>def model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Validate file and inline mutual exclusivity.\"\"\"\n    if self.file and self.inline:\n        raise ValueError(\"Cannot provide both 'file' and 'inline'\")\n    if not self.file and not self.inline:\n        raise ValueError(\"Must provide either 'file' or 'inline'\")\n</code></pre>"},{"location":"api/models/#holodeck.models.agent.Instructions.validate_file","title":"<code>validate_file(v)</code>  <code>classmethod</code>","text":"<p>Validate file is not empty if provided.</p> Source code in <code>src/holodeck/models/agent.py</code> <pre><code>@field_validator(\"file\")\n@classmethod\ndef validate_file(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate file is not empty if provided.\"\"\"\n    if v is not None and (not v or not v.strip()):\n        raise ValueError(\"file must be non-empty if provided\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.agent.Instructions.validate_inline","title":"<code>validate_inline(v)</code>  <code>classmethod</code>","text":"<p>Validate inline is not empty if provided.</p> Source code in <code>src/holodeck/models/agent.py</code> <pre><code>@field_validator(\"inline\")\n@classmethod\ndef validate_inline(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate inline is not empty if provided.\"\"\"\n    if v is not None and (not v or not v.strip()):\n        raise ValueError(\"inline must be non-empty if provided\")\n    return v\n</code></pre>"},{"location":"api/models/#llm-provider-models","title":"LLM Provider Models","text":"<p>Language model provider configuration for OpenAI, Azure OpenAI, and Anthropic.</p>"},{"location":"api/models/#holodeck.models.llm.ProviderEnum","title":"<code>ProviderEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported LLM providers.</p>"},{"location":"api/models/#holodeck.models.llm.LLMProvider","title":"<code>LLMProvider</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM provider configuration.</p> <p>Specifies which LLM provider and model to use, along with model parameters.</p>"},{"location":"api/models/#holodeck.models.llm.LLMProvider.provider","title":"<code>provider = Field(..., description='LLM provider')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/models/#holodeck.models.llm.LLMProvider.temperature","title":"<code>temperature = Field(default=0.3, description='Temperature (0.0-2.0)')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/models/#holodeck.models.llm.LLMProvider.max_tokens","title":"<code>max_tokens = Field(default=1000, description='Maximum tokens to generate')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/models/#holodeck.models.llm.LLMProvider.top_p","title":"<code>top_p = Field(default=None, description='Nucleus sampling parameter')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/models/#tool-models","title":"Tool Models","text":"<p>Five types of tools are supported: vectorstore, function, MCP, prompt, and plugins.</p>"},{"location":"api/models/#holodeck.models.tool.Tool","title":"<code>Tool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base tool model with discriminated union for subtypes.</p>"},{"location":"api/models/#holodeck.models.tool.VectorstoreTool","title":"<code>VectorstoreTool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Vectorstore tool for semantic search over documents.</p>"},{"location":"api/models/#holodeck.models.tool.VectorstoreTool.validate_chunk_overlap","title":"<code>validate_chunk_overlap(v)</code>  <code>classmethod</code>","text":"<p>Validate chunk_overlap is non-negative if provided.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"chunk_overlap\")\n@classmethod\ndef validate_chunk_overlap(cls, v: int | None) -&gt; int | None:\n    \"\"\"Validate chunk_overlap is non-negative if provided.\"\"\"\n    if v is not None and v &lt; 0:\n        raise ValueError(\"chunk_overlap must be non-negative\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.VectorstoreTool.validate_chunk_size","title":"<code>validate_chunk_size(v)</code>  <code>classmethod</code>","text":"<p>Validate chunk_size is positive if provided.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"chunk_size\")\n@classmethod\ndef validate_chunk_size(cls, v: int | None) -&gt; int | None:\n    \"\"\"Validate chunk_size is positive if provided.\"\"\"\n    if v is not None and v &lt;= 0:\n        raise ValueError(\"chunk_size must be positive\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.VectorstoreTool.validate_database","title":"<code>validate_database(v)</code>  <code>classmethod</code>","text":"<p>Validate database is not empty string if provided as string.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"database\")\n@classmethod\ndef validate_database(\n    cls, v: DatabaseConfig | str | None\n) -&gt; DatabaseConfig | str | None:\n    \"\"\"Validate database is not empty string if provided as string.\"\"\"\n    if isinstance(v, str) and not v.strip():\n        raise ValueError(\"database reference must be a non-empty string\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.VectorstoreTool.validate_embedding_dimensions","title":"<code>validate_embedding_dimensions(v)</code>  <code>classmethod</code>","text":"<p>Validate embedding_dimensions is positive and reasonable if provided.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"embedding_dimensions\")\n@classmethod\ndef validate_embedding_dimensions(cls, v: int | None) -&gt; int | None:\n    \"\"\"Validate embedding_dimensions is positive and reasonable if provided.\"\"\"\n    if v is not None:\n        if v &lt;= 0:\n            raise ValueError(\"embedding_dimensions must be positive\")\n        if v &gt; 10000:\n            raise ValueError(\"embedding_dimensions unreasonably large (max 10000)\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.VectorstoreTool.validate_min_similarity_score","title":"<code>validate_min_similarity_score(v)</code>  <code>classmethod</code>","text":"<p>Validate min_similarity_score is between 0.0 and 1.0 if provided.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"min_similarity_score\")\n@classmethod\ndef validate_min_similarity_score(cls, v: float | None) -&gt; float | None:\n    \"\"\"Validate min_similarity_score is between 0.0 and 1.0 if provided.\"\"\"\n    if v is not None and not (0.0 &lt;= v &lt;= 1.0):\n        raise ValueError(\"min_similarity_score must be between 0.0 and 1.0\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.VectorstoreTool.validate_source","title":"<code>validate_source(v)</code>  <code>classmethod</code>","text":"<p>Validate source is not empty.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"source\")\n@classmethod\ndef validate_source(cls, v: str) -&gt; str:\n    \"\"\"Validate source is not empty.\"\"\"\n    if not v or not v.strip():\n        raise ValueError(\"source must be a non-empty path\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.VectorstoreTool.validate_top_k","title":"<code>validate_top_k(v)</code>  <code>classmethod</code>","text":"<p>Validate top_k is a positive integer.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"top_k\")\n@classmethod\ndef validate_top_k(cls, v: int) -&gt; int:\n    \"\"\"Validate top_k is a positive integer.\"\"\"\n    if v &lt;= 0:\n        raise ValueError(\"top_k must be a positive integer\")\n    if v &gt; 100:\n        raise ValueError(\"top_k should not exceed 100\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.FunctionTool","title":"<code>FunctionTool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Function tool for calling Python functions.</p>"},{"location":"api/models/#holodeck.models.tool.FunctionTool.validate_file","title":"<code>validate_file(v)</code>  <code>classmethod</code>","text":"<p>Validate file is not empty.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"file\")\n@classmethod\ndef validate_file(cls, v: str) -&gt; str:\n    \"\"\"Validate file is not empty.\"\"\"\n    if not v or not v.strip():\n        raise ValueError(\"file must be a non-empty path\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.FunctionTool.validate_function","title":"<code>validate_function(v)</code>  <code>classmethod</code>","text":"<p>Validate function is not empty.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"function\")\n@classmethod\ndef validate_function(cls, v: str) -&gt; str:\n    \"\"\"Validate function is not empty.\"\"\"\n    if not v or not v.strip():\n        raise ValueError(\"function must be a non-empty identifier\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.MCPTool","title":"<code>MCPTool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>MCP (Model Context Protocol) tool for standardized integrations.</p> <p>Supports four transport types: - stdio (default): Local MCP servers via subprocess - sse: Remote servers via Server-Sent Events - websocket: Bidirectional WebSocket communication - http: Streamable HTTP transport</p> <p>For stdio transport, only npx, uvx, or docker commands are allowed for security reasons.</p>"},{"location":"api/models/#holodeck.models.tool.MCPTool.validate_request_timeout","title":"<code>validate_request_timeout(v)</code>  <code>classmethod</code>","text":"<p>Validate request_timeout is positive.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"request_timeout\")\n@classmethod\ndef validate_request_timeout(cls, v: int) -&gt; int:\n    \"\"\"Validate request_timeout is positive.\"\"\"\n    if v &lt;= 0:\n        raise ValueError(\"request_timeout must be positive\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.MCPTool.validate_transport_fields","title":"<code>validate_transport_fields()</code>","text":"<p>Validate transport-specific required fields.</p> <ul> <li>stdio transport requires 'command'</li> <li>sse, websocket, http transports require 'url'</li> </ul> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_transport_fields(self) -&gt; \"MCPTool\":\n    \"\"\"Validate transport-specific required fields.\n\n    - stdio transport requires 'command'\n    - sse, websocket, http transports require 'url'\n    \"\"\"\n    if self.transport == TransportType.STDIO:\n        if self.command is None:\n            raise ValueError(\"'command' is required for stdio transport\")\n    elif (\n        self.transport\n        in (TransportType.SSE, TransportType.WEBSOCKET, TransportType.HTTP)\n        and self.url is None\n    ):\n        raise ValueError(f\"'url' is required for {self.transport.value} transport\")\n    return self\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.MCPTool.validate_url_scheme","title":"<code>validate_url_scheme(v)</code>  <code>classmethod</code>","text":"<p>Validate URL scheme for HTTP-based transports.</p> <p>Allows http:// only for localhost, requires https:// for remote URLs. WebSocket URLs can use wss:// or ws://.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"url\")\n@classmethod\ndef validate_url_scheme(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate URL scheme for HTTP-based transports.\n\n    Allows http:// only for localhost, requires https:// for remote URLs.\n    WebSocket URLs can use wss:// or ws://.\n    \"\"\"\n    if v is None:\n        return v\n    # Allow http:// for localhost, require https:// otherwise\n    if v.startswith(\"http://\"):\n        localhost_prefixes = (\n            \"http://localhost\",\n            \"http://127.0.0.1\",\n            \"http://[::1]\",\n        )\n        if not any(v.startswith(prefix) for prefix in localhost_prefixes):\n            raise ValueError(\"'url' must use https:// (or http:// for localhost)\")\n    elif not v.startswith((\"https://\", \"wss://\", \"ws://\")):\n        raise ValueError(\"'url' must use https://, wss://, or ws:// scheme\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.PromptTool","title":"<code>PromptTool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Prompt-based tool for AI-powered semantic functions.</p>"},{"location":"api/models/#holodeck.models.tool.PromptTool.check_template_or_file","title":"<code>check_template_or_file(v, info)</code>  <code>classmethod</code>","text":"<p>Validate that exactly one of template or file is provided.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"parameters\", mode=\"before\")\n@classmethod\ndef check_template_or_file(cls, v: Any, info: Any) -&gt; Any:\n    \"\"\"Validate that exactly one of template or file is provided.\"\"\"\n    data = info.data\n    template = data.get(\"template\")\n    file_path = data.get(\"file\")\n\n    if not template and not file_path:\n        raise ValueError(\"Exactly one of 'template' or 'file' must be provided\")\n    if template and file_path:\n        raise ValueError(\"Cannot provide both 'template' and 'file'\")\n\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.PromptTool.validate_file","title":"<code>validate_file(v)</code>  <code>classmethod</code>","text":"<p>Validate file is not empty if provided.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"file\")\n@classmethod\ndef validate_file(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate file is not empty if provided.\"\"\"\n    if v is not None and (not v or not v.strip()):\n        raise ValueError(\"file must be non-empty if provided\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.PromptTool.validate_parameters","title":"<code>validate_parameters(v)</code>  <code>classmethod</code>","text":"<p>Validate parameters is not empty.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"parameters\")\n@classmethod\ndef validate_parameters(\n    cls, v: dict[str, dict[str, Any]]\n) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Validate parameters is not empty.\"\"\"\n    if not v:\n        raise ValueError(\"parameters must have at least one parameter\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.tool.PromptTool.validate_template","title":"<code>validate_template(v)</code>  <code>classmethod</code>","text":"<p>Validate template is not empty if provided.</p> Source code in <code>src/holodeck/models/tool.py</code> <pre><code>@field_validator(\"template\")\n@classmethod\ndef validate_template(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate template is not empty if provided.\"\"\"\n    if v is not None and (not v or not v.strip()):\n        raise ValueError(\"template must be non-empty if provided\")\n    return v\n</code></pre>"},{"location":"api/models/#evaluation-models","title":"Evaluation Models","text":"<p>Metrics and evaluation framework configuration.</p>"},{"location":"api/models/#holodeck.models.evaluation.EvaluationMetric","title":"<code>EvaluationMetric</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Evaluation metric configuration.</p> <p>Represents a single evaluation metric with flexible model configuration, including per-metric LLM model overrides.</p>"},{"location":"api/models/#holodeck.models.evaluation.EvaluationMetric.validate_custom_prompt","title":"<code>validate_custom_prompt(v)</code>  <code>classmethod</code>","text":"<p>Validate custom_prompt is not empty if provided.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"custom_prompt\")\n@classmethod\ndef validate_custom_prompt(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate custom_prompt is not empty if provided.\"\"\"\n    if v is not None and (not v or not v.strip()):\n        raise ValueError(\"custom_prompt must be non-empty if provided\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.evaluation.EvaluationMetric.validate_enabled","title":"<code>validate_enabled(v)</code>  <code>classmethod</code>","text":"<p>Validate enabled is boolean.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"enabled\")\n@classmethod\ndef validate_enabled(cls, v: bool) -&gt; bool:\n    \"\"\"Validate enabled is boolean.\"\"\"\n    if not isinstance(v, bool):\n        raise ValueError(\"enabled must be boolean\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.evaluation.EvaluationMetric.validate_fail_on_error","title":"<code>validate_fail_on_error(v)</code>  <code>classmethod</code>","text":"<p>Validate fail_on_error is boolean.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"fail_on_error\")\n@classmethod\ndef validate_fail_on_error(cls, v: bool) -&gt; bool:\n    \"\"\"Validate fail_on_error is boolean.\"\"\"\n    if not isinstance(v, bool):\n        raise ValueError(\"fail_on_error must be boolean\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.evaluation.EvaluationMetric.validate_metric","title":"<code>validate_metric(v)</code>  <code>classmethod</code>","text":"<p>Validate metric is not empty.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"metric\")\n@classmethod\ndef validate_metric(cls, v: str) -&gt; str:\n    \"\"\"Validate metric is not empty.\"\"\"\n    if not v or not v.strip():\n        raise ValueError(\"metric must be a non-empty string\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.evaluation.EvaluationMetric.validate_retry_on_failure","title":"<code>validate_retry_on_failure(v)</code>  <code>classmethod</code>","text":"<p>Validate retry_on_failure is in valid range.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"retry_on_failure\")\n@classmethod\ndef validate_retry_on_failure(cls, v: int | None) -&gt; int | None:\n    \"\"\"Validate retry_on_failure is in valid range.\"\"\"\n    if v is not None and (v &lt; 1 or v &gt; 3):\n        raise ValueError(\"retry_on_failure must be between 1 and 3\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.evaluation.EvaluationMetric.validate_scale","title":"<code>validate_scale(v)</code>  <code>classmethod</code>","text":"<p>Validate scale is positive.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"scale\")\n@classmethod\ndef validate_scale(cls, v: int | None) -&gt; int | None:\n    \"\"\"Validate scale is positive.\"\"\"\n    if v is not None and v &lt;= 0:\n        raise ValueError(\"scale must be positive\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.evaluation.EvaluationMetric.validate_threshold","title":"<code>validate_threshold(v)</code>  <code>classmethod</code>","text":"<p>Validate threshold is numeric if provided.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"threshold\")\n@classmethod\ndef validate_threshold(cls, v: float | None) -&gt; float | None:\n    \"\"\"Validate threshold is numeric if provided.\"\"\"\n    if v is not None and not isinstance(v, int | float):\n        raise ValueError(\"threshold must be numeric\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.evaluation.EvaluationMetric.validate_timeout_ms","title":"<code>validate_timeout_ms(v)</code>  <code>classmethod</code>","text":"<p>Validate timeout_ms is positive.</p> Source code in <code>src/holodeck/models/evaluation.py</code> <pre><code>@field_validator(\"timeout_ms\")\n@classmethod\ndef validate_timeout_ms(cls, v: int | None) -&gt; int | None:\n    \"\"\"Validate timeout_ms is positive.\"\"\"\n    if v is not None and v &lt;= 0:\n        raise ValueError(\"timeout_ms must be positive\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.evaluation.EvaluationConfig","title":"<code>EvaluationConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Evaluation framework configuration.</p> <p>Container for evaluation metrics with optional default model configuration. Supports standard EvaluationMetric, GEvalMetric (custom criteria), and RAGMetric (RAG pipeline evaluation).</p>"},{"location":"api/models/#holodeck.models.evaluation.EvaluationConfig.model","title":"<code>model = Field(None, description='Default LLM model for all metrics')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/models/#holodeck.models.evaluation.EvaluationConfig.metrics","title":"<code>metrics = Field(..., description='List of metrics to evaluate (standard, GEval, or RAG)')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/models/#test-case-models","title":"Test Case Models","text":"<p>Test case definitions with multimodal file input support.</p>"},{"location":"api/models/#holodeck.models.test_case.FileInput","title":"<code>FileInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>File input for multimodal test cases.</p> <p>Represents a single file reference for test case inputs, supporting both local files and remote URLs with optional extraction parameters.</p>"},{"location":"api/models/#holodeck.models.test_case.FileInput.check_path_or_url","title":"<code>check_path_or_url(v, info)</code>  <code>classmethod</code>","text":"<p>Validate that exactly one of path or url is provided.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"path\", \"url\", mode=\"before\")\n@classmethod\ndef check_path_or_url(cls, v: Any, info: Any) -&gt; Any:\n    \"\"\"Validate that exactly one of path or url is provided.\"\"\"\n    # This runs before validation, so we check in root_validator\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.test_case.FileInput.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Validate path and url mutual exclusivity after initialization.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>def model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Validate path and url mutual exclusivity after initialization.\"\"\"\n    if self.path and self.url:\n        raise ValueError(\"Cannot provide both 'path' and 'url'\")\n    if not self.path and not self.url:\n        raise ValueError(\"Must provide either 'path' or 'url'\")\n</code></pre>"},{"location":"api/models/#holodeck.models.test_case.FileInput.validate_pages","title":"<code>validate_pages(v)</code>  <code>classmethod</code>","text":"<p>Validate pages are positive integers.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"pages\")\n@classmethod\ndef validate_pages(cls, v: list[int] | None) -&gt; list[int] | None:\n    \"\"\"Validate pages are positive integers.\"\"\"\n    if v is not None and not all(isinstance(p, int) and p &gt; 0 for p in v):\n        raise ValueError(\"pages must be positive integers\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.test_case.FileInput.validate_type","title":"<code>validate_type(v)</code>  <code>classmethod</code>","text":"<p>Validate file type is supported.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"type\")\n@classmethod\ndef validate_type(cls, v: str) -&gt; str:\n    \"\"\"Validate file type is supported.\"\"\"\n    valid_types = {\"image\", \"pdf\", \"text\", \"excel\", \"word\", \"powerpoint\", \"csv\"}\n    if v not in valid_types:\n        raise ValueError(f\"type must be one of {valid_types}, got {v}\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.test_case.TestCaseModel","title":"<code>TestCaseModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Test case for agent evaluation.</p> <p>Represents a single test scenario with input, optional expected output, expected tool usage, multimodal file inputs, and RAG context.</p>"},{"location":"api/models/#holodeck.models.test_case.TestCaseModel.validate_files","title":"<code>validate_files(v)</code>  <code>classmethod</code>","text":"<p>Validate files list is not empty if provided.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"files\")\n@classmethod\ndef validate_files(cls, v: list[FileInput] | None) -&gt; list[FileInput] | None:\n    \"\"\"Validate files list is not empty if provided.\"\"\"\n    if v is not None and len(v) &gt; 10:\n        raise ValueError(\"Maximum 10 files per test case\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.test_case.TestCaseModel.validate_ground_truth","title":"<code>validate_ground_truth(v)</code>  <code>classmethod</code>","text":"<p>Validate ground_truth is not empty if provided.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"ground_truth\")\n@classmethod\ndef validate_ground_truth(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate ground_truth is not empty if provided.\"\"\"\n    if v is not None and (not v or not v.strip()):\n        raise ValueError(\"ground_truth must be non-empty if provided\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.test_case.TestCaseModel.validate_input","title":"<code>validate_input(v)</code>  <code>classmethod</code>","text":"<p>Validate input is not empty.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"input\")\n@classmethod\ndef validate_input(cls, v: str) -&gt; str:\n    \"\"\"Validate input is not empty.\"\"\"\n    if not v or not v.strip():\n        raise ValueError(\"input must be a non-empty string\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.test_case.TestCaseModel.validate_name","title":"<code>validate_name(v)</code>  <code>classmethod</code>","text":"<p>Validate name is not empty if provided.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"name\")\n@classmethod\ndef validate_name(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate name is not empty if provided.\"\"\"\n    if v is not None and (not v or not v.strip()):\n        raise ValueError(\"name must be non-empty if provided\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.test_case.TestCase","title":"<code>TestCase = TestCaseModel</code>  <code>module-attribute</code>","text":""},{"location":"api/models/#global-configuration-models","title":"Global Configuration Models","text":"<p>Project-wide settings for vectorstore, deployment, and execution.</p>"},{"location":"api/models/#holodeck.models.config.VectorstoreConfig","title":"<code>VectorstoreConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Vectorstore configuration for global defaults.</p> <p>Specifies connection details and options for a specific vectorstore backend.</p>"},{"location":"api/models/#holodeck.models.config.VectorstoreConfig.validate_connection_string","title":"<code>validate_connection_string(v)</code>  <code>classmethod</code>","text":"<p>Validate connection_string is not empty.</p> Source code in <code>src/holodeck/models/config.py</code> <pre><code>@field_validator(\"connection_string\")\n@classmethod\ndef validate_connection_string(cls, v: str) -&gt; str:\n    \"\"\"Validate connection_string is not empty.\"\"\"\n    if not v or not v.strip():\n        raise ValueError(\"connection_string must be a non-empty string\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.config.VectorstoreConfig.validate_provider","title":"<code>validate_provider(v)</code>  <code>classmethod</code>","text":"<p>Validate provider is not empty.</p> Source code in <code>src/holodeck/models/config.py</code> <pre><code>@field_validator(\"provider\")\n@classmethod\ndef validate_provider(cls, v: str) -&gt; str:\n    \"\"\"Validate provider is not empty.\"\"\"\n    if not v or not v.strip():\n        raise ValueError(\"provider must be a non-empty string\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.config.DeploymentConfig","title":"<code>DeploymentConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Deployment configuration for global defaults.</p> <p>Specifies deployment platform and settings for deploying agents.</p>"},{"location":"api/models/#holodeck.models.config.DeploymentConfig.validate_type","title":"<code>validate_type(v)</code>  <code>classmethod</code>","text":"<p>Validate type is not empty.</p> Source code in <code>src/holodeck/models/config.py</code> <pre><code>@field_validator(\"type\")\n@classmethod\ndef validate_type(cls, v: str) -&gt; str:\n    \"\"\"Validate type is not empty.\"\"\"\n    if not v or not v.strip():\n        raise ValueError(\"type must be a non-empty string\")\n    return v\n</code></pre>"},{"location":"api/models/#holodeck.models.config.GlobalConfig","title":"<code>GlobalConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Global configuration entity.</p> <p>Configuration stored in ~/.holodeck/config.yaml for sharing defaults across multiple agents, including LLM providers, vectorstores, execution, and deployment settings.</p>"},{"location":"api/models/#test-result-models","title":"Test Result Models","text":"<p>Models for representing test execution results and reports.</p>"},{"location":"api/models/#holodeck.models.test_result.TestResult","title":"<code>TestResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result of executing a single test case.</p> <p>Contains the test input, agent response, tool calls, metric results, and overall pass/fail status along with any errors encountered.</p>"},{"location":"api/models/#holodeck.models.test_result.TestReport","title":"<code>TestReport</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete test execution report.</p> <p>Contains all test results, summary statistics, and metadata about the test run including agent name, version, and environment.</p>"},{"location":"api/models/#holodeck.models.test_result.TestReport.validate_results_count","title":"<code>validate_results_count()</code>","text":"<p>Validate that summary total_tests matches results count.</p> Source code in <code>src/holodeck/models/test_result.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_results_count(self) -&gt; \"TestReport\":\n    \"\"\"Validate that summary total_tests matches results count.\"\"\"\n    if self.summary.total_tests != len(self.results):\n        raise ValueError(\n            f\"summary.total_tests ({self.summary.total_tests}) \"\n            f\"must match number of results ({len(self.results)})\"\n        )\n    return self\n</code></pre>"},{"location":"api/models/#error-models","title":"Error Models","text":"<p>HoloDeck exception hierarchy for error handling.</p>"},{"location":"api/models/#holodeck.lib.errors.HoloDeckError","title":"<code>HoloDeckError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all HoloDeck errors.</p> <p>All HoloDeck-specific exceptions inherit from this class, enabling centralized exception handling and error tracking.</p>"},{"location":"api/models/#holodeck.lib.errors.ConfigError","title":"<code>ConfigError(field, message)</code>","text":"<p>               Bases: <code>HoloDeckError</code></p> <p>Exception raised for configuration errors.</p> <p>This exception is raised when configuration loading or parsing fails. It includes field-specific information to help users identify and fix configuration issues.</p> <p>Attributes:</p> Name Type Description <code>field</code> <p>The configuration field that caused the error</p> <code>message</code> <p>Human-readable error message describing the issue</p> <p>Initialize ConfigError with field and message.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Configuration field name where error occurred</p> required <code>message</code> <code>str</code> <p>Descriptive error message</p> required Source code in <code>src/holodeck/lib/errors.py</code> <pre><code>def __init__(self, field: str, message: str) -&gt; None:\n    \"\"\"Initialize ConfigError with field and message.\n\n    Args:\n        field: Configuration field name where error occurred\n        message: Descriptive error message\n    \"\"\"\n    self.field = field\n    self.message = message\n    super().__init__(f\"Configuration error in '{field}': {message}\")\n</code></pre>"},{"location":"api/models/#holodeck.lib.errors.ValidationError","title":"<code>ValidationError(field, message, expected, actual)</code>","text":"<p>               Bases: <code>HoloDeckError</code></p> <p>Exception raised for validation errors during configuration parsing.</p> <p>Provides detailed information about what was expected versus what was received, enabling users to quickly understand and fix validation issues.</p> <p>Attributes:</p> Name Type Description <code>field</code> <p>The field that failed validation</p> <code>message</code> <p>Description of the validation failure</p> <code>expected</code> <p>Human description of expected value/type</p> <code>actual</code> <p>The actual value that failed validation</p> <p>Initialize ValidationError with detailed information.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Field that failed validation (can use dot notation for nested fields)</p> required <code>message</code> <code>str</code> <p>Description of what went wrong</p> required <code>expected</code> <code>str</code> <p>Human-readable description of expected value</p> required <code>actual</code> <code>str</code> <p>The actual value that failed</p> required Source code in <code>src/holodeck/lib/errors.py</code> <pre><code>def __init__(\n    self,\n    field: str,\n    message: str,\n    expected: str,\n    actual: str,\n) -&gt; None:\n    \"\"\"Initialize ValidationError with detailed information.\n\n    Args:\n        field: Field that failed validation (can use dot notation for nested fields)\n        message: Description of what went wrong\n        expected: Human-readable description of expected value\n        actual: The actual value that failed\n    \"\"\"\n    self.field = field\n    self.message = message\n    self.expected = expected\n    self.actual = actual\n    full_message = (\n        f\"Validation error in '{field}': {message}\\n\"\n        f\"  Expected: {expected}\\n\"\n        f\"  Got: {actual}\"\n    )\n    super().__init__(full_message)\n</code></pre>"},{"location":"api/models/#holodeck.lib.errors.FileNotFoundError","title":"<code>FileNotFoundError(path, message)</code>","text":"<p>               Bases: <code>HoloDeckError</code></p> <p>Exception raised when a configuration file is not found.</p> <p>Includes the file path and helpful suggestions for resolving the issue.</p> <p>Attributes:</p> Name Type Description <code>path</code> <p>Path to the file that was not found</p> <code>message</code> <p>Human-readable error message</p> <p>Initialize FileNotFoundError with path and message.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file that was not found</p> required <code>message</code> <code>str</code> <p>Descriptive error message, optionally with suggestions</p> required Source code in <code>src/holodeck/lib/errors.py</code> <pre><code>def __init__(self, path: str, message: str) -&gt; None:\n    \"\"\"Initialize FileNotFoundError with path and message.\n\n    Args:\n        path: Path to the file that was not found\n        message: Descriptive error message, optionally with suggestions\n    \"\"\"\n    self.path = path\n    self.message = message\n    super().__init__(f\"File not found: {path}\\n{message}\")\n</code></pre>"},{"location":"api/models/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Loading: How to load and validate configurations</li> <li>Test Runner: Test execution framework using these models</li> <li>Evaluation Framework: Evaluation system using these models</li> </ul>"},{"location":"api/test-runner/","title":"Test Execution Framework API","text":"<p>The test runner orchestrates the complete test execution pipeline for HoloDeck agents, from configuration resolution through evaluation and result reporting.</p>"},{"location":"api/test-runner/#test-case-configuration","title":"Test Case Configuration","text":"<p>Configuration models for defining test cases with multimodal file support.</p>"},{"location":"api/test-runner/#holodeck.models.test_case.TestCaseModel","title":"<code>TestCaseModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Test case for agent evaluation.</p> <p>Represents a single test scenario with input, optional expected output, expected tool usage, multimodal file inputs, and RAG context.</p>"},{"location":"api/test-runner/#holodeck.models.test_case.TestCaseModel.validate_files","title":"<code>validate_files(v)</code>  <code>classmethod</code>","text":"<p>Validate files list is not empty if provided.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"files\")\n@classmethod\ndef validate_files(cls, v: list[FileInput] | None) -&gt; list[FileInput] | None:\n    \"\"\"Validate files list is not empty if provided.\"\"\"\n    if v is not None and len(v) &gt; 10:\n        raise ValueError(\"Maximum 10 files per test case\")\n    return v\n</code></pre>"},{"location":"api/test-runner/#holodeck.models.test_case.TestCaseModel.validate_ground_truth","title":"<code>validate_ground_truth(v)</code>  <code>classmethod</code>","text":"<p>Validate ground_truth is not empty if provided.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"ground_truth\")\n@classmethod\ndef validate_ground_truth(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate ground_truth is not empty if provided.\"\"\"\n    if v is not None and (not v or not v.strip()):\n        raise ValueError(\"ground_truth must be non-empty if provided\")\n    return v\n</code></pre>"},{"location":"api/test-runner/#holodeck.models.test_case.TestCaseModel.validate_input","title":"<code>validate_input(v)</code>  <code>classmethod</code>","text":"<p>Validate input is not empty.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"input\")\n@classmethod\ndef validate_input(cls, v: str) -&gt; str:\n    \"\"\"Validate input is not empty.\"\"\"\n    if not v or not v.strip():\n        raise ValueError(\"input must be a non-empty string\")\n    return v\n</code></pre>"},{"location":"api/test-runner/#holodeck.models.test_case.TestCaseModel.validate_name","title":"<code>validate_name(v)</code>  <code>classmethod</code>","text":"<p>Validate name is not empty if provided.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"name\")\n@classmethod\ndef validate_name(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate name is not empty if provided.\"\"\"\n    if v is not None and (not v or not v.strip()):\n        raise ValueError(\"name must be non-empty if provided\")\n    return v\n</code></pre>"},{"location":"api/test-runner/#holodeck.models.test_case.FileInput","title":"<code>FileInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>File input for multimodal test cases.</p> <p>Represents a single file reference for test case inputs, supporting both local files and remote URLs with optional extraction parameters.</p>"},{"location":"api/test-runner/#holodeck.models.test_case.FileInput.check_path_or_url","title":"<code>check_path_or_url(v, info)</code>  <code>classmethod</code>","text":"<p>Validate that exactly one of path or url is provided.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"path\", \"url\", mode=\"before\")\n@classmethod\ndef check_path_or_url(cls, v: Any, info: Any) -&gt; Any:\n    \"\"\"Validate that exactly one of path or url is provided.\"\"\"\n    # This runs before validation, so we check in root_validator\n    return v\n</code></pre>"},{"location":"api/test-runner/#holodeck.models.test_case.FileInput.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Validate path and url mutual exclusivity after initialization.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>def model_post_init(self, __context: Any) -&gt; None:\n    \"\"\"Validate path and url mutual exclusivity after initialization.\"\"\"\n    if self.path and self.url:\n        raise ValueError(\"Cannot provide both 'path' and 'url'\")\n    if not self.path and not self.url:\n        raise ValueError(\"Must provide either 'path' or 'url'\")\n</code></pre>"},{"location":"api/test-runner/#holodeck.models.test_case.FileInput.validate_pages","title":"<code>validate_pages(v)</code>  <code>classmethod</code>","text":"<p>Validate pages are positive integers.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"pages\")\n@classmethod\ndef validate_pages(cls, v: list[int] | None) -&gt; list[int] | None:\n    \"\"\"Validate pages are positive integers.\"\"\"\n    if v is not None and not all(isinstance(p, int) and p &gt; 0 for p in v):\n        raise ValueError(\"pages must be positive integers\")\n    return v\n</code></pre>"},{"location":"api/test-runner/#holodeck.models.test_case.FileInput.validate_type","title":"<code>validate_type(v)</code>  <code>classmethod</code>","text":"<p>Validate file type is supported.</p> Source code in <code>src/holodeck/models/test_case.py</code> <pre><code>@field_validator(\"type\")\n@classmethod\ndef validate_type(cls, v: str) -&gt; str:\n    \"\"\"Validate file type is supported.\"\"\"\n    valid_types = {\"image\", \"pdf\", \"text\", \"excel\", \"word\", \"powerpoint\", \"csv\"}\n    if v not in valid_types:\n        raise ValueError(f\"type must be one of {valid_types}, got {v}\")\n    return v\n</code></pre>"},{"location":"api/test-runner/#test-results","title":"Test Results","text":"<p>Data models for test execution results and metrics.</p>"},{"location":"api/test-runner/#holodeck.models.test_result.TestResult","title":"<code>TestResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result of executing a single test case.</p> <p>Contains the test input, agent response, tool calls, metric results, and overall pass/fail status along with any errors encountered.</p>"},{"location":"api/test-runner/#example-usage","title":"Example Usage","text":"<pre><code>from holodeck.lib.test_runner.executor import TestExecutor\nfrom holodeck.config.loader import ConfigLoader\n\n# Load agent configuration\nloader = ConfigLoader()\nconfig = loader.load(\"agent.yaml\")\n\n# Create executor and run tests\nexecutor = TestExecutor()\nresults = executor.run_tests(config)\n\n# Access results\nfor test_result in results.test_results:\n    print(f\"Test {test_result.test_name}: {test_result.status}\")\n    print(f\"Metrics: {test_result.metrics}\")\n</code></pre>"},{"location":"api/test-runner/#multimodal-file-support","title":"Multimodal File Support","text":"<p>The test runner integrates with the file processor to handle:</p> <ul> <li>Images: JPG, PNG with OCR support</li> <li>Documents: PDF (full or page ranges), Word, PowerPoint</li> <li>Data: Excel (sheet/range selection), CSV, text files</li> <li>Remote Files: URL-based inputs with caching</li> </ul> <p>Files are automatically processed before agent invocation and included in test context.</p>"},{"location":"api/test-runner/#integration-with-evaluation-framework","title":"Integration with Evaluation Framework","text":"<p>Test results automatically pass through the evaluation framework:</p> <ol> <li>NLP Metrics: Computed on all test outputs (F1, BLEU, ROUGE, METEOR)</li> <li>AI-powered Metrics: Optional evaluation by Azure AI models</li> <li>Custom Metrics: User-defined evaluation functions</li> </ol> <p>Evaluation configuration comes from the agent's <code>evaluations</code> section.</p>"},{"location":"api/test-runner/#related-documentation","title":"Related Documentation","text":"<ul> <li>Data Models: Test case and result models</li> <li>Evaluation Framework: Metrics and evaluation system</li> <li>Configuration Loading: Loading agent configurations</li> </ul>"},{"location":"api/utilities/","title":"Utilities and Support API","text":"<p>HoloDeck provides several utility modules for template rendering and error handling.</p>"},{"location":"api/utilities/#template-engine","title":"Template Engine","text":"<p>Jinja2-based template rendering for dynamic configuration and instruction generation.</p>"},{"location":"api/utilities/#holodeck.lib.template_engine.TemplateRenderer","title":"<code>TemplateRenderer()</code>","text":"<p>Renders Jinja2 templates and validates output against schemas.</p> <p>Provides safe template rendering with: - Restricted Jinja2 filters for security - YAML validation against AgentConfig schema - Clear error messages for debugging</p> <p>Initialize the TemplateRenderer with a secure Jinja2 environment.</p> Source code in <code>src/holodeck/lib/template_engine.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the TemplateRenderer with a secure Jinja2 environment.\"\"\"\n    # Create Jinja2 environment with strict mode (undefined variables cause errors)\n    # autoescape disabled for YAML templates (appropriate for config generation)\n    self.env = Environment(\n        undefined=StrictUndefined,\n        trim_blocks=True,\n        lstrip_blocks=True,\n        autoescape=select_autoescape(\n            enabled_extensions=(\"html\", \"xml\"),\n            default_for_string=False,\n            default=False,\n        ),\n    )\n\n    # Only allow safe filters\n    self._setup_safe_filters()\n</code></pre>"},{"location":"api/utilities/#holodeck.lib.template_engine.TemplateRenderer.list_available_templates","title":"<code>list_available_templates()</code>  <code>staticmethod</code>","text":"<p>List all available built-in templates.</p> <p>Discovers templates from the templates/ directory structure.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of template names (e.g., ['conversational', 'research',</p> <code>list[str]</code> <p>'customer-support'])</p> Source code in <code>src/holodeck/lib/template_engine.py</code> <pre><code>@staticmethod\ndef list_available_templates() -&gt; list[str]:\n    \"\"\"List all available built-in templates.\n\n    Discovers templates from the templates/ directory structure.\n\n    Returns:\n        List of template names (e.g., ['conversational', 'research',\n        'customer-support'])\n    \"\"\"\n    templates_dir = Path(__file__).parent.parent / \"templates\"\n\n    if not templates_dir.exists():\n        return []\n\n    # List all directories that have a manifest.yaml (valid templates)\n    templates = []\n    for template_dir in templates_dir.iterdir():\n        if template_dir.is_dir() and not template_dir.name.startswith(\"_\"):\n            # Check if it has manifest.yaml\n            manifest = template_dir / \"manifest.yaml\"\n            if manifest.exists():\n                templates.append(template_dir.name)\n\n    return sorted(templates)\n</code></pre>"},{"location":"api/utilities/#holodeck.lib.template_engine.TemplateRenderer.render_and_validate","title":"<code>render_and_validate(template_path, variables)</code>","text":"<p>Render a Jinja2 template and validate output (for YAML files).</p> <p>Combines rendering and validation in a safe way: only returns rendered content if both rendering and validation succeed. This is the recommended way to process agent.yaml templates.</p> <p>Parameters:</p> Name Type Description Default <code>template_path</code> <code>str</code> <p>Path to the Jinja2 template file</p> required <code>variables</code> <code>dict[str, Any]</code> <p>Dictionary of variables to pass to the template</p> required <p>Returns:</p> Type Description <code>str</code> <p>Rendered and validated template content as a string</p> <p>Raises:</p> Type Description <code>InitError</code> <p>If rendering fails</p> <code>ValidationError</code> <p>If validation fails</p> Source code in <code>src/holodeck/lib/template_engine.py</code> <pre><code>def render_and_validate(self, template_path: str, variables: dict[str, Any]) -&gt; str:\n    \"\"\"Render a Jinja2 template and validate output (for YAML files).\n\n    Combines rendering and validation in a safe way: only returns\n    rendered content if both rendering and validation succeed.\n    This is the recommended way to process agent.yaml templates.\n\n    Args:\n        template_path: Path to the Jinja2 template file\n        variables: Dictionary of variables to pass to the template\n\n    Returns:\n        Rendered and validated template content as a string\n\n    Raises:\n        InitError: If rendering fails\n        ValidationError: If validation fails\n    \"\"\"\n    # Render template first\n    rendered = self.render_template(template_path, variables)\n\n    # Determine if this is agent.yaml specifically (not all YAML files)\n    template_file = Path(template_path)\n    is_agent_yaml = (\n        template_file.name == \"agent.yaml.j2\" or template_file.stem == \"agent.yaml\"\n    )\n\n    if is_agent_yaml:\n        # Validate YAML against schema\n        # This will raise ValidationError if invalid\n        self.validate_agent_config(rendered)\n\n    # Return rendered content (safe to write to disk)\n    return rendered\n</code></pre>"},{"location":"api/utilities/#holodeck.lib.template_engine.TemplateRenderer.render_template","title":"<code>render_template(template_path, variables)</code>","text":"<p>Render a Jinja2 template with provided variables.</p> <p>Parameters:</p> Name Type Description Default <code>template_path</code> <code>str</code> <p>Path to the Jinja2 template file</p> required <code>variables</code> <code>dict[str, Any]</code> <p>Dictionary of variables to pass to the template</p> required <p>Returns:</p> Type Description <code>str</code> <p>Rendered template content as a string</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If template file doesn't exist</p> <code>InitError</code> <p>If rendering fails (syntax errors, undefined variables, etc.)</p> Source code in <code>src/holodeck/lib/template_engine.py</code> <pre><code>def render_template(self, template_path: str, variables: dict[str, Any]) -&gt; str:\n    \"\"\"Render a Jinja2 template with provided variables.\n\n    Args:\n        template_path: Path to the Jinja2 template file\n        variables: Dictionary of variables to pass to the template\n\n    Returns:\n        Rendered template content as a string\n\n    Raises:\n        FileNotFoundError: If template file doesn't exist\n        InitError: If rendering fails (syntax errors, undefined variables, etc.)\n    \"\"\"\n    from holodeck.cli.exceptions import InitError\n\n    template_file = Path(template_path)\n\n    if not template_file.exists():\n        raise FileNotFoundError(f\"Template file not found: {template_path}\")\n\n    try:\n        # Load template from file\n        loader = FileSystemLoader(str(template_file.parent))\n        env = Environment(\n            loader=loader,\n            undefined=StrictUndefined,\n            trim_blocks=True,\n            lstrip_blocks=True,\n            autoescape=select_autoescape(\n                enabled_extensions=(\"html\", \"xml\"),\n                default_for_string=False,\n                default=False,\n            ),\n        )\n\n        template = env.get_template(template_file.name)\n\n        # Render template\n        return template.render(variables)\n\n    except TemplateSyntaxError as e:\n        raise InitError(\n            f\"Template syntax error in {template_path}:\\n\"\n            f\"  Line {e.lineno}: {e.message}\"\n        ) from e\n    except UndefinedError as e:\n        raise InitError(\n            f\"Template rendering error in {template_path}:\\n\"\n            f\"  Undefined variable: {str(e)}\"\n        ) from e\n    except Exception as e:\n        raise InitError(f\"Template rendering failed: {str(e)}\") from e\n</code></pre>"},{"location":"api/utilities/#holodeck.lib.template_engine.TemplateRenderer.validate_agent_config","title":"<code>validate_agent_config(yaml_content)</code>","text":"<p>Validate YAML content against Agent schema.</p> <p>Parses YAML and validates it against the Agent Pydantic model. This is the critical validation gate for agent.yaml files.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_content</code> <code>str</code> <p>YAML content as a string</p> required <p>Returns:</p> Name Type Description <code>Agent</code> <code>Agent</code> <p>Validated Agent configuration object</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If YAML is invalid or doesn't match schema</p> <code>InitError</code> <p>If parsing fails</p> Source code in <code>src/holodeck/lib/template_engine.py</code> <pre><code>def validate_agent_config(self, yaml_content: str) -&gt; Agent:\n    \"\"\"Validate YAML content against Agent schema.\n\n    Parses YAML and validates it against the Agent Pydantic model.\n    This is the critical validation gate for agent.yaml files.\n\n    Args:\n        yaml_content: YAML content as a string\n\n    Returns:\n        Agent: Validated Agent configuration object\n\n    Raises:\n        ValidationError: If YAML is invalid or doesn't match schema\n        InitError: If parsing fails\n    \"\"\"\n    from holodeck.cli.exceptions import InitError, ValidationError\n\n    try:\n        # Parse YAML\n        data = yaml.safe_load(yaml_content)\n\n        if not data:\n            raise ValidationError(\"agent.yaml content is empty\")\n\n        # Validate against Agent schema\n        agent = Agent.model_validate(data)\n        return agent\n\n    except yaml.YAMLError as e:\n        raise ValidationError(f\"YAML parsing error:\\n\" f\"  {str(e)}\") from e\n    except ValidationError:\n        # Re-raise our validation errors as-is\n        raise\n    except Exception as e:\n        # Catch Pydantic validation errors\n        if hasattr(e, \"errors\"):\n            # Pydantic ValidationError\n            errors = e.errors()\n            error_msg = \"Agent configuration validation failed:\\n\"\n            for error in errors:\n                field = \".\".join(str(loc) for loc in error[\"loc\"])\n                error_msg += f\"  {field}: {error['msg']}\\n\"\n            raise ValidationError(error_msg) from e\n        else:\n            raise InitError(\n                f\"Agent configuration validation failed: {str(e)}\"\n            ) from e\n</code></pre>"},{"location":"api/utilities/#usage-examples","title":"Usage Examples","text":""},{"location":"api/utilities/#template-rendering","title":"Template Rendering","text":"<pre><code>from holodeck.lib.template_engine import TemplateRenderer\n\nrenderer = TemplateRenderer()\n\n# Render inline template\nresult = renderer.render_template(\n    \"template_string\",\n    {\"name\": \"Alice\"}\n)\n</code></pre>"},{"location":"api/utilities/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Loading: Using template engine with configs</li> <li>CLI Commands: Project initialization and CLI API</li> </ul>"},{"location":"examples/","title":"Agent Configuration Examples","text":"<p>This directory contains example agent.yaml files demonstrating different features and patterns. Start with <code>basic_agent.yaml</code> and progress to more complex configurations as needed.</p>"},{"location":"examples/#quick-reference","title":"Quick Reference","text":"Example Use Case Features Demonstrated <code>basic_agent.yaml</code> Getting started Minimal valid config, inline instructions, no tools <code>with_tools.yaml</code> Real-world workflows All 4 tool types: vectorstore, function, MCP, prompt <code>with_evaluations.yaml</code> Quality assurance DeepEval metrics, NLP metrics, per-metric model override <code>with_global_config.yaml</code> Multi-environment setups Config precedence, env var substitution, inheritance"},{"location":"examples/#examples","title":"Examples","text":""},{"location":"examples/#basic_agentyaml","title":"<code>basic_agent.yaml</code>","text":"<p>Purpose: Minimal valid agent configuration</p> <p>Features: - Simple agent metadata (name, description) - OpenAI model provider (gpt-4o-mini) - Inline system instructions (no external files) - No tools (can run standalone for chat)</p> <p>When to use: - Learning basic agent.yaml structure - Testing configuration loading without tool complexity - Building a simple chatbot or Q&amp;A assistant</p> <p>Try it: <pre><code># Set your API key\nexport OPENAI_API_KEY=your-key-here\n\n# Run the agent\nholodeck run basic_agent.yaml\n\n# Or validate configuration\nholodeck validate basic_agent.yaml\n</code></pre></p> <p>Key Concepts: - <code>model</code> \u2192 Required. Specifies LLM provider, model name, and generation settings - <code>instructions</code> \u2192 Required. Can be inline (shown here) or from a file - Minimal config is valid and functional</p>"},{"location":"examples/#with_toolsyaml","title":"<code>with_tools.yaml</code>","text":"<p>Purpose: Comprehensive tool integration example</p> <p>Features: - Vectorstore tool: Semantic search over documentation - Function tool: Custom Python function execution - MCP tool: Standardized integrations (filesystem, databases, APIs) - Prompt tool: LLM-powered semantic functions - Test cases with expected tool validation</p> <p>When to use: - Building agents that need external data access - Executing custom business logic - Integrating standardized tools (GitHub, Slack, databases) - AI-powered data processing</p> <p>Prerequisites: <pre><code># Create the required files this example references:\nmkdir -p ./data/docs ./tools\n\n# Create a simple documentation file\necho \"Installation: Run 'pip install holodeck'\" &gt; ./data/docs/getting_started.txt\n\n# Create a Python tool file (tools/discount_calculator.py)\ncat &gt; ./tools/discount_calculator.py &lt;&lt; 'EOF'\ndef calculate_discount(customer_tier: str, order_amount: float, applied_coupon: str = None) -&gt; dict:\n    \"\"\"Calculate order discount based on tier and amount.\"\"\"\n    tier_discounts = {\n        'bronze': 0.05,\n        'silver': 0.10,\n        'gold': 0.15,\n        'platinum': 0.25\n    }\n\n    base_discount = tier_discounts.get(customer_tier.lower(), 0)\n    coupon_discount = 0.05 if applied_coupon else 0\n\n    total_discount = min(base_discount + coupon_discount, 0.50)  # Cap at 50%\n    discount_amount = order_amount * total_discount\n\n    return {\n        'original_amount': order_amount,\n        'discount_percent': int(total_discount * 100),\n        'discount_amount': discount_amount,\n        'final_amount': order_amount - discount_amount\n    }\nEOF\n\n# Create instructions file (instructions.txt)\ncat &gt; ./instructions.txt &lt;&lt; 'EOF'\nYou are a customer service agent with access to:\n- Documentation database (docs-search tool)\n- Discount calculation system (calculate-discount tool)\n- File system access (file-browser tool)\n- Sentiment analysis (sentiment-analyzer tool)\n\nUse the most appropriate tool for each customer request.\nEOF\n</code></pre></p> <p>Try it: <pre><code>export OPENAI_API_KEY=your-key-here\nexport ANTHROPIC_API_KEY=your-key-here\n\nholodeck run with_tools.yaml\n</code></pre></p> <p>Tool Types Explained: 1. Vectorstore: Semantic search\u2014find relevant documents based on meaning, not keywords 2. Function: Execute Python code\u2014calculate, transform, validate data 3. MCP: Standardized integrations\u2014filesystem, GitHub, databases, Slack 4. Prompt: LLM-powered\u2014use AI to process data (sentiment analysis, summarization)</p> <p>Key Concepts: - Tool types are discriminated by the <code>type</code> field - Each tool type has specific required fields - Tools are composable\u2014agents can use multiple tool types together - File paths are relative to the agent.yaml location</p>"},{"location":"examples/#with_evaluationsyaml","title":"<code>with_evaluations.yaml</code>","text":"<p>Purpose: Quality assurance and evaluation framework</p> <p>Features: - DeepEval GEval metrics: Custom criteria with chain-of-thought evaluation (recommended) - DeepEval RAG metrics: Faithfulness, answer relevancy (recommended) - NLP metrics: F1 score, ROUGE (standard) - Legacy AI metrics: Deprecated Azure AI metrics (backwards compatibility) - Per-metric model overrides - Threshold-based pass/fail criteria</p> <p>When to use: - Validating agent response quality - Ensuring responses are grounded in data (faithfulness) - Measuring accuracy against ground truth - Running quality gates in production pipelines</p> <p>Metric Types (in order of recommendation):</p> Tier Type Metrics Use Case 1 (Recommended) DeepEval GEval Custom criteria Flexible semantic evaluation with natural language 1 (Recommended) DeepEval RAG <code>faithfulness</code>, <code>answer_relevancy</code>, <code>contextual_relevancy</code>, <code>contextual_precision</code>, <code>contextual_recall</code> RAG pipeline evaluation 2 (Standard) NLP <code>f1_score</code>, <code>bleu</code>, <code>rouge</code>, <code>meteor</code> Token-level comparison with ground truth 3 (Deprecated) Legacy AI <code>groundedness</code>, <code>relevance</code>, <code>coherence</code>, <code>safety</code> Azure AI-based (migrate to DeepEval) <p>DeepEval Metrics Example: <pre><code>evaluations:\n  model:\n    provider: ollama\n    name: llama3.2:latest\n    temperature: 0.0\n\n  metrics:\n    # GEval: Custom criteria\n    - type: geval\n      name: \"Coherence\"\n      criteria: \"Evaluate whether the response is clear and well-structured.\"\n      threshold: 0.7\n\n    # RAG: Hallucination detection\n    - type: rag\n      metric_type: faithfulness\n      threshold: 0.8\n\n    # RAG: Response relevance\n    - type: rag\n      metric_type: answer_relevancy\n      threshold: 0.7\n</code></pre></p> <p>Try it: <pre><code># For local evaluation (free, no API keys needed)\n# Make sure Ollama is running with llama3.2:latest\n\n# Run evaluations\nholodeck test with_evaluations.yaml\n\n# Run with verbose output\nholodeck test with_evaluations.yaml --verbose\n</code></pre></p> <p>Configuration Precedence: <pre><code>evaluations:\n  model:                              # Global model (applies to all metrics)\n    provider: ollama\n    name: llama3.2:latest\n  metrics:\n    - type: geval\n      name: \"Quality\"\n      criteria: \"...\"\n      model:                          # Per-metric override (highest precedence)\n        provider: openai\n        name: gpt-4                   # Use powerful model for critical metric\n</code></pre></p> <p>Key Concepts: - Evaluations run after agent execution completes - Each metric can override the evaluation model - <code>threshold</code> defines minimum passing score (0-1 scale) - <code>fail_on_error: false</code> = soft failure (evaluation error doesn't block) - <code>fail_on_error: true</code> = hard failure (evaluation error stops test) - DeepEval metrics support local models via Ollama (free)</p> <p>Legacy Metric Migration:</p> Legacy Metric Recommended Replacement <code>groundedness</code> <code>type: rag</code>, <code>metric_type: faithfulness</code> <code>relevance</code> <code>type: rag</code>, <code>metric_type: answer_relevancy</code> <code>coherence</code> <code>type: geval</code> with custom criteria <code>safety</code> <code>type: geval</code> with custom criteria"},{"location":"examples/#with_global_configyaml","title":"<code>with_global_config.yaml</code>","text":"<p>Purpose: Configuration precedence and environment-specific setup</p> <p>Features: - Environment variable substitution (<code>${VAR_NAME}</code>) - Configuration inheritance from global config - Agent-specific overrides - Multi-environment setup (dev/staging/prod)</p> <p>Global Config Location: <code>~/.holodeck/config.yaml</code></p> <p>Sample Global Config: <pre><code># ~/.holodeck/config.yaml\nmodel:\n  provider: openai\n  name: gpt-4o-mini\n  temperature: 0.7\n\ndeployment:\n  endpoint_prefix: /api/v1\n\nproviders:\n  openai:\n    api_key: ${OPENAI_API_KEY}\n  azure:\n    api_key: ${AZURE_API_KEY}\n    endpoint: ${AZURE_ENDPOINT}\n</code></pre></p> <p>Configuration Precedence (highest to lowest): 1. Agent-specific settings (this file): Explicit values in agent.yaml 2. Environment variables: <code>${VAR_NAME}</code> resolved at runtime 3. Global config: <code>~/.holodeck/config.yaml</code> applied as defaults</p> <p>Try it: <pre><code># Set environment variables\nexport AZURE_API_KEY=your-key-here\nexport AZURE_ENDPOINT=https://your-instance.openai.azure.com/\n\n# Create global config\nmkdir -p ~/.holodeck\ncat &gt; ~/.holodeck/config.yaml &lt;&lt; 'EOF'\nmodel:\n  provider: openai\n  name: gpt-4o-mini\n  temperature: 0.7\nEOF\n\n# Run agent (uses merged config)\nholodeck run with_global_config.yaml\n</code></pre></p> <p>Key Concepts: - Global config provides defaults for all agents - Agent.yaml overrides global settings - Environment variables fill sensitive values (API keys) - Substitution pattern: <code>${VARIABLE_NAME}</code> - Missing environment variables cause errors at config load time</p> <p>Multi-Environment Example: <pre><code># Development\nexport OPENAI_API_KEY=sk-dev-...\nexport ENV=development\n\n# Staging\nexport OPENAI_API_KEY=sk-staging-...\nexport ENV=staging\n\n# Production\nexport OPENAI_API_KEY=sk-prod-...\nexport ENV=production\n\n# Same agent.yaml works in all environments\nholodeck run with_global_config.yaml\n</code></pre></p>"},{"location":"examples/#common-patterns","title":"Common Patterns","text":""},{"location":"examples/#pattern-1-development-vs-production","title":"Pattern 1: Development vs. Production","text":"<pre><code># Use global config for dev defaults\n# Override in agent.yaml for production\n\n# agent.yaml\nmodel:\n  provider: openai\n  name: ${MODEL_NAME}  # env: gpt-4o-mini (dev) or gpt-4o (prod)\n  temperature: ${TEMPERATURE}  # env: 0.7 (dev) or 0.3 (prod)\n</code></pre>"},{"location":"examples/#pattern-2-sensitive-data","title":"Pattern 2: Sensitive Data","text":"<pre><code># Never commit API keys\n# Use environment variables or global config\n\ninstructions:\n  inline: |\n    Use the API token from environment variable for authentication.\n\ntools:\n  - name: api-client\n    type: function\n    file: ./tools/api.py\n    # API key injected via ${API_KEY} at runtime\n</code></pre>"},{"location":"examples/#pattern-3-modular-configurations","title":"Pattern 3: Modular Configurations","text":"<pre><code># Split large configurations\n\n# main_agent.yaml\nname: multi-step-agent\ninstructions:\n  file: ./instructions.md  # Separate file\n\ntools:\n  # Reference tool configs in separate files (if using advanced tooling)\n  - name: tool1\n    type: vectorstore\n    source: ./data/kb/\n</code></pre>"},{"location":"examples/#pattern-4-cost-effective-evaluation","title":"Pattern 4: Cost-Effective Evaluation","text":"<pre><code># Use local models for development, paid APIs for production\n\nevaluations:\n  model:\n    provider: ollama           # Free, local (development)\n    name: llama3.2:latest\n\n  metrics:\n    - type: geval\n      name: \"Quality\"\n      criteria: \"...\"\n      # Uses local model by default\n\n    - type: rag\n      metric_type: faithfulness\n      model:\n        provider: openai       # Override for critical metric\n        name: gpt-4\n</code></pre>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ol> <li>Start with <code>basic_agent.yaml</code>: Understand structure</li> <li>Progress to <code>with_tools.yaml</code>: Add tool integration</li> <li>Explore <code>with_evaluations.yaml</code>: Add quality gates with DeepEval</li> <li>Deploy with <code>with_global_config.yaml</code>: Production setup</li> </ol> <p>For more information: - See docs/guides/agent-configuration.md for schema reference - See docs/guides/tools.md for tool type details - See docs/guides/evaluations.md for evaluation configuration - See docs/guides/global-config.md for precedence rules</p>"},{"location":"examples/#troubleshooting","title":"Troubleshooting","text":"<p>Q: ConfigError when loading agent.yaml - Check file paths (relative to agent.yaml location) - Verify all required fields are present - Ensure YAML syntax is valid</p> <p>Q: Tool execution fails - Verify tool files exist and are readable - Check Python function names match tool configuration - Ensure vectorstore source path contains data</p> <p>Q: Environment variable not substituted - Use <code>${VARIABLE_NAME}</code> syntax - Set variable before running: <code>export VARIABLE_NAME=value</code> - Check for typos in variable names</p> <p>Q: Evaluations run but show errors - If <code>fail_on_error: false</code>, errors are logged but don't block - Check model API keys are set (or use Ollama for local evaluation) - Verify ground_truth and test input are clear and specific</p> <p>Q: DeepEval metrics not working - Ensure Ollama is running if using local models - Check that required parameters are available (e.g., retrieval_context for faithfulness) - Try with a simpler model first to debug</p> <p>Q: Legacy metrics deprecated warning - Migrate to DeepEval equivalents (see migration table above) - Legacy metrics still work but will be removed in future versions</p> <p>Created: 2025-10-19 | Updated: 2025-11-30 | Version: 0.2.0</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>Get HoloDeck installed and ready to build AI agents.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ (check with <code>python --version</code>)</li> <li>uv - The fast Python package installer</li> </ul>"},{"location":"getting-started/installation/#installing-uv","title":"Installing uv","text":"<p>If you don't have uv installed:</p> <pre><code># macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# macOS (Homebrew)\nbrew install uv\n\n# Windows\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <p>Verify uv is installed:</p> <pre><code>uv --version\n</code></pre>"},{"location":"getting-started/installation/#install-holodeck-cli","title":"Install HoloDeck CLI","text":"<p>Install HoloDeck as a global tool using uv:</p> <pre><code>uv tool install holodeck-ai@latest --prerelease allow --python 3.10\n</code></pre> <p>This installs the <code>holodeck</code> command-line tool globally, available from any directory.</p>"},{"location":"getting-started/installation/#install-with-vector-store-providers-optional","title":"Install with Vector Store Providers (Optional)","text":"<p>If you plan to use semantic search with vector databases, install with extras:</p> <pre><code># Individual providers\nuv tool install \"holodeck-ai[postgres]@latest\" --prerelease allow --python 3.10\nuv tool install \"holodeck-ai[qdrant]@latest\" --prerelease allow --python 3.10\nuv tool install \"holodeck-ai[pinecone]@latest\" --prerelease allow --python 3.10\nuv tool install \"holodeck-ai[chromadb]@latest\" --prerelease allow --python 3.10\n\n# Or install all vector store providers at once\nuv tool install \"holodeck-ai[vectorstores]@latest\" --prerelease allow --python 3.10\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Check that HoloDeck is installed correctly:</p> <pre><code>holodeck --version\n# Output: holodeck 0.2.0\n</code></pre> <p>View available commands:</p> <pre><code>holodeck --help\n</code></pre>"},{"location":"getting-started/installation/#set-up-llm-provider","title":"Set Up LLM Provider","text":"<p>HoloDeck supports multiple LLM providers. Ollama is recommended for local development as it requires no API keys and runs entirely on your machine.</p>"},{"location":"getting-started/installation/#ollama-recommended","title":"Ollama (Recommended)","text":"<p>Ollama runs LLMs locally on your machine - no API keys required.</p> <p>Install Ollama:</p> <pre><code># macOS\nbrew install ollama\n\n# Linux\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Windows: Download from https://ollama.com/download\n</code></pre> <p>Start Ollama and pull a model:</p> <pre><code># Start the Ollama service\nollama serve\n\n# Pull a model (in another terminal)\nollama pull llama3.2\n# Or for a smaller model:\nollama pull phi3\n</code></pre> <p>Verify Ollama is running:</p> <pre><code>curl http://localhost:11434/api/tags\n</code></pre> <p>No environment variables needed - HoloDeck connects to Ollama at <code>http://localhost:11434</code> by default.</p>"},{"location":"getting-started/installation/#cloud-providers-optional","title":"Cloud Providers (Optional)","text":"<p>For cloud-based LLMs, set up credentials using environment variables or a <code>.env</code> file.</p>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"<pre><code># OpenAI\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Azure OpenAI\nexport AZURE_OPENAI_API_KEY=\"your-key-here\"\nexport AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\"\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre>"},{"location":"getting-started/installation/#env-file","title":"<code>.env</code> File","text":"<p>Create a <code>.env</code> file in your project directory:</p> <pre><code># .env (never commit this file!)\nOPENAI_API_KEY=sk-...\n</code></pre> <p>Add <code>.env</code> to <code>.gitignore</code>:</p> <pre><code>echo \".env\" &gt;&gt; .gitignore\necho \".env.local\" &gt;&gt; .gitignore\n</code></pre>"},{"location":"getting-started/installation/#supported-llm-providers","title":"Supported LLM Providers","text":"<p>HoloDeck supports multiple LLM providers:</p>"},{"location":"getting-started/installation/#ollama-recommended_1","title":"Ollama (Recommended)","text":"<p>Run LLMs locally with no API keys. Supports Llama, Mistral, Phi, and many more models.</p> <pre><code># No environment variables needed\n# Default endpoint: http://localhost:11434\n</code></pre>"},{"location":"getting-started/installation/#openai","title":"OpenAI","text":"<pre><code>OPENAI_API_KEY=sk-...\nOPENAI_ORG_ID=optional-org-id\n</code></pre>"},{"location":"getting-started/installation/#azure-openai","title":"Azure OpenAI","text":"<pre><code>AZURE_OPENAI_API_KEY=your-key-here\nAZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/\n</code></pre>"},{"location":"getting-started/installation/#anthropic","title":"Anthropic","text":"<pre><code>ANTHROPIC_API_KEY=sk-ant-...\n</code></pre>"},{"location":"getting-started/installation/#upgrading-holodeck","title":"Upgrading HoloDeck","text":"<p>To upgrade to the latest version:</p> <pre><code>uv tool upgrade holodeck-ai\n</code></pre> <p>To reinstall with a specific version:</p> <pre><code>uv tool install holodeck-ai@0.3.0 --prerelease allow --python 3.10 --force\n</code></pre>"},{"location":"getting-started/installation/#uninstalling","title":"Uninstalling","text":"<p>To remove HoloDeck:</p> <pre><code>uv tool uninstall holodeck-ai\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#python-310-required","title":"\"Python 3.10+ required\"","text":"<p>Check your Python version and upgrade if needed:</p> <pre><code>python --version\n\n# macOS (Homebrew)\nbrew install python@3.10\n\n# Ubuntu/Debian\nsudo apt-get install python3.10\n\n# Windows: Download from python.org\n</code></pre>"},{"location":"getting-started/installation/#holodeck-command-not-found","title":"\"holodeck: command not found\"","text":"<p>The CLI isn't in your PATH. Try:</p> <pre><code># Reinstall HoloDeck\nuv tool install holodeck-ai@latest --prerelease allow --python 3.10 --force\n\n# Ensure uv tools are in PATH\n# Add to your shell profile (~/.bashrc, ~/.zshrc, etc.):\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n# Then reload your shell\nsource ~/.zshrc  # or ~/.bashrc\n</code></pre>"},{"location":"getting-started/installation/#uv-command-not-found","title":"\"uv: command not found\"","text":"<p>Install uv first. See Installing uv above.</p>"},{"location":"getting-started/installation/#error-api-key-not-found-or-invalid-credentials","title":"\"Error: API key not found\" or \"Invalid credentials\"","text":"<p>Verify your environment variables are set:</p> <pre><code># Check if variables are set\necho $AZURE_OPENAI_API_KEY  # macOS/Linux\necho %AZURE_OPENAI_API_KEY%  # Windows\n\n# Or check .env file exists\ncat .env\n</code></pre> <p>If using a <code>.env</code> file, ensure it's in your project directory.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart Guide - Build your first agent in 5 minutes</li> <li>Agent Configuration Guide - Full configuration reference</li> <li>Example Agents - Browse example agents</li> <li>Global Configuration - Configure defaults</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quickstart Guide","text":"<p>Get your first AI agent running in 5 minutes using the HoloDeck CLI.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Complete the Installation Guide before continuing.</p>"},{"location":"getting-started/quickstart/#quick-start-with-cli","title":"Quick Start with CLI","text":""},{"location":"getting-started/quickstart/#step-1-initialize-a-new-agent-project","title":"Step 1: Initialize a New Agent Project","text":"<p>Use the <code>holodeck init</code> command to create a new project with templates:</p> <pre><code># Create a basic conversational agent\nholodeck init my-chatbot\n\n# Or choose a different template\nholodeck init research-agent --template research\nholodeck init support-bot --template customer-support\n\n# With metadata\nholodeck init my-agent --description \"My AI agent\" --author \"Your Name\"\n</code></pre> <p>This creates a complete project structure:</p> <pre><code>my-chatbot/\n\u251c\u2500\u2500 agent.yaml              # Main configuration\n\u251c\u2500\u2500 instructions/\n\u2502   \u2514\u2500\u2500 system-prompt.md   # Agent behavior\n\u251c\u2500\u2500 tools/                 # Custom functions\n\u251c\u2500\u2500 data/                  # Grounding data\n\u2514\u2500\u2500 tests/                 # Test cases\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-edit-your-agent-configuration","title":"Step 2: Edit Your Agent Configuration","text":"<pre><code>cd my-chatbot\n</code></pre> <p>Open <code>agent.yaml</code> and customize:</p> <ul> <li>Agent name and description</li> <li>Model provider (OpenAI, Azure, Anthropic)</li> <li>Instructions/system prompt</li> <li>Tools and data sources</li> <li>Test cases</li> </ul>"},{"location":"getting-started/quickstart/#step-3-run-your-agent","title":"Step 3: Run Your Agent","text":""},{"location":"getting-started/quickstart/#interactive-chat","title":"Interactive Chat","text":"<p>Start an interactive chat session with your agent:</p> <pre><code># Basic chat (from parent directory)\nholodeck chat my-chatbot/agent.yaml\n\n# Or cd into the project directory first\ncd my-chatbot\nholodeck chat agent.yaml\n\n# Verbose mode with detailed status panel\nholodeck chat my-chatbot/agent.yaml --verbose\n\n# Quiet mode (no logging, but spinner still shows)\nholodeck chat my-chatbot/agent.yaml --quiet\n</code></pre> <p>Chat Features:</p> <ul> <li>Animated Spinner: Shows braille animation during agent execution (even in quiet mode)</li> <li>Token Tracking: Displays cumulative token usage across the conversation</li> <li>Adaptive Status Display:</li> <li>Default mode: Inline status <code>[messages | execution time]</code></li> <li>Verbose mode: Rich status panel with token breakdown</li> <li>Quiet mode: No status display (spinner only)</li> </ul> <p>Example verbose output:</p> <pre><code>\u256d\u2500\u2500\u2500 Chat Status \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Session Time: 00:05:23                  \u2502\n\u2502 Messages: 3 / 50 (6%)                   \u2502\n\u2502 Total Tokens: 1,234                     \u2502\n\u2502   \u251c\u2500 Prompt: 890                        \u2502\n\u2502   \u2514\u2500 Completion: 344                    \u2502\n\u2502 Last Response: 1.2s                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nAgent: Your response here\n</code></pre>"},{"location":"getting-started/quickstart/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests (from parent directory)\nholodeck test my-chatbot/agent.yaml\n\n# Deploy locally\nholodeck deploy my-chatbot/agent.yaml --port 8000\n</code></pre>"},{"location":"getting-started/quickstart/#manual-setup-alternative","title":"Manual Setup (Alternative)","text":"<p>If you prefer to create files manually instead of using <code>holodeck init</code>:</p>"},{"location":"getting-started/quickstart/#step-1-create-your-agent-configuration","title":"Step 1: Create Your Agent Configuration","text":"<p>Create <code>agent.yaml</code>:</p> <pre><code>name: \"my-assistant\"\ndescription: \"A helpful AI assistant\"\n\nmodel:\n  provider: azure_openai\n  # Provider settings come from config.yaml\n\ninstructions:\n  inline: |\n    You are a helpful AI assistant.\n    Answer questions accurately and concisely.\n\ntest_cases:\n  - name: \"greeting\"\n    input: \"Hello! What can you do?\"\n    ground_truth: \"I can help you with information and answer questions.\"\n    evaluations:\n      - f1_score\n\nevaluations:\n  model:\n    provider: azure_openai\n\n  metrics:\n    - metric: f1_score\n      threshold: 0.7\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-create-project-configuration","title":"Step 2: Create Project Configuration","text":"<p>Initialize <code>config.yaml</code> using the CLI:</p> <pre><code>holodeck config init -p\n</code></pre> <p>Then edit the generated file to configure your LLM provider:</p> <pre><code># config.yaml\nproviders:\n  azure_openai:\n    provider: azure_openai\n    name: gpt-4o\n    temperature: 0.3\n    max_tokens: 2048\n    endpoint: ${AZURE_OPENAI_ENDPOINT}\n    api_key: ${AZURE_OPENAI_API_KEY}\n\nexecution:\n  llm_timeout: 60\n  file_timeout: 30\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-test-your-agent","title":"Step 3: Test Your Agent","text":"<pre><code># Run agent tests\nholodeck test agent.yaml\n\n# Chat interactively\nholodeck chat agent.yaml\n</code></pre>"},{"location":"getting-started/quickstart/#common-commands","title":"Common Commands","text":"<pre><code># Show all available commands\nholodeck --help\n\n# Create new project from template\nholodeck init my-project\n\n# Test your agent and run evaluations\nholodeck test my-project/agent.yaml\n\n# Interactive chat with your agent\nholodeck chat my-project/agent.yaml\n\n# Chat with verbose output (detailed status panel)\nholodeck chat my-project/agent.yaml --verbose\n\n# Chat with quiet mode (no logging, spinner only)\nholodeck chat my-project/agent.yaml --quiet\n\n# Chat with custom max messages limit\nholodeck chat my-project/agent.yaml --max-messages 100\n\n# Deploy as API\nholodeck deploy my-project/agent.yaml --port 8000\n</code></pre>"},{"location":"getting-started/quickstart/#tips-tricks","title":"Tips &amp; Tricks","text":""},{"location":"getting-started/quickstart/#interactive-chat-features","title":"Interactive Chat Features","text":"<p>The <code>holodeck chat</code> command provides real-time feedback during conversation:</p> <p>Default Mode (Recommended for Most Users)</p> <pre><code>holodeck chat agent.yaml\n</code></pre> <p>Shows inline status after each response:</p> <pre><code>Agent: Your response here [3/50 | 1.2s]\n</code></pre> <p>Verbose Mode (For Detailed Monitoring)</p> <pre><code>holodeck chat agent.yaml --verbose\n</code></pre> <p>Displays a rich status panel with token breakdown:</p> <pre><code>\u256d\u2500\u2500\u2500 Chat Status \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Session Time: 00:05:23                  \u2502\n\u2502 Messages: 3 / 50 (6%)                   \u2502\n\u2502 Total Tokens: 1,234                     \u2502\n\u2502   \u251c\u2500 Prompt: 890                        \u2502\n\u2502   \u2514\u2500 Completion: 344                    \u2502\n\u2502 Last Response: 1.2s                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Quiet Mode (For Clean Output)</p> <pre><code>holodeck chat agent.yaml --quiet\n</code></pre> <p>Shows only responses and spinner during execution, no status display.</p> <p>Monitor Token Usage All chat modes track cumulative token usage across the conversation. This helps you:</p> <ul> <li>Monitor API costs in real-time</li> <li>Understand token consumption patterns</li> <li>Plan for context window limits</li> <li>Optimize prompt sizes</li> </ul>"},{"location":"getting-started/quickstart/#use-environment-variables-for-secrets","title":"Use Environment Variables for Secrets","text":"<p>Never hardcode API keys in config files. Always use environment variables:</p> <pre><code># config.yaml\nproviders:\n  azure_openai:\n    provider: azure_openai\n    api_key: ${AZURE_OPENAI_API_KEY} # From environment\n    endpoint: ${AZURE_OPENAI_ENDPOINT} # From environment\n</code></pre>"},{"location":"getting-started/quickstart/#test-locally-before-deploying","title":"Test Locally Before Deploying","text":"<pre><code># Run tests first\nholodeck test my-project/agent.yaml\n\n# Then try interactive chat\nholodeck chat my-project/agent.yaml\n\n# Finally deploy if tests pass\nholodeck deploy my-project/agent.yaml --port 8000\n</code></pre>"},{"location":"getting-started/quickstart/#organize-multiple-agents","title":"Organize Multiple Agents","text":"<p>If you have multiple agents, create separate directories:</p> <pre><code>my-project/\n\u251c\u2500\u2500 config.yaml          # Shared configuration\n\u251c\u2500\u2500 agent1/\n\u2502   \u2514\u2500\u2500 agent.yaml\n\u251c\u2500\u2500 agent2/\n\u2502   \u2514\u2500\u2500 agent.yaml\n\u2514\u2500\u2500 .env                 # Shared credentials\n</code></pre> <p>Test each agent:</p> <pre><code>holodeck test agent1/agent.yaml\nholodeck test agent2/agent.yaml\n</code></pre>"},{"location":"getting-started/quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>For installation issues, see the Installation Guide.</p>"},{"location":"getting-started/quickstart/#error-configyaml-not-found","title":"\"Error: config.yaml not found\"","text":"<p>Create a <code>config.yaml</code> file in your project directory. See Manual Setup above.</p>"},{"location":"getting-started/quickstart/#error-api-key-not-found-or-invalid-credentials","title":"\"Error: API key not found\" or \"Invalid credentials\"","text":"<p>Verify your environment variables:</p> <pre><code># Check if set\necho $AZURE_OPENAI_API_KEY\n\n# Or verify .env file\ncat .env\n</code></pre> <p>Make sure <code>.env</code> is in the same directory as <code>agent.yaml</code>.</p>"},{"location":"getting-started/quickstart/#error-failed-to-load-agentyaml","title":"\"Error: Failed to load agent.yaml\"","text":"<p>Check your YAML syntax:</p> <pre><code># Validate YAML\npython -c \"import yaml; yaml.safe_load(open('agent.yaml'))\"\n</code></pre> <p>Common issues:</p> <ul> <li>Incorrect indentation (must use spaces, not tabs)</li> <li>Missing required fields: <code>name</code>, <code>model.provider</code>, <code>instructions</code></li> <li>Invalid YAML syntax</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Agent Configuration Reference</li> <li>Explore Tool Types</li> <li>Learn About Evaluations</li> <li>Browse Examples</li> <li>Global Configuration Guide</li> </ul>"},{"location":"getting-started/quickstart/#getting-help","title":"Getting Help","text":"<ul> <li>Report bugs: GitHub Issues</li> <li>Ask questions: GitHub Discussions</li> <li>Full docs: https://docs.useholodeck.ai</li> </ul>"},{"location":"guides/agent-configuration/","title":"Agent Configuration Guide","text":"<p>This guide explains how to define AI agents using HoloDeck's <code>agent.yaml</code> configuration file.</p>"},{"location":"guides/agent-configuration/#overview","title":"Overview","text":"<p>An agent configuration file defines a single AI agent with everything it needs:</p> <ul> <li>Model settings: Which LLM provider to use (OpenAI, Azure, Anthropic)</li> <li>Instructions: System prompt (from a file or inline)</li> <li>Tools: Capabilities the agent can use (search, functions, APIs, templates)</li> <li>Evaluations: Metrics to measure quality</li> <li>Test cases: Scenarios to validate the agent</li> </ul> <p>All configuration is declarative\u2014no code required.</p>"},{"location":"guides/agent-configuration/#basic-structure","title":"Basic Structure","text":"<pre><code>name: my-agent                    # Required: Agent name\ndescription: Agent description    # Optional: What this agent does\nauthor: \"Your Name\"               # Optional: Who created this agent\n\nmodel:                            # Required: LLM configuration\n  provider: openai                # Required: openai|azure_openai|anthropic\n  name: gpt-4o                    # Required: Model identifier\n  temperature: 0.7                # Optional: 0.0-2.0\n  max_tokens: 2000                # Optional: Maximum generation tokens\n\ninstructions:                     # Required: System prompt\n  inline: \"You are a helpful...\"  # Option 1: Inline text\n  # OR\n  # file: instructions.txt        # Option 2: External file\n\ntools: []                         # Optional: Agent capabilities\nevaluations:                      # Optional: Quality metrics\n  metrics: []\ntest_cases: []                    # Optional: Test scenarios\n</code></pre>"},{"location":"guides/agent-configuration/#agent-name","title":"Agent Name","text":"<ul> <li>Required: Yes</li> <li>Type: String</li> <li>Format: 1-100 characters, alphanumeric + hyphens, must start with letter</li> <li>Examples: <code>customer-support</code>, <code>code-reviewer</code>, <code>data-analyzer</code></li> </ul> <pre><code>name: customer-support\n</code></pre>"},{"location":"guides/agent-configuration/#agent-description","title":"Agent Description","text":"<ul> <li>Required: No</li> <li>Type: String</li> <li>Constraints: Max 500 characters</li> <li>Purpose: Describe what this agent does for documentation</li> </ul> <pre><code>description: Handles customer support queries with ticket creation\n</code></pre>"},{"location":"guides/agent-configuration/#agent-author","title":"Agent Author","text":"<ul> <li>Required: No</li> <li>Type: String</li> <li>Constraints: Max 256 characters</li> <li>Purpose: Document who created or maintains this agent</li> </ul> <pre><code>author: \"Alice Johnson\"\n</code></pre> <p>This field is useful for: - Attribution and credit in multi-team environments - Understanding who to contact for questions about the agent - Tracking agent ownership and maintenance responsibility</p>"},{"location":"guides/agent-configuration/#model-configuration","title":"Model Configuration","text":"<p>Defines which LLM provider and model to use.</p>"},{"location":"guides/agent-configuration/#provider-field","title":"Provider Field","text":"<ul> <li>Required: Yes</li> <li>Type: String (Enum)</li> <li>Options:</li> <li><code>openai</code> - OpenAI API (GPT-4o, GPT-4o-mini, etc.)</li> <li><code>azure_openai</code> - Azure OpenAI Service</li> <li><code>anthropic</code> - Anthropic Claude</li> </ul> <pre><code>model:\n  provider: openai\n</code></pre>"},{"location":"guides/agent-configuration/#model-name","title":"Model Name","text":"<ul> <li>Required: Yes</li> <li>Type: String</li> <li>Purpose: Identifies specific model within provider</li> <li>Examples by Provider:</li> <li>OpenAI: <code>gpt-4o</code>, <code>gpt-4o-mini</code>, <code>gpt-4-turbo</code></li> <li>Azure: <code>gpt-4</code>, <code>gpt-4-32k</code></li> <li>Anthropic: <code>claude-3-opus</code>, <code>claude-3-sonnet</code>, <code>claude-3-haiku</code></li> </ul> <pre><code>model:\n  name: gpt-4o\n</code></pre>"},{"location":"guides/agent-configuration/#temperature","title":"Temperature","text":"<ul> <li>Optional: Yes</li> <li>Type: Float</li> <li>Range: 0.0 to 2.0</li> <li>Default: 0.7 (if not specified)</li> <li>Meaning:</li> <li><code>0.0</code> - Deterministic, focused responses</li> <li><code>0.7</code> - Balanced randomness</li> <li><code>1.5+</code> - Very creative, random responses</li> </ul> <pre><code>model:\n  temperature: 0.8  # More creative\n</code></pre>"},{"location":"guides/agent-configuration/#max-tokens","title":"Max Tokens","text":"<ul> <li>Optional: Yes</li> <li>Type: Integer</li> <li>Constraint: Must be positive</li> <li>Purpose: Limit maximum length of generated responses</li> </ul> <pre><code>model:\n  max_tokens: 4000\n</code></pre>"},{"location":"guides/agent-configuration/#top-p","title":"Top P","text":"<ul> <li>Optional: Yes</li> <li>Type: Float</li> <li>Range: 0.0 to 1.0</li> <li>Purpose: Nucleus sampling (alternative to temperature)</li> </ul> <pre><code>model:\n  top_p: 0.9\n</code></pre>"},{"location":"guides/agent-configuration/#instructions","title":"Instructions","text":"<p>Defines the system prompt that guides agent behavior.</p>"},{"location":"guides/agent-configuration/#inline-instructions","title":"Inline Instructions","text":"<p>Embed the prompt directly in <code>agent.yaml</code>:</p> <pre><code>instructions:\n  inline: |\n    You are a customer support specialist.\n\n    Guidelines:\n    - Be polite and professional\n    - Provide accurate information\n    - Escalate complex issues to supervisors\n</code></pre>"},{"location":"guides/agent-configuration/#file-based-instructions","title":"File-Based Instructions","text":"<p>Reference an external file (path relative to <code>agent.yaml</code>):</p> <pre><code>instructions:\n  file: system_prompt.txt\n</code></pre> <p>File at <code>system_prompt.txt</code>:</p> <pre><code>You are a customer support specialist.\n\nGuidelines:\n- Be polite and professional\n- Provide accurate information\n- Escalate complex issues to supervisors\n</code></pre>"},{"location":"guides/agent-configuration/#rules","title":"Rules","text":"<ul> <li>Exactly one required: Either <code>inline</code> OR <code>file</code>, not both</li> <li>Max length (inline): 5000 characters</li> <li>File path: Relative to <code>agent.yaml</code> directory (see File References guide)</li> </ul>"},{"location":"guides/agent-configuration/#response-format","title":"Response Format","text":"<p>Defines the expected structure of the agent's responses (agent-level only).</p> <p>The <code>response_format</code> field constrains the LLM to generate structured output following a JSON Schema. This is useful for:</p> <ul> <li>Ensuring consistent response structure</li> <li>Integrating with downstream systems that expect specific formats</li> <li>Validating response quality programmatically</li> <li>Guiding the LLM toward well-formatted outputs</li> </ul>"},{"location":"guides/agent-configuration/#inline-response-format","title":"Inline Response Format","text":"<p>Define the schema directly in <code>agent.yaml</code>:</p> <pre><code>response_format:\n  type: object\n  properties:\n    answer:\n      type: string\n      description: The answer to the user's question\n    confidence:\n      type: number\n      description: Confidence score 0.0-1.0\n    sources:\n      type: array\n      items:\n        type: string\n      description: References used in the answer\n  required:\n    - answer\n    - confidence\n  additionalProperties: false\n</code></pre>"},{"location":"guides/agent-configuration/#file-based-response-format","title":"File-Based Response Format","text":"<p>Reference an external JSON Schema file (path relative to <code>agent.yaml</code>):</p> <pre><code>response_format: schemas/response.json\n</code></pre> <p>File at <code>schemas/response.json</code>:</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"answer\": {\n      \"type\": \"string\",\n      \"description\": \"The answer to the user's question\"\n    },\n    \"confidence\": {\n      \"type\": \"number\",\n      \"description\": \"Confidence score 0.0-1.0\"\n    },\n    \"sources\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"References used in the answer\"\n    }\n  },\n  \"required\": [\"answer\", \"confidence\"],\n  \"additionalProperties\": false\n}\n</code></pre>"},{"location":"guides/agent-configuration/#supported-json-schema-keywords","title":"Supported JSON Schema Keywords","text":"<p>Response format uses a Basic JSON Schema subset supporting:</p> <ul> <li><code>type</code> - Data type (string, number, integer, boolean, object, array, null)</li> <li><code>properties</code> - Object properties and their schemas</li> <li><code>required</code> - Array of required property names</li> <li><code>items</code> - Schema for array items</li> <li><code>enum</code> - List of allowed values</li> <li><code>description</code> - Documentation string</li> <li><code>minimum</code> - Minimum numeric value</li> <li><code>maximum</code> - Maximum numeric value</li> <li><code>additionalProperties</code> - Whether to allow extra properties (true|false)</li> </ul>"},{"location":"guides/agent-configuration/#unsupported-keywords","title":"Unsupported Keywords","text":"<p>The following keywords are not supported and will cause validation errors:</p> <ul> <li><code>$ref</code> - Schema references</li> <li><code>anyOf</code> - Multiple schema options</li> <li><code>oneOf</code> - Exactly one schema match</li> <li><code>allOf</code> - All schemas must match</li> <li><code>patternProperties</code> - Regex-based properties</li> <li><code>minLength</code>, <code>maxLength</code> - String length constraints</li> <li><code>minItems</code>, <code>maxItems</code> - Array length constraints</li> <li>And other JSON Schema draft keywords</li> </ul>"},{"location":"guides/agent-configuration/#rules_1","title":"Rules","text":"<ul> <li>Optional: Response format is not required</li> <li>Agent-level only: Not inherited from global configuration</li> <li>Validated at load time: Invalid schemas cause errors with clear messages</li> <li>File path: Relative to <code>agent.yaml</code> directory</li> </ul>"},{"location":"guides/agent-configuration/#validation","title":"Validation","text":"<p>Invalid response formats will produce clear error messages:</p> <pre><code>Error: Invalid JSON in response_format field\nFile: agent.yaml\nLine: 42\nDetails: Missing required property: 'answer'\n</code></pre> <pre><code>Error: Unknown JSON Schema keyword: '$ref'\nFile: schemas/response.json\nDetails: Keyword '$ref' is not supported. Use basic JSON Schema keywords only.\n</code></pre>"},{"location":"guides/agent-configuration/#tools","title":"Tools","text":"<p>Define capabilities the agent can use. See the Tools Reference Guide for detailed documentation.</p> <pre><code>tools:\n  - name: search-docs\n    description: Search company documentation\n    type: vectorstore\n    source: docs/\n\n  - name: get-user\n    description: Retrieve user information\n    type: function\n    file: tools/user_tools.py\n    function: get_user\n\n  - name: file-system\n    description: File system access\n    type: mcp\n    command: npx\n    args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/data\"]\n    config:\n      allowed_directories: [\"/data\", \"/tmp\"]\n\n  - name: summarize\n    description: Summarize text\n    type: prompt\n    template: \"Summarize this in 2-3 sentences: {{text}}\"\n    parameters:\n      text:\n        type: string\n        description: Text to summarize\n</code></pre>"},{"location":"guides/agent-configuration/#tool-constraints","title":"Tool Constraints","text":"<ul> <li>Max tools: 50 per agent</li> <li>Tool names: Must be unique, alphanumeric + underscores</li> <li>Required fields: <code>name</code>, <code>description</code>, <code>type</code></li> </ul>"},{"location":"guides/agent-configuration/#evaluations","title":"Evaluations","text":"<p>Defines metrics to measure agent quality. See the Evaluations Guide for details.</p> <pre><code>evaluations:\n  model:  # Optional: default model for all metrics\n    provider: openai\n    name: gpt-4o\n\n  metrics:\n    - metric: groundedness\n      threshold: 0.8\n      enabled: true\n\n    - metric: relevance\n      threshold: 0.75\n</code></pre>"},{"location":"guides/agent-configuration/#test-cases","title":"Test Cases","text":"<p>Defines scenarios to validate agent behavior.</p> <pre><code>test_cases:\n  - name: \"Support request\"\n    input: \"How do I reset my password?\"\n    expected_tools: [search-docs]\n    ground_truth: \"Instructions for password reset\"\n\n  - input: \"What are your hours?\"\n    expected_tools: []\n</code></pre>"},{"location":"guides/agent-configuration/#test-case-fields","title":"Test Case Fields","text":"<ul> <li>name: Test identifier (optional)</li> <li>input: User query (required, max 5000 chars)</li> <li>expected_tools: Tools that should be called (optional)</li> <li>ground_truth: Expected response for comparison (optional)</li> <li>files: Multimodal inputs like images, PDFs (optional, max 10 per test)</li> </ul>"},{"location":"guides/agent-configuration/#constraints","title":"Constraints","text":"<ul> <li>Max test cases: 100 per agent</li> <li>Test names: Must be unique if provided</li> </ul>"},{"location":"guides/agent-configuration/#complete-example","title":"Complete Example","text":"<pre><code>name: support-agent\ndescription: Handles customer support queries with knowledge base search\n\nmodel:\n  provider: azure_openai\n  name: gpt-4o\n  temperature: 0.7\n  max_tokens: 2000\n\ninstructions:\n  file: system_prompt.txt\n\nresponse_format:\n  type: object\n  properties:\n    answer:\n      type: string\n      description: The support response\n    confidence:\n      type: number\n      description: How confident we are in this answer\n    escalation_needed:\n      type: boolean\n      description: Whether the issue should be escalated\n  required:\n    - answer\n    - confidence\n\ntools:\n  - name: search-kb\n    description: Search knowledge base\n    type: vectorstore\n    source: knowledge_base.json\n    chunk_size: 500\n\n  - name: create-ticket\n    description: Create support ticket\n    type: function\n    file: tools/support.py\n    function: create_ticket\n    parameters:\n      title:\n        type: string\n        description: Ticket title\n      priority:\n        type: string\n        description: low|medium|high\n\nevaluations:\n  model:\n    provider: azure_openai\n    name: gpt-4o\n    temperature: 0.0\n\n  metrics:\n    - metric: f1_score\n      threshold: 0.8\n    - metric: bleu\n      threshold: 0.75\n\ntest_cases:\n  - name: \"Password reset\"\n    input: \"How do I reset my password?\"\n    expected_tools: [search-kb]\n    ground_truth: \"Step-by-step password reset instructions\"\n    evaluations:\n      - f1_score\n      - bleu\n\n  - name: \"Open ticket\"\n    input: \"I need help with my account\"\n    expected_tools: [search-kb, create-ticket]\n    ground_truth: \"Ticket created and knowledge base searched\"\n    evaluations:\n      - f1_score\n</code></pre>"},{"location":"guides/agent-configuration/#validation-rules","title":"Validation Rules","text":""},{"location":"guides/agent-configuration/#required-fields","title":"Required Fields","text":"<ul> <li><code>name</code>: Must be provided</li> <li><code>model.provider</code>: Must be provided and valid</li> <li><code>model.name</code>: Must be provided</li> <li><code>instructions</code>: Must have either <code>inline</code> or <code>file</code></li> </ul>"},{"location":"guides/agent-configuration/#mutual-exclusivity","title":"Mutual Exclusivity","text":"<ul> <li>Instructions: Either <code>inline</code> OR <code>file</code>, not both</li> <li>Prompt tools: Either <code>template</code> OR <code>file</code>, not both</li> </ul>"},{"location":"guides/agent-configuration/#ranges","title":"Ranges","text":"<ul> <li>Temperature: 0.0 to 2.0</li> <li>Max tokens: Must be &gt; 0</li> <li>Tool limit: Max 50 per agent</li> <li>Test cases: Max 100 per agent</li> </ul>"},{"location":"guides/agent-configuration/#response-format-validation","title":"Response Format Validation","text":"<ul> <li><code>response_format</code>: Optional</li> <li>Must be valid JSON Schema (if inline) or valid JSON file (if external)</li> <li>Only Basic JSON Schema keywords supported</li> <li>Invalid schemas produce clear error messages with file location and details</li> <li>Not inherited from global configuration (agent-level only)</li> </ul>"},{"location":"guides/agent-configuration/#file-references","title":"File References","text":"<ul> <li>Paths are relative to <code>agent.yaml</code> directory</li> <li>Files must exist (checked during loading)</li> <li>Absolute paths are supported</li> </ul>"},{"location":"guides/agent-configuration/#environment-variables","title":"Environment Variables","text":"<p>Replace sensitive values with environment variables:</p> <pre><code>model:\n  provider: openai\n  name: gpt-4o\n  # API key from environment, see global config guide\n</code></pre> <p>See the Global Configuration Guide for environment variable interpolation details.</p>"},{"location":"guides/agent-configuration/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/agent-configuration/#minimal-agent-inline-instructions","title":"Minimal Agent (Inline Instructions)","text":"<pre><code>name: simple-agent\n\nmodel:\n  provider: openai\n  name: gpt-4o\n\ninstructions:\n  inline: \"You are a helpful assistant.\"\n</code></pre>"},{"location":"guides/agent-configuration/#agent-with-structured-output-response-format","title":"Agent with Structured Output (Response Format)","text":"<pre><code>name: structured-agent\ndescription: Returns structured responses\n\nmodel:\n  provider: openai\n  name: gpt-4o\n\ninstructions:\n  inline: \"Answer questions in the specified format.\"\n\nresponse_format:\n  type: object\n  properties:\n    answer:\n      type: string\n    confidence:\n      type: number\n      minimum: 0\n      maximum: 1\n  required:\n    - answer\n    - confidence\n</code></pre>"},{"location":"guides/agent-configuration/#agent-with-file-references","title":"Agent with File References","text":"<pre><code>name: documented-agent\n\nmodel:\n  provider: azure_openai\n  name: gpt-4\n\ninstructions:\n  file: prompts/system.txt\n\nresponse_format: schemas/output.json\n\ntools:\n  - name: search\n    type: vectorstore\n    source: data/docs.json\n</code></pre>"},{"location":"guides/agent-configuration/#full-featured-agent","title":"Full-Featured Agent","text":"<pre><code>name: enterprise-agent\ndescription: Production-ready support agent\n\nmodel:\n  provider: azure_openai\n  name: gpt-4o\n  temperature: 0.6\n  max_tokens: 4096\n\ninstructions:\n  file: system_prompt.txt\n\nresponse_format:\n  type: object\n  properties:\n    response:\n      type: string\n    sources:\n      type: array\n      items:\n        type: string\n    confidence:\n      type: number\n      minimum: 0\n      maximum: 1\n  required:\n    - response\n\ntools:\n  - name: knowledge-base\n    type: vectorstore\n    source: kb/\n  - name: system-check\n    type: function\n    file: tools/system.py\n    function: check_status\n  - name: external-api\n    type: mcp\n    command: npx\n    args: [\"-y\", \"custom-mcp-server\"]\n\nevaluations:\n  model:\n    provider: azure_openai\n    name: gpt-4o\n    temperature: 0.0\n\n  metrics:\n    - metric: f1_score\n      threshold: 0.85\n    - metric: bleu\n      threshold: 0.8\n\ntest_cases:\n  - name: \"Basic query\"\n    input: \"Hello, can you help?\"\n    expected_tools: [knowledge-base]\n    ground_truth: \"Yes, I'm here to help with your support request\"\n    evaluations:\n      - f1_score\n      - bleu\n</code></pre>"},{"location":"guides/agent-configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/agent-configuration/#error-name-is-required","title":"Error: \"name is required\"","text":"<ul> <li>Add <code>name</code> field at top level</li> </ul>"},{"location":"guides/agent-configuration/#error-instructions-either-file-or-inline-must-be-provided","title":"Error: \"instructions: Either file or inline must be provided\"","text":"<ul> <li>Ensure instructions section has either <code>inline</code> or <code>file</code> field</li> </ul>"},{"location":"guides/agent-configuration/#error-instruction-file-not-found","title":"Error: \"instruction file not found\"","text":"<ul> <li>Check file path is correct and relative to <code>agent.yaml</code> directory</li> <li>Use absolute paths if needed</li> </ul>"},{"location":"guides/agent-configuration/#error-invalid-model-provider","title":"Error: \"Invalid model provider\"","text":"<ul> <li>Use valid provider: <code>openai</code>, <code>azure_openai</code>, or <code>anthropic</code></li> </ul>"},{"location":"guides/agent-configuration/#error-tool-name-must-be-unique","title":"Error: \"Tool name must be unique\"","text":"<ul> <li>Each tool must have a unique <code>name</code> field</li> </ul>"},{"location":"guides/agent-configuration/#error-invalid-json-in-response_format-field","title":"Error: \"Invalid JSON in response_format field\"","text":"<ul> <li>Ensure response_format is valid JSON (if inline) or references a valid JSON file</li> <li>Check for missing quotes, commas, or brackets in JSON</li> </ul>"},{"location":"guides/agent-configuration/#error-response_format-file-not-found","title":"Error: \"response_format file not found\"","text":"<ul> <li>Check the file path is correct and relative to <code>agent.yaml</code> directory</li> <li>File must exist and be readable</li> </ul>"},{"location":"guides/agent-configuration/#error-unknown-json-schema-keyword-ref","title":"Error: \"Unknown JSON Schema keyword: '$ref'\"","text":"<ul> <li>Remove unsupported JSON Schema keywords ($ref, anyOf, oneOf, allOf, patternProperties, etc.)</li> <li>Use only Basic JSON Schema keywords: type, properties, required, items, enum, description, minimum, maximum, additionalProperties</li> </ul>"},{"location":"guides/agent-configuration/#next-steps","title":"Next Steps","text":"<ul> <li>See Tools Reference for tool configuration details</li> <li>See Evaluations Guide for quality metrics</li> <li>See Global Configuration for shared settings</li> <li>See File References for path resolution</li> </ul>"},{"location":"guides/evaluations/","title":"Evaluations Guide","text":"<p>This guide explains HoloDeck's evaluation system for measuring agent quality.</p>"},{"location":"guides/evaluations/#overview","title":"Overview","text":"<p>Evaluations measure how well your agent performs. You define metrics in <code>agent.yaml</code> to automatically grade agent responses against test cases.</p> <p>HoloDeck supports three categories of metrics (in order of recommendation):</p> <ol> <li>DeepEval Metrics (Recommended) - LLM-as-a-judge with custom criteria (GEval) and RAG-specific metrics</li> <li>NLP Metrics (Standard) - Text comparison algorithms (F1, BLEU, ROUGE, METEOR)</li> <li>Legacy AI Metrics (Deprecated) - Azure AI-based metrics (groundedness, relevance, coherence, safety)</li> </ol>"},{"location":"guides/evaluations/#basic-structure","title":"Basic Structure","text":"<pre><code>evaluations:\n  model:      # Optional: Default LLM for evaluation\n    provider: ollama\n    name: llama3.2:latest\n    temperature: 0.0\n\n  metrics:    # Required: Metrics to compute\n    # DeepEval GEval metric (recommended)\n    - type: geval\n      name: \"Response Quality\"\n      criteria: \"Evaluate if the response is helpful and accurate\"\n      threshold: 0.7\n\n    # DeepEval RAG metric\n    - type: rag\n      metric_type: answer_relevancy\n      threshold: 0.7\n</code></pre>"},{"location":"guides/evaluations/#configuration-levels","title":"Configuration Levels","text":"<p>Model configuration for evaluations works at three levels (priority order):</p>"},{"location":"guides/evaluations/#level-1-per-metric-override-highest-priority","title":"Level 1: Per-Metric Override (Highest Priority)","text":"<p>Override model for a specific metric:</p> <pre><code>evaluations:\n  metrics:\n    - type: geval\n      name: \"Critical Metric\"\n      criteria: \"...\"\n      model:                    # Uses this model for this metric only\n        provider: openai\n        name: gpt-4\n</code></pre>"},{"location":"guides/evaluations/#level-2-evaluation-wide-model","title":"Level 2: Evaluation-Wide Model","text":"<p>Default for all metrics without override:</p> <pre><code>evaluations:\n  model:                        # Uses for all metrics\n    provider: ollama\n    name: llama3.2:latest\n\n  metrics:\n    - type: geval\n      name: \"Coherence\"\n      criteria: \"...\"\n      # Uses evaluation.model above\n    - type: rag\n      metric_type: faithfulness\n      # Also uses evaluation.model above\n</code></pre>"},{"location":"guides/evaluations/#level-3-agent-model-lowest-priority","title":"Level 3: Agent Model (Lowest Priority)","text":"<p>Used if neither Level 1 nor Level 2 specified:</p> <pre><code>model:                          # Agent's main model\n  provider: openai\n  name: gpt-4o\n\nevaluations:\n  metrics:\n    - type: geval\n      name: \"Quality\"\n      criteria: \"...\"\n      # Falls back to agent.model above\n</code></pre>"},{"location":"guides/evaluations/#deepeval-metrics-recommended","title":"DeepEval Metrics (Recommended)","text":"<p>DeepEval provides powerful LLM-as-a-judge evaluation with two metric types:</p> <ul> <li>GEval: Custom criteria evaluation using chain-of-thought prompting</li> <li>RAG Metrics: Specialized metrics for retrieval-augmented generation pipelines</li> </ul>"},{"location":"guides/evaluations/#why-deepeval","title":"Why DeepEval?","text":"<ul> <li>Flexible: Define custom evaluation criteria in natural language</li> <li>Local Models: Works with Ollama for free, local evaluation</li> <li>RAG-Focused: Purpose-built metrics for RAG pipeline evaluation</li> <li>Chain-of-Thought: Uses G-Eval algorithm for more accurate scoring</li> </ul>"},{"location":"guides/evaluations/#supported-providers","title":"Supported Providers","text":"<pre><code>model:\n  provider: ollama        # Free, local inference (recommended for development)\n  # provider: openai      # OpenAI API\n  # provider: anthropic   # Anthropic API\n  # provider: azure_openai # Azure OpenAI\n  name: llama3.2:latest\n  temperature: 0.0        # Use 0 for deterministic evaluation\n</code></pre>"},{"location":"guides/evaluations/#geval-metrics","title":"GEval Metrics","text":"<p>GEval uses the G-Eval algorithm with chain-of-thought prompting to evaluate responses against custom criteria.</p>"},{"location":"guides/evaluations/#basic-configuration","title":"Basic Configuration","text":"<pre><code>- type: geval\n  name: \"Coherence\"\n  criteria: \"Evaluate whether the response is clear, well-structured, and easy to understand.\"\n  threshold: 0.7\n</code></pre>"},{"location":"guides/evaluations/#full-configuration","title":"Full Configuration","text":"<pre><code>- type: geval\n  name: \"Technical Accuracy\"\n  criteria: |\n    Evaluate whether the response provides accurate technical information\n    that correctly addresses the user's question.\n  evaluation_steps:              # Optional: Auto-generated if omitted\n    - \"Check if the response directly addresses the user's question\"\n    - \"Verify technical accuracy of any code or commands provided\"\n    - \"Ensure explanations are correct and not misleading\"\n  evaluation_params:             # Which test case fields to use\n    - actual_output              # Required: Agent's response\n    - input                      # Optional: User's query\n    - expected_output            # Optional: Ground truth\n    - context                    # Optional: Additional context\n    - retrieval_context          # Optional: Retrieved documents\n  threshold: 0.8\n  strict_mode: false             # Binary scoring (1.0 or 0.0) when true\n  enabled: true\n  fail_on_error: false\n  model:                         # Optional: Per-metric model override\n    provider: openai\n    name: gpt-4\n</code></pre>"},{"location":"guides/evaluations/#geval-configuration-options","title":"GEval Configuration Options","text":"Field Type Required Description <code>type</code> string Yes Must be <code>\"geval\"</code> <code>name</code> string Yes Custom metric name (e.g., \"Coherence\", \"Helpfulness\") <code>criteria</code> string Yes Natural language evaluation criteria <code>evaluation_steps</code> list No Step-by-step evaluation instructions (auto-generated if omitted) <code>evaluation_params</code> list No Test case fields to use (default: <code>[\"actual_output\"]</code>) <code>threshold</code> float No Minimum passing score (0-1) <code>strict_mode</code> bool No Binary scoring when true (default: false) <code>enabled</code> bool No Enable/disable metric (default: true) <code>fail_on_error</code> bool No Fail test on evaluation error (default: false) <code>model</code> object No Per-metric model override"},{"location":"guides/evaluations/#evaluation-parameters","title":"Evaluation Parameters","text":"Parameter Description When to Use <code>actual_output</code> Agent's response Always (required for evaluation) <code>input</code> User's query/question When relevance to query matters <code>expected_output</code> Ground truth answer When comparing to expected response <code>context</code> Additional context provided When evaluating context usage <code>retrieval_context</code> Retrieved documents For RAG pipeline evaluation"},{"location":"guides/evaluations/#geval-examples","title":"GEval Examples","text":"<p>Coherence Check: <pre><code>- type: geval\n  name: \"Coherence\"\n  criteria: \"Evaluate whether the response is clear, well-structured, and easy to understand.\"\n  evaluation_steps:\n    - \"Evaluate whether the response uses clear and direct language.\"\n    - \"Check if the explanation avoids jargon or explains it when used.\"\n    - \"Assess whether complex ideas are presented in a way that's easy to follow.\"\n  evaluation_params:\n    - actual_output\n  threshold: 0.7\n</code></pre></p> <p>Helpfulness Check: <pre><code>- type: geval\n  name: \"Helpfulness\"\n  criteria: |\n    Evaluate whether the response provides actionable, practical help\n    that addresses the user's needs.\n  evaluation_params:\n    - actual_output\n    - input\n  threshold: 0.75\n</code></pre></p> <p>Factual Accuracy: <pre><code>- type: geval\n  name: \"Factual Accuracy\"\n  criteria: |\n    Evaluate whether the response is factually accurate when compared\n    to the expected answer and provided context.\n  evaluation_params:\n    - actual_output\n    - expected_output\n    - context\n  threshold: 0.85\n  strict_mode: true  # Binary pass/fail\n</code></pre></p>"},{"location":"guides/evaluations/#rag-metrics","title":"RAG Metrics","text":"<p>RAG (Retrieval-Augmented Generation) metrics evaluate the quality of responses generated using retrieved context.</p>"},{"location":"guides/evaluations/#available-rag-metrics","title":"Available RAG Metrics","text":"Metric Type Purpose Required Parameters <code>faithfulness</code> Detects hallucinations input, actual_output, retrieval_context <code>answer_relevancy</code> Response relevance to query input, actual_output <code>contextual_relevancy</code> Retrieved chunks relevance input, actual_output, retrieval_context <code>contextual_precision</code> Chunk ranking quality input, actual_output, expected_output, retrieval_context <code>contextual_recall</code> Retrieval completeness input, actual_output, expected_output, retrieval_context"},{"location":"guides/evaluations/#basic-configuration_1","title":"Basic Configuration","text":"<pre><code>- type: rag\n  metric_type: faithfulness\n  threshold: 0.8\n\n- type: rag\n  metric_type: answer_relevancy\n  threshold: 0.7\n</code></pre>"},{"location":"guides/evaluations/#full-configuration_1","title":"Full Configuration","text":"<pre><code>- type: rag\n  metric_type: faithfulness\n  threshold: 0.8\n  include_reason: true           # Include reasoning in results\n  enabled: true\n  fail_on_error: false\n  model:                         # Optional: Per-metric model override\n    provider: openai\n    name: gpt-4\n</code></pre>"},{"location":"guides/evaluations/#rag-metric-details","title":"RAG Metric Details","text":"<p>Faithfulness - Detects hallucinations by comparing response to retrieval context: <pre><code>- type: rag\n  metric_type: faithfulness\n  threshold: 0.8\n  include_reason: true\n</code></pre> - What it measures: Whether claims in the response are supported by retrieved documents - When to use: Critical for factual accuracy in RAG pipelines - Example: Agent says \"The product costs $99\" - faithfulness checks if this is in the retrieved context</p> <p>Answer Relevancy - Measures response relevance to the query: <pre><code>- type: rag\n  metric_type: answer_relevancy\n  threshold: 0.7\n</code></pre> - What it measures: How well the response addresses the user's question - When to use: General quality assurance for any agent - Example: User asks \"How do I reset my password?\" - checks if response actually explains password reset</p> <p>Contextual Relevancy - Measures relevance of retrieved chunks: <pre><code>- type: rag\n  metric_type: contextual_relevancy\n  threshold: 0.75\n</code></pre> - What it measures: Whether retrieved documents are relevant to the query - When to use: Diagnosing retrieval quality issues</p> <p>Contextual Precision - Evaluates chunk ranking quality: <pre><code>- type: rag\n  metric_type: contextual_precision\n  threshold: 0.8\n</code></pre> - What it measures: Whether the most relevant chunks are ranked highest - When to use: Optimizing retrieval ranking algorithms</p> <p>Contextual Recall - Measures retrieval completeness: <pre><code>- type: rag\n  metric_type: contextual_recall\n  threshold: 0.7\n</code></pre> - What it measures: Whether all information needed for the expected answer was retrieved - When to use: Ensuring comprehensive retrieval coverage</p>"},{"location":"guides/evaluations/#nlp-metrics-standard","title":"NLP Metrics (Standard)","text":"<p>NLP metrics compare response text to expected output using algorithms. They're fast, free (no LLM calls), and deterministic.</p>"},{"location":"guides/evaluations/#f1-score","title":"F1 Score","text":"<p>Measures precision and recall of token overlap.</p> <pre><code>- type: standard\n  metric: f1_score\n  threshold: 0.8\n</code></pre> <p>Scale: 0.0-1.0 (higher is better)</p> <p>What it measures: - Token-level match with ground truth - Balanced precision/recall</p> <p>When to use: When exact word matching is important</p>"},{"location":"guides/evaluations/#bleu-bilingual-evaluation-understudy","title":"BLEU (Bilingual Evaluation Understudy)","text":"<p>Measures n-gram overlap with reference translation.</p> <pre><code>- type: standard\n  metric: bleu\n  threshold: 0.6\n</code></pre> <p>Scale: 0.0-1.0 (higher is better)</p> <p>What it measures: - N-gram similarity to reference - Penalizes brevity</p> <p>When to use: For translation, paraphrase evaluation</p>"},{"location":"guides/evaluations/#rouge-recall-oriented-understudy-for-gisting-evaluation","title":"ROUGE (Recall-Oriented Understudy for Gisting Evaluation)","text":"<p>Measures recall of n-grams with reference.</p> <pre><code>- type: standard\n  metric: rouge\n  threshold: 0.7\n</code></pre> <p>Scale: 0.0-1.0 (higher is better)</p> <p>What it measures: - Recall of n-grams - Coverage of reference content</p> <p>When to use: For summarization tasks</p>"},{"location":"guides/evaluations/#meteor-metric-for-evaluation-of-translation-with-explicit-ordering","title":"METEOR (Metric for Evaluation of Translation with Explicit Ordering)","text":"<p>Similar to BLEU but with better handling of synonyms.</p> <pre><code>- type: standard\n  metric: meteor\n  threshold: 0.65\n</code></pre> <p>Scale: 0.0-1.0 (higher is better)</p> <p>What it measures: - N-gram match with synonyms - Word order</p> <p>When to use: For translation, paraphrase with synonyms</p>"},{"location":"guides/evaluations/#legacy-ai-metrics-deprecated","title":"Legacy AI Metrics (Deprecated)","text":"<p>DEPRECATED: Azure AI-based metrics (groundedness, relevance, coherence, safety) are deprecated and will be removed in a future version.</p> <p>Migration Guide: - For custom criteria evaluation \u2192 Use <code>type: geval</code> with natural language criteria - For RAG hallucination detection \u2192 Use <code>type: rag</code> with <code>metric_type: faithfulness</code> - For relevance checking \u2192 Use <code>type: rag</code> with <code>metric_type: answer_relevancy</code></p> <p>These legacy metrics are still supported for backwards compatibility:</p>"},{"location":"guides/evaluations/#groundedness-deprecated","title":"Groundedness (Deprecated)","text":"<pre><code># DEPRECATED - Use type: rag with metric_type: faithfulness instead\n- type: standard\n  metric: groundedness\n  threshold: 0.8\n</code></pre> <p>Migration: Replace with: <pre><code>- type: rag\n  metric_type: faithfulness\n  threshold: 0.8\n</code></pre></p>"},{"location":"guides/evaluations/#relevance-deprecated","title":"Relevance (Deprecated)","text":"<pre><code># DEPRECATED - Use type: rag with metric_type: answer_relevancy instead\n- type: standard\n  metric: relevance\n  threshold: 0.75\n</code></pre> <p>Migration: Replace with: <pre><code>- type: rag\n  metric_type: answer_relevancy\n  threshold: 0.75\n</code></pre></p>"},{"location":"guides/evaluations/#coherence-deprecated","title":"Coherence (Deprecated)","text":"<pre><code># DEPRECATED - Use type: geval with custom criteria instead\n- type: standard\n  metric: coherence\n  threshold: 0.7\n</code></pre> <p>Migration: Replace with: <pre><code>- type: geval\n  name: \"Coherence\"\n  criteria: \"Evaluate whether the response is clear, well-structured, and easy to understand.\"\n  threshold: 0.7\n</code></pre></p>"},{"location":"guides/evaluations/#safety-deprecated","title":"Safety (Deprecated)","text":"<pre><code># DEPRECATED - Use type: geval with custom criteria instead\n- type: standard\n  metric: safety\n  threshold: 0.9\n</code></pre> <p>Migration: Replace with: <pre><code>- type: geval\n  name: \"Safety\"\n  criteria: |\n    Evaluate whether the response is safe and appropriate:\n    - No harmful or dangerous content\n    - No PII leakage\n    - Appropriate tone and language\n  threshold: 0.9\n</code></pre></p>"},{"location":"guides/evaluations/#metric-configuration-options","title":"Metric Configuration Options","text":""},{"location":"guides/evaluations/#threshold","title":"Threshold","text":"<ul> <li>Type: Float</li> <li>Purpose: Minimum score for test to pass</li> <li>Scale: 0-1 for all metrics</li> <li>Optional: Yes (default: no threshold, metric is informational)</li> </ul> <pre><code>- type: geval\n  name: \"Quality\"\n  criteria: \"...\"\n  threshold: 0.8\n</code></pre>"},{"location":"guides/evaluations/#enabled","title":"Enabled","text":"<ul> <li>Type: Boolean</li> <li>Default: <code>true</code></li> <li>Purpose: Temporarily disable metric without removing it</li> </ul> <pre><code>- type: rag\n  metric_type: answer_relevancy\n  enabled: false  # Metric runs but doesn't fail test\n</code></pre>"},{"location":"guides/evaluations/#fail-on-error","title":"Fail on Error","text":"<ul> <li>Type: Boolean</li> <li>Default: <code>false</code> (soft failure)</li> <li>Purpose: Whether to fail test if evaluation errors</li> </ul> <pre><code>- type: geval\n  name: \"Quality\"\n  criteria: \"...\"\n  fail_on_error: false  # Continues even if LLM evaluation fails\n</code></pre>"},{"location":"guides/evaluations/#complete-examples","title":"Complete Examples","text":""},{"location":"guides/evaluations/#basic-deepeval-setup","title":"Basic DeepEval Setup","text":"<pre><code>evaluations:\n  model:\n    provider: ollama\n    name: llama3.2:latest\n    temperature: 0.0\n\n  metrics:\n    - type: geval\n      name: \"Coherence\"\n      criteria: \"Evaluate whether the response is clear and well-structured.\"\n      threshold: 0.7\n\n    - type: rag\n      metric_type: answer_relevancy\n      threshold: 0.7\n</code></pre>"},{"location":"guides/evaluations/#rag-pipeline-evaluation","title":"RAG Pipeline Evaluation","text":"<pre><code>evaluations:\n  model:\n    provider: ollama\n    name: llama3.2:latest\n    temperature: 0.0\n\n  metrics:\n    # Detect hallucinations\n    - type: rag\n      metric_type: faithfulness\n      threshold: 0.85\n      include_reason: true\n\n    # Check response relevance\n    - type: rag\n      metric_type: answer_relevancy\n      threshold: 0.75\n\n    # Evaluate retrieval quality\n    - type: rag\n      metric_type: contextual_relevancy\n      threshold: 0.7\n\n    # Check retrieval completeness\n    - type: rag\n      metric_type: contextual_recall\n      threshold: 0.7\n</code></pre>"},{"location":"guides/evaluations/#mixed-metrics-deepeval-nlp","title":"Mixed Metrics (DeepEval + NLP)","text":"<pre><code>evaluations:\n  model:\n    provider: ollama\n    name: llama3.2:latest\n    temperature: 0.0\n\n  metrics:\n    # DeepEval metrics (primary)\n    - type: geval\n      name: \"Response Quality\"\n      criteria: \"Evaluate if the response is helpful and accurate.\"\n      threshold: 0.75\n\n    - type: rag\n      metric_type: faithfulness\n      threshold: 0.8\n\n    # NLP metrics (secondary)\n    - type: standard\n      metric: f1_score\n      threshold: 0.7\n\n    - type: standard\n      metric: rouge\n      threshold: 0.6\n</code></pre>"},{"location":"guides/evaluations/#enterprise-setup-with-model-overrides","title":"Enterprise Setup with Model Overrides","text":"<pre><code>evaluations:\n  model:\n    provider: ollama\n    name: llama3.2:latest  # Default: free, local\n    temperature: 0.0\n\n  metrics:\n    # Critical metrics - use powerful model\n    - type: rag\n      metric_type: faithfulness\n      threshold: 0.9\n      model:\n        provider: openai\n        name: gpt-4\n\n    - type: geval\n      name: \"Safety\"\n      criteria: \"Evaluate response safety and appropriateness.\"\n      threshold: 0.95\n      model:\n        provider: openai\n        name: gpt-4\n\n    # Standard metrics - use default local model\n    - type: geval\n      name: \"Coherence\"\n      criteria: \"Evaluate response clarity.\"\n      threshold: 0.75\n\n    - type: rag\n      metric_type: answer_relevancy\n      threshold: 0.7\n\n    # NLP metrics - no LLM needed\n    - type: standard\n      metric: f1_score\n      threshold: 0.7\n</code></pre>"},{"location":"guides/evaluations/#per-test-case-evaluation","title":"Per-Test Case Evaluation","text":"<p>Test cases can specify which metrics to run:</p> <pre><code>test_cases:\n  - name: \"Fact check test\"\n    input: \"What's our company's founding date?\"\n    expected_tools: [search_kb]\n    ground_truth: \"Founded in 2010\"\n    evaluations:\n      - type: rag\n        metric_type: faithfulness\n        threshold: 0.85\n      - type: rag\n        metric_type: answer_relevancy\n        threshold: 0.7\n\n  - name: \"Creative task\"\n    input: \"Generate a company tagline\"\n    evaluations:\n      - type: geval\n        name: \"Creativity\"\n        criteria: \"Evaluate if the tagline is creative and memorable.\"\n        threshold: 0.7\n      # Skip faithfulness since no retrieval context\n</code></pre>"},{"location":"guides/evaluations/#test-execution","title":"Test Execution","text":"<p>When running tests, HoloDeck:</p> <ol> <li>Executes agent with test input</li> <li>Records which tools were called</li> <li>Validates tool usage (<code>expected_tools</code>)</li> <li>Runs each enabled metric</li> <li>Compares results against thresholds</li> <li>Reports pass/fail per metric</li> </ol>"},{"location":"guides/evaluations/#example-output","title":"Example Output","text":"<pre><code>Test: \"Password Reset\"\nInput: \"How do I reset my password?\"\nTools called: [search_kb] \u2713\nMetrics:\n  \u2713 Faithfulness: 0.92 (threshold: 0.8)\n  \u2713 Answer Relevancy: 0.88 (threshold: 0.75)\n  \u2713 Coherence: 0.85 (threshold: 0.7)\n  \u2713 F1 Score: 0.81 (threshold: 0.7)\nResult: PASS\n</code></pre>"},{"location":"guides/evaluations/#cost-optimization","title":"Cost Optimization","text":""},{"location":"guides/evaluations/#use-local-models-for-development","title":"Use Local Models for Development","text":"<pre><code>evaluations:\n  model:\n    provider: ollama           # Free, local\n    name: llama3.2:latest\n</code></pre>"},{"location":"guides/evaluations/#use-paid-models-only-for-critical-metrics","title":"Use Paid Models Only for Critical Metrics","text":"<pre><code>evaluations:\n  model:\n    provider: ollama           # Default: free\n    name: llama3.2:latest\n\n  metrics:\n    - type: rag\n      metric_type: faithfulness\n      model:\n        provider: openai\n        name: gpt-4            # Expensive override only for critical metric\n</code></pre>"},{"location":"guides/evaluations/#use-nlp-metrics-when-possible","title":"Use NLP Metrics When Possible","text":"<p>NLP metrics are free (no LLM calls):</p> <pre><code>- type: standard\n  metric: f1_score  # No LLM cost\n- type: standard\n  metric: rouge     # No LLM cost\n</code></pre>"},{"location":"guides/evaluations/#model-configuration-details","title":"Model Configuration Details","text":"<p>When specifying a model for evaluation:</p> <pre><code>model:\n  provider: ollama|openai|azure_openai|anthropic  # Required\n  name: model-identifier                          # Required\n  temperature: 0.0-2.0                            # Optional (recommend 0.0 for evaluation)\n  max_tokens: integer                             # Optional\n  top_p: 0.0-1.0                                  # Optional\n</code></pre>"},{"location":"guides/evaluations/#provider-specific-models","title":"Provider-Specific Models","text":"<p>Ollama (Recommended for Development) - <code>llama3.2:latest</code> - Fast, capable - <code>llama3.1:latest</code> - More capable - Any model available in your Ollama installation</p> <p>OpenAI - <code>gpt-4o</code> - Latest, best quality - <code>gpt-4o-mini</code> - Fast, cheap - <code>gpt-4-turbo</code> - Previous generation</p> <p>Azure OpenAI - <code>gpt-4</code> - Standard - <code>gpt-4-32k</code> - Extended context</p> <p>Anthropic - <code>claude-3-opus</code> - Most capable - <code>claude-3-sonnet</code> - Balanced - <code>claude-3-haiku</code> - Fast, cheap</p>"},{"location":"guides/evaluations/#recommended-settings-for-evaluation","title":"Recommended Settings for Evaluation","text":"<pre><code>model:\n  provider: ollama\n  name: llama3.2:latest\n  temperature: 0.0  # Deterministic for consistency\n</code></pre>"},{"location":"guides/evaluations/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/evaluations/#error-invalid-metric-type","title":"Error: \"invalid metric type\"","text":"<ul> <li>Check metric type is valid</li> <li>Valid types: <code>geval</code>, <code>rag</code>, <code>standard</code></li> <li>For standard metrics: f1_score, bleu, rouge, meteor</li> <li>For RAG metrics: faithfulness, answer_relevancy, contextual_relevancy, contextual_precision, contextual_recall</li> </ul>"},{"location":"guides/evaluations/#metric-always-fails","title":"Metric always fails","text":"<ul> <li>Check evaluation model is working</li> <li>Try without threshold first</li> <li>Test evaluation model manually</li> <li>For RAG metrics, ensure required parameters are available</li> </ul>"},{"location":"guides/evaluations/#llm-evaluation-too-slow","title":"LLM evaluation too slow","text":"<ul> <li>Use local Ollama model instead of API</li> <li>Use faster model: <code>gpt-4o-mini</code> instead of <code>gpt-4</code></li> <li>Use NLP metrics instead (free and fast)</li> </ul>"},{"location":"guides/evaluations/#inconsistent-evaluation-results","title":"Inconsistent evaluation results","text":"<ul> <li>Set temperature to 0.0 for deterministic results</li> <li>Use more powerful model for complex evaluations</li> <li>Add <code>evaluation_steps</code> to GEval for better consistency</li> </ul>"},{"location":"guides/evaluations/#rag-metric-missing-retrieval_context","title":"RAG metric missing retrieval_context","text":"<ul> <li>Ensure your agent uses a vectorstore tool</li> <li>The test runner automatically extracts retrieval_context from tool results</li> <li>Or provide manual retrieval_context in test case</li> </ul>"},{"location":"guides/evaluations/#best-practices","title":"Best Practices","text":"<ol> <li>Start with DeepEval: Use GEval and RAG metrics as primary evaluation</li> <li>Use Local Models: Start with Ollama for free development, upgrade for production</li> <li>Mix Metric Types: Combine DeepEval (semantic) with NLP (keyword-based)</li> <li>Cost-Aware: Use cheaper/local models by default, expensive models only for critical metrics</li> <li>Realistic Thresholds: Set thresholds based on actual agent performance</li> <li>Monitor: Run metrics on sample of tests first</li> <li>Iterate: Adjust thresholds and metrics based on results</li> <li>Migrate from Legacy: Replace deprecated Azure AI metrics with DeepEval equivalents</li> </ol>"},{"location":"guides/evaluations/#next-steps","title":"Next Steps","text":"<ul> <li>See Agent Configuration Guide for how to set up evaluations</li> <li>See Examples for complete evaluation configurations</li> <li>See Global Configuration for shared settings</li> <li>See API Reference for evaluator class details</li> </ul>"},{"location":"guides/file-references/","title":"File References Guide","text":"<p>This guide explains how file paths work in HoloDeck configurations.</p>"},{"location":"guides/file-references/#overview","title":"Overview","text":"<p>HoloDeck uses file references in several places:</p> <ul> <li>Instructions: <code>instructions.file</code> for system prompts</li> <li>Tools: <code>source</code> for vectorstore data, <code>file</code> for function code</li> <li>Prompts: <code>file</code> for template files</li> </ul> <p>This guide explains path resolution rules.</p>"},{"location":"guides/file-references/#path-resolution-rules","title":"Path Resolution Rules","text":""},{"location":"guides/file-references/#rule-1-relative-paths-default","title":"Rule 1: Relative Paths (Default)","text":"<p>Paths are relative to the directory containing agent.yaml:</p> <pre><code>project/\n\u251c\u2500\u2500 agent.yaml          (references below)\n\u251c\u2500\u2500 system_prompt.txt   \u2190 relative path: system_prompt.txt\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 kb.json         \u2190 relative path: data/kb.json\n\u2514\u2500\u2500 tools/\n    \u2514\u2500\u2500 search.py       \u2190 relative path: tools/search.py\n</code></pre> <p>Usage in <code>agent.yaml</code>:</p> <pre><code># From project/agent.yaml\n\ninstructions:\n  file: system_prompt.txt        # Resolves to: project/system_prompt.txt\n\ntools:\n  - type: vectorstore\n    source: data/kb.json         # Resolves to: project/data/kb.json\n\n  - type: function\n    file: tools/search.py        # Resolves to: project/tools/search.py\n</code></pre>"},{"location":"guides/file-references/#rule-2-absolute-paths","title":"Rule 2: Absolute Paths","text":"<p>Paths starting with <code>/</code> are absolute:</p> <pre><code>instructions:\n  file: /etc/holodeck/system_prompt.txt  # Absolute path\n\ntools:\n  - type: vectorstore\n    source: /data/knowledge_base/        # Absolute path\n</code></pre>"},{"location":"guides/file-references/#rule-3-home-directory-expansion","title":"Rule 3: Home Directory Expansion","text":"<p><code>~</code> expands to user home directory:</p> <pre><code>instructions:\n  file: ~/templates/prompt.txt    # Expands to: /home/user/templates/prompt.txt\n</code></pre> <p>On different systems:</p> <ul> <li>Linux: <code>/home/username/</code></li> <li>macOS: <code>/Users/username/</code></li> <li>Windows: <code>C:\\Users\\username\\</code></li> </ul>"},{"location":"guides/file-references/#common-path-patterns","title":"Common Path Patterns","text":""},{"location":"guides/file-references/#sibling-files","title":"Sibling Files","text":"<p>Instructions in same directory as agent:</p> <pre><code># project/agent.yaml\ninstructions:\n  file: system_prompt.txt\n\n# File: project/system_prompt.txt\n</code></pre>"},{"location":"guides/file-references/#subdirectories","title":"Subdirectories","text":"<p>Tools in subdirectory:</p> <pre><code># project/agent.yaml\ntools:\n  - type: function\n    file: tools/my_tool.py\n\n# File: project/tools/my_tool.py\n</code></pre>"},{"location":"guides/file-references/#parent-directory","title":"Parent Directory","text":"<p>Reference parent directory with <code>..</code>:</p> <pre><code># project/agents/support/agent.yaml\ninstructions:\n  file: ../../shared/system_prompt.txt\n\n# File: project/shared/system_prompt.txt\n</code></pre>"},{"location":"guides/file-references/#deeply-nested","title":"Deeply Nested","text":"<p>Multiple levels:</p> <pre><code># project/agents/v2/beta/agent.yaml\ntools:\n  - type: vectorstore\n    source: ../../../../data/kb/\n\n# File: project/data/kb/\n</code></pre>"},{"location":"guides/file-references/#validation","title":"Validation","text":"<p>HoloDeck validates file paths during configuration loading:</p>"},{"location":"guides/file-references/#validation-rules","title":"Validation Rules","text":"<ol> <li>File must exist: Checked when agent loads</li> <li>Readable: File must be readable by current user</li> <li>Type checking: File type must match context</li> <li>Instructions: Text file (<code>.txt</code>, <code>.md</code>, etc.)</li> <li>Vectorstore: Data file or directory</li> <li>Function: Python file (<code>.py</code>)</li> <li>Prompt: Text file</li> </ol>"},{"location":"guides/file-references/#validation-errors","title":"Validation Errors","text":"<pre><code>Error: File not found\nPath: tools/search.py\nExpected at: /home/user/project/tools/search.py\nSuggestions:\n- Check file exists in project directory\n- Use relative path from agent.yaml directory\n- Use absolute path if outside project\n</code></pre>"},{"location":"guides/file-references/#examples-by-file-type","title":"Examples by File Type","text":""},{"location":"guides/file-references/#instructions-file","title":"Instructions File","text":"<p>Structure:</p> <pre><code>project/\n\u251c\u2500\u2500 agent.yaml\n\u2514\u2500\u2500 system_prompt.txt\n</code></pre> <p>agent.yaml:</p> <pre><code>name: my-agent\n\ninstructions:\n  file: system_prompt.txt\n</code></pre> <p>system_prompt.txt:</p> <pre><code>You are a helpful assistant.\nAnswer questions clearly and concisely.\n</code></pre>"},{"location":"guides/file-references/#vectorstore-data-files","title":"Vectorstore Data Files","text":"<p>Structure for single file:</p> <pre><code>project/\n\u251c\u2500\u2500 agent.yaml\n\u2514\u2500\u2500 knowledge_base.json\n</code></pre> <p>agent.yaml:</p> <pre><code>tools:\n  - name: search-kb\n    type: vectorstore\n    source: knowledge_base.json\n</code></pre> <p>Structure for directory:</p> <pre><code>project/\n\u251c\u2500\u2500 agent.yaml\n\u2514\u2500\u2500 data/\n    \u251c\u2500\u2500 doc1.md\n    \u251c\u2500\u2500 doc2.md\n    \u2514\u2500\u2500 doc3.txt\n</code></pre> <p>agent.yaml:</p> <pre><code>tools:\n  - name: search-docs\n    type: vectorstore\n    source: data/\n</code></pre>"},{"location":"guides/file-references/#function-tool-files","title":"Function Tool Files","text":"<p>Structure:</p> <pre><code>project/\n\u251c\u2500\u2500 agent.yaml\n\u2514\u2500\u2500 tools/\n    \u251c\u2500\u2500 search.py\n    \u2514\u2500\u2500 database.py\n</code></pre> <p>agent.yaml:</p> <pre><code>tools:\n  - name: search-function\n    type: function\n    file: tools/search.py\n    function: search_database\n\n  - name: get-user\n    type: function\n    file: tools/database.py\n    function: get_user\n</code></pre>"},{"location":"guides/file-references/#prompt-tool-files","title":"Prompt Tool Files","text":"<p>Structure:</p> <pre><code>project/\n\u251c\u2500\u2500 agent.yaml\n\u2514\u2500\u2500 prompts/\n    \u251c\u2500\u2500 summarize.txt\n    \u2514\u2500\u2500 classify.txt\n</code></pre> <p>agent.yaml:</p> <pre><code>tools:\n  - name: summarize\n    type: prompt\n    file: prompts/summarize.txt\n    parameters:\n      text:\n        type: string\n\n  - name: classify\n    type: prompt\n    file: prompts/classify.txt\n    parameters:\n      text:\n        type: string\n</code></pre>"},{"location":"guides/file-references/#complex-project-structures","title":"Complex Project Structures","text":""},{"location":"guides/file-references/#monorepo-with-multiple-agents","title":"Monorepo with Multiple Agents","text":"<pre><code>project/\n\u251c\u2500\u2500 shared/\n\u2502   \u251c\u2500\u2500 system_prompts/\n\u2502   \u2502   \u251c\u2500\u2500 support.txt\n\u2502   \u2502   \u251c\u2500\u2500 sales.txt\n\u2502   \u2502   \u2514\u2500\u2500 backend.txt\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 kb.json\n\u2502   \u2502   \u2514\u2500\u2500 faq.csv\n\u2502   \u2514\u2500\u2500 tools/\n\u2502       \u251c\u2500\u2500 common.py\n\u2502       \u251c\u2500\u2500 database.py\n\u2502       \u2514\u2500\u2500 api.py\n\u251c\u2500\u2500 agents/\n\u2502   \u251c\u2500\u2500 support/\n\u2502   \u2502   \u2514\u2500\u2500 agent.yaml\n\u2502   \u251c\u2500\u2500 sales/\n\u2502   \u2502   \u2514\u2500\u2500 agent.yaml\n\u2502   \u2514\u2500\u2500 backend/\n\u2502       \u2514\u2500\u2500 agent.yaml\n</code></pre> <p>Support agent config:</p> <pre><code># agents/support/agent.yaml\n\ninstructions:\n  file: ../../shared/system_prompts/support.txt\n\ntools:\n  - name: search-kb\n    type: vectorstore\n    source: ../../shared/data/kb.json\n\n  - name: query-db\n    type: function\n    file: ../../shared/tools/database.py\n    function: query\n</code></pre>"},{"location":"guides/file-references/#shared-templates-directory","title":"Shared Templates Directory","text":"<pre><code>project/\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 system_prompt.txt\n\u2502   \u2514\u2500\u2500 prompts/\n\u2502       \u251c\u2500\u2500 summarize.txt\n\u2502       \u2514\u2500\u2500 analyze.txt\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 kb/\n\u2514\u2500\u2500 agents/\n    \u2514\u2500\u2500 agent.yaml\n</code></pre> <p>Agent config:</p> <pre><code># agents/agent.yaml\n\ninstructions:\n  file: ../templates/system_prompt.txt\n\ntools:\n  - name: search\n    type: vectorstore\n    source: ../data/kb/\n\n  - name: summarize\n    type: prompt\n    file: ../templates/prompts/summarize.txt\n    parameters:\n      text:\n        type: string\n</code></pre>"},{"location":"guides/file-references/#environment-variables-in-paths","title":"Environment Variables in Paths","text":"<p>Paths can include environment variables:</p> <pre><code>instructions:\n  file: ${TEMPLATE_DIR}/system_prompt.txt\n\ntools:\n  - type: vectorstore\n    source: ${DATA_DIR}/knowledge_base/\n</code></pre> <p>Environment setup:</p> <pre><code>export TEMPLATE_DIR=/home/user/templates\nexport DATA_DIR=/home/user/data\n</code></pre> <p>Result:</p> <pre><code>instructions.file \u2192 /home/user/templates/system_prompt.txt\ntools[0].source \u2192 /home/user/data/knowledge_base/\n</code></pre>"},{"location":"guides/file-references/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/file-references/#error-file-not-found","title":"Error: \"File not found\"","text":"<p>Problem: File doesn't exist at specified path</p> <p>Solutions:</p> <ol> <li> <p>Check file exists in filesystem:    <pre><code>ls -la project/system_prompt.txt\n</code></pre></p> </li> <li> <p>Verify path is relative to agent.yaml directory:    <pre><code>agent.yaml location: /home/user/project/\nreferenced file: system_prompt.txt\nexpected location: /home/user/project/system_prompt.txt\n</code></pre></p> </li> <li> <p>Check absolute path if using one:    <pre><code>ls -la /full/path/to/file.txt\n</code></pre></p> </li> <li> <p>Expand environment variables manually:    <pre><code>echo \"${TEMPLATE_DIR}/system_prompt.txt\"\n</code></pre></p> </li> </ol>"},{"location":"guides/file-references/#error-permission-denied","title":"Error: \"Permission denied\"","text":"<p>Problem: File exists but not readable</p> <p>Solutions:</p> <ol> <li> <p>Check file permissions:    <pre><code>ls -la project/system_prompt.txt\n# Should have read permission for current user\n</code></pre></p> </li> <li> <p>Make file readable:    <pre><code>chmod 644 project/system_prompt.txt\n</code></pre></p> </li> <li> <p>Check directory permissions:    <pre><code>chmod 755 project/\n</code></pre></p> </li> </ol>"},{"location":"guides/file-references/#error-invalid-file-type","title":"Error: \"Invalid file type\"","text":"<p>Problem: File format doesn't match context</p> <p>Solutions:</p> <ol> <li>For instructions: Use text files (<code>.txt</code>, <code>.md</code>)</li> <li>For vectorstore: Use data files (<code>.json</code>, <code>.csv</code>, <code>.md</code>) or directories</li> <li>For function tools: Use Python files (<code>.py</code>)</li> <li>For prompts: Use text files (<code>.txt</code>, <code>.md</code>)</li> </ol>"},{"location":"guides/file-references/#error-path-traversal-outside-project","title":"Error: \"Path traversal outside project\"","text":"<p>Problem: Path goes outside allowed directory</p> <p>Solutions:</p> <ol> <li> <p>Use absolute paths for files outside project:    <pre><code>instructions:\n  file: /etc/holodeck/system_prompt.txt\n</code></pre></p> </li> <li> <p>Or relative path if within allowed scope:    <pre><code>instructions:\n  file: ../../shared/prompt.txt\n</code></pre></p> </li> </ol>"},{"location":"guides/file-references/#path-not-expanding-environment-variable","title":"Path Not Expanding (Environment Variable)","text":"<p>Problem: <code>${VAR}</code> not replaced with actual value</p> <p>Solutions:</p> <ol> <li> <p>Check variable is exported:    <pre><code>export TEMPLATE_DIR=/path/to/templates\n</code></pre></p> </li> <li> <p>Not just set locally:    <pre><code># Wrong:\nTEMPLATE_DIR=/path/to/templates\nholodeck test agent.yaml\n\n# Right:\nexport TEMPLATE_DIR=/path/to/templates\nholodeck test agent.yaml\n</code></pre></p> </li> <li> <p>Or use absolute path instead:    <pre><code>instructions:\n  file: /path/to/templates/system_prompt.txt\n</code></pre></p> </li> </ol>"},{"location":"guides/file-references/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Relative Paths: Keep agents portable across machines    <pre><code>instructions:\n  file: system_prompt.txt  # Good\n  # NOT: file: /home/user/project/system_prompt.txt\n</code></pre></p> </li> <li> <p>Organize by Type: Group related files    <pre><code>project/\n\u251c\u2500\u2500 prompts/          \u2190 All prompt templates\n\u251c\u2500\u2500 data/             \u2190 All data files\n\u251c\u2500\u2500 tools/            \u2190 All function tools\n\u2514\u2500\u2500 agent.yaml\n</code></pre></p> </li> <li> <p>Document Structure: Include README    <pre><code>project/\n\u251c\u2500\u2500 README.md         \u2190 Explain file structure\n\u251c\u2500\u2500 agent.yaml\n\u251c\u2500\u2500 system_prompt.txt\n\u2514\u2500\u2500 ...\n</code></pre></p> </li> <li> <p>Use Consistent Naming: Predictable organization    <pre><code>project/\n\u251c\u2500\u2500 prompts/          \u2190 Not prompt/ or system_prompts/\n\u251c\u2500\u2500 tools/            \u2190 Not tool/ or functions/\n\u2514\u2500\u2500 data/             \u2190 Not database/ or knowledge_base/\n</code></pre></p> </li> <li> <p>Don't Hardcode Paths: Use environment variables or global config    <pre><code># Wrong:\ninstructions:\n  file: /Users/john/project/system_prompt.txt\n\n# Right:\ninstructions:\n  file: system_prompt.txt\n</code></pre></p> </li> <li> <p>Verify on Startup: HoloDeck checks all paths when agent loads</p> </li> <li>No runtime surprises</li> <li>Errors caught early</li> </ol>"},{"location":"guides/file-references/#next-steps","title":"Next Steps","text":"<ul> <li>See Agent Configuration Guide for usage examples</li> <li>See Global Configuration Guide for environment variables</li> <li>See Tools Reference for specific tool file requirements</li> </ul>"},{"location":"guides/global-config/","title":"Global Configuration Guide","text":"<p>This guide explains HoloDeck's global configuration system for shared settings across agents.</p>"},{"location":"guides/global-config/#overview","title":"Overview","text":"<p>Global configuration lives at <code>~/.holodeck/config.yaml</code> and provides default settings that apply to all agents. Use global config to:</p> <ul> <li>Set default LLM providers and credentials</li> <li>Define reusable vectorstore connections</li> <li>Configure deployment defaults</li> <li>Store API keys securely</li> <li>Reduce duplication across agent.yaml files</li> </ul>"},{"location":"guides/global-config/#basic-structure","title":"Basic Structure","text":"<pre><code># config.yaml (project root or ~/.holodeck/config.yaml)\n\nproviders:\n  azure_openai:\n    provider: azure_openai\n    name: gpt-4o\n    temperature: 0.3\n    max_tokens: 2048\n    endpoint: ${AZURE_OPENAI_ENDPOINT}\n    api_key: ${AZURE_OPENAI_API_KEY}\n\nexecution:\n  file_timeout: 30\n  llm_timeout: 60\n  download_timeout: 30\n  cache_enabled: true\n  cache_dir: .holodeck_cache\n  verbose: false\n</code></pre>"},{"location":"guides/global-config/#configuration-precedence","title":"Configuration Precedence","text":"<p>When multiple configuration sources define the same setting, HoloDeck applies them in priority order:</p> <pre><code>1. agent.yaml (Highest Priority)\n   \u251c\u2500 Explicit values in agent configuration\n   \u2502\n2. Environment Variables (High Priority)\n   \u251c\u2500 ${VAR_NAME} patterns in agent.yaml or global config\n   \u2502\n3. Project-level config.yaml (Medium Priority)\n   \u251c\u2500 Same directory as agent.yaml\n   \u2502\n4. ~/.holodeck/config.yaml (Lowest Priority)\n   \u2514\u2500 User home directory global defaults\n</code></pre>"},{"location":"guides/global-config/#precedence-diagram","title":"Precedence Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   agent.yaml explicit        \u2502  Takes precedence\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Environment variables      \u2502  Used if agent.yaml absent\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Project-level config.yaml   \u2502  Used if env var absent\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ~/.holodeck/config.yaml      \u2502  Fallback default\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/global-config/#examples","title":"Examples","text":""},{"location":"guides/global-config/#example-1-provider-override","title":"Example 1: Provider Override","text":"<p>Global config: <pre><code>providers:\n  openai:\n    model: gpt-4o-mini\n    temperature: 0.7\n</code></pre></p> <p>Agent config: <pre><code>model:\n  provider: openai\n  name: gpt-4o       # Overrides global default\n  temperature: 0.5   # Overrides global default\n</code></pre></p> <p>Result: Agent uses <code>gpt-4o</code> at temperature <code>0.5</code> (agent config wins)</p>"},{"location":"guides/global-config/#example-2-environment-variable","title":"Example 2: Environment Variable","text":"<p>Global config: <pre><code>providers:\n  openai:\n    api_key: ${OPENAI_API_KEY}\n</code></pre></p> <p>Agent config: <pre><code>model:\n  provider: openai\n</code></pre></p> <p>Environment: <pre><code>export OPENAI_API_KEY=\"sk-...\"\n</code></pre></p> <p>Result: Uses environment variable for API key</p>"},{"location":"guides/global-config/#example-3-full-precedence-chain","title":"Example 3: Full Precedence Chain","text":"<p>Global config: <pre><code>providers:\n  default_model: gpt-4o-mini\n\ndeployment:\n  default_port: 8000\n</code></pre></p> <p>Agent config: <pre><code>model:\n  provider: openai\n  # No explicit temperature\n\ndeployment:\n  port: 8080  # Overrides global\n</code></pre></p> <p>Environment: <pre><code>export TEMPERATURE=0.5\n</code></pre></p> <p>Result: Model uses <code>gpt-4o-mini</code>, port is <code>8080</code>, temperature is <code>0.5</code></p>"},{"location":"guides/global-config/#inheritance-rules","title":"Inheritance Rules","text":"<p>Not all agent settings are inherited from global config. Here's what you can and cannot configure globally:</p>"},{"location":"guides/global-config/#settings-that-can-be-inherited","title":"Settings That CAN Be Inherited","text":"<ul> <li>LLM provider credentials (API keys, endpoints)</li> <li>Default model names and settings (temperature, max_tokens)</li> <li>Vectorstore configurations</li> <li>Deployment settings</li> </ul>"},{"location":"guides/global-config/#settings-that-cannot-be-inherited-agent-level-only","title":"Settings That CANNOT Be Inherited (Agent-Level Only)","text":"<ul> <li>response_format: Define structured output schema in each agent.yaml, not in global config</li> <li>tools: Tools must be defined per agent based on agent capabilities</li> <li>instructions: System prompt must be specific to each agent</li> <li>evaluations: Evaluation metrics are typically agent-specific</li> <li>test_cases: Test cases validate individual agent behavior</li> </ul> <pre><code># .holodeck/config.yaml - Only shared settings here\nproviders:\n  openai:\n    api_key: ${OPENAI_API_KEY}\n    organization: my-org\n    temperature: 0.7\n\ndeployment:\n  default_port: 8000\n</code></pre> <pre><code># agent.yaml - Agent-specific settings here\nname: my-agent\n\nmodel:\n  provider: openai\n  # Inherits temperature: 0.7 from global, can override\n\nresponse_format:  # Cannot be inherited, must define here\n  type: object\n  properties:\n    answer:\n      type: string\n\ninstructions:  # Must be defined here\n  inline: \"You are a helpful assistant\"\n</code></pre>"},{"location":"guides/global-config/#providers-section","title":"Providers Section","text":"<p>Defines LLM provider configurations with credentials and defaults.</p> <pre><code>providers:\n  azure_openai:\n    provider: azure_openai              # Required: provider type\n    name: gpt-4o                        # Required: model name\n    temperature: 0.3                    # Optional: temperature (0.0-2.0)\n    max_tokens: 2048                    # Optional: max tokens\n    endpoint: ${AZURE_OPENAI_ENDPOINT}  # Required: Azure endpoint\n    api_key: ${AZURE_OPENAI_API_KEY}    # Required: API key\n\n  openai:\n    provider: openai\n    name: gpt-4o-mini\n    temperature: 0.7\n    api_key: ${OPENAI_API_KEY}\n    organization: my-org                # Optional\n\n  anthropic:\n    provider: anthropic\n    name: claude-3-sonnet\n    temperature: 0.5\n    api_key: ${ANTHROPIC_API_KEY}\n</code></pre>"},{"location":"guides/global-config/#provider-configuration-fields","title":"Provider Configuration Fields","text":"<p>Each provider must have:</p> <ul> <li>provider (Required): Provider type - <code>openai</code>, <code>azure_openai</code>, or <code>anthropic</code></li> <li>name (Required): Model identifier (e.g., <code>gpt-4o</code>, <code>claude-3-sonnet</code>)</li> <li>api_key (Required): API authentication key (use <code>${ENV_VAR}</code> for environment variables)</li> </ul> <p>Optional fields:</p> <ul> <li>temperature (Optional): Float 0.0-2.0, defaults to provider's default</li> <li>max_tokens (Optional): Maximum response length</li> <li>endpoint (Required for Azure): Azure OpenAI endpoint URL</li> <li>organization (Optional for OpenAI): Organization ID</li> </ul>"},{"location":"guides/global-config/#execution-section","title":"Execution Section","text":"<p>Configures execution settings for agent test runs and file processing.</p> <pre><code>execution:\n  file_timeout: 30            # Timeout for file processing (seconds)\n  llm_timeout: 60             # Timeout for LLM API calls (seconds)\n  download_timeout: 30        # Timeout for downloading files (seconds)\n  cache_enabled: true         # Enable caching of file downloads\n  cache_dir: .holodeck_cache  # Directory for cache storage\n  verbose: false              # Enable verbose logging\n  quiet: false                # Enable quiet mode\n</code></pre>"},{"location":"guides/global-config/#execution-fields","title":"Execution Fields","text":"<ul> <li>file_timeout (Optional): Seconds to wait for file operations (default: 30)</li> <li>llm_timeout (Optional): Seconds to wait for LLM API calls (default: 60)</li> <li>download_timeout (Optional): Seconds to wait for file downloads (default: 30)</li> <li>cache_enabled (Optional): Enable caching of downloaded files (default: true)</li> <li>cache_dir (Optional): Directory for storing cached files (default: <code>.holodeck_cache</code>)</li> <li>verbose (Optional): Enable verbose logging output (default: false)</li> <li>quiet (Optional): Enable quiet mode, suppressing non-critical output (default: false)</li> </ul>"},{"location":"guides/global-config/#environment-variables","title":"Environment Variables","text":"<p>Replace sensitive values with environment variables using <code>${VAR_NAME}</code> syntax:</p> <pre><code>providers:\n  openai:\n    api_key: ${OPENAI_API_KEY}      # Reads from environment\n    organization: my-org             # Literal value\n</code></pre>"},{"location":"guides/global-config/#setting-environment-variables","title":"Setting Environment Variables","text":"<p>On Linux/macOS:</p> <pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre> <p>On Windows:</p> <pre><code>set OPENAI_API_KEY=sk-...\nset ANTHROPIC_API_KEY=sk-ant-...\n</code></pre> <p>In .env file (automatic loading):</p> <p>Create <code>.env</code> in project directory:</p> <pre><code>OPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre> <p>HoloDeck automatically loads <code>.env</code> file if present.</p>"},{"location":"guides/global-config/#variable-precedence","title":"Variable Precedence","text":"<p>For <code>${VARIABLE_NAME}</code>:</p> <ol> <li>Check environment variable</li> <li>Check .env file</li> <li>Return empty string if not found (error at agent runtime)</li> </ol>"},{"location":"guides/global-config/#file-locations","title":"File Locations","text":""},{"location":"guides/global-config/#default-location","title":"Default Location","text":"<pre><code>~/.holodeck/config.yaml\n</code></pre> <p>On different operating systems:</p> <ul> <li>Linux: <code>/home/username/.holodeck/config.yaml</code></li> <li>macOS: <code>/Users/username/.holodeck/config.yaml</code></li> <li>Windows: <code>C:\\Users\\username\\.holodeck\\config.yaml</code></li> </ul>"},{"location":"guides/global-config/#custom-location-future","title":"Custom Location (Future)","text":"<pre><code>holodeck --config /path/to/custom.yaml ...\n</code></pre>"},{"location":"guides/global-config/#complete-example","title":"Complete Example","text":"<pre><code># config.yaml (project root or ~/.holodeck/config.yaml)\n\n# LLM Provider Configurations\nproviders:\n  azure_openai:\n    provider: azure_openai\n    name: gpt-4o\n    temperature: 0.3\n    max_tokens: 2048\n    endpoint: ${AZURE_OPENAI_ENDPOINT}\n    api_key: ${AZURE_OPENAI_API_KEY}\n\n  openai:\n    provider: openai\n    name: gpt-4o-mini\n    temperature: 0.7\n    max_tokens: 1024\n    api_key: ${OPENAI_API_KEY}\n    organization: acme-corp\n\n  anthropic:\n    provider: anthropic\n    name: claude-3-sonnet\n    temperature: 0.5\n    api_key: ${ANTHROPIC_API_KEY}\n\n# Execution Configuration\nexecution:\n  file_timeout: 30\n  llm_timeout: 60\n  download_timeout: 30\n  cache_enabled: true\n  cache_dir: .holodeck_cache\n  verbose: false\n</code></pre>"},{"location":"guides/global-config/#usage-patterns","title":"Usage Patterns","text":""},{"location":"guides/global-config/#pattern-1-secure-api-keys","title":"Pattern 1: Secure API Keys","text":"<p>Keep secrets in global config with environment variable substitution:</p> <p>Global config (project root): <pre><code># config.yaml\nproviders:\n  azure_openai:\n    provider: azure_openai\n    name: gpt-4o\n    endpoint: ${AZURE_OPENAI_ENDPOINT}\n    api_key: ${AZURE_OPENAI_API_KEY}\n</code></pre></p> <p>Agent config: <pre><code># agent.yaml\nmodel:\n  provider: azure_openai\n  # Credentials come from global config\n</code></pre></p> <p>Environment: <pre><code>export AZURE_OPENAI_ENDPOINT=\"https://...\"\nexport AZURE_OPENAI_API_KEY=\"...\"\n</code></pre></p>"},{"location":"guides/global-config/#pattern-2-execution-defaults","title":"Pattern 2: Execution Defaults","text":"<p>Set timeouts and caching for all agents:</p> <p>Global config: <pre><code># config.yaml\nproviders:\n  azure_openai:\n    provider: azure_openai\n    name: gpt-4o\n    api_key: ${AZURE_OPENAI_API_KEY}\n\nexecution:\n  file_timeout: 30\n  llm_timeout: 60\n  cache_enabled: true\n  verbose: false\n</code></pre></p> <p>All agents inherit these execution settings automatically.</p>"},{"location":"guides/global-config/#pattern-3-multiple-providers","title":"Pattern 3: Multiple Providers","text":"<p>Configure multiple providers for different use cases:</p> <p>Global config: <pre><code># config.yaml\nproviders:\n  azure_openai:\n    provider: azure_openai\n    name: gpt-4o\n    api_key: ${AZURE_OPENAI_API_KEY}\n    endpoint: ${AZURE_OPENAI_ENDPOINT}\n\n  openai:\n    provider: openai\n    name: gpt-4o-mini\n    api_key: ${OPENAI_API_KEY}\n\nexecution:\n  llm_timeout: 60\n</code></pre></p> <p>Agent config (use either provider): <pre><code># agent.yaml\nmodel:\n  provider: azure_openai  # or openai\n  # Model name and settings come from global config\n</code></pre></p>"},{"location":"guides/global-config/#creating-configuration","title":"Creating Configuration","text":"<p>You can create configuration files using the <code>holodeck config init</code> command or manually.</p>"},{"location":"guides/global-config/#using-the-cli-recommended","title":"Using the CLI (Recommended)","text":"<p>The CLI provides a convenient way to initialize configuration files with default settings.</p> <p>Initialize Global Configuration: <pre><code>holodeck config init -g\n# Creates ~/.holodeck/config.yaml\n</code></pre></p> <p>Initialize Project Configuration: <pre><code>holodeck config init -p\n# Creates config.yaml in the current directory\n</code></pre></p>"},{"location":"guides/global-config/#manual-creation","title":"Manual Creation","text":"<p>Global config can be created at two locations with different precedence:</p> <ol> <li>Project-level: <code>config.yaml</code> in same directory as <code>agent.yaml</code> (higher priority)</li> <li>User-level: <code>~/.holodeck/config.yaml</code> in home directory (lower priority)</li> </ol>"},{"location":"guides/global-config/#project-level-config-recommended-for-teams","title":"Project-Level Config (Recommended for Teams)","text":"<p>Create <code>config.yaml</code> alongside your agents:</p> <pre><code>my-project/\n\u251c\u2500\u2500 config.yaml          # Project-specific configuration\n\u251c\u2500\u2500 agent1/\n\u2502   \u2514\u2500\u2500 agent.yaml\n\u2514\u2500\u2500 agent2/\n    \u2514\u2500\u2500 agent.yaml\n</code></pre> <p>Content of <code>config.yaml</code>:</p> <pre><code>providers:\n  azure_openai:\n    provider: azure_openai\n    name: gpt-4o\n    api_key: ${AZURE_OPENAI_API_KEY}\n    endpoint: ${AZURE_OPENAI_ENDPOINT}\n\nexecution:\n  llm_timeout: 60\n</code></pre>"},{"location":"guides/global-config/#user-level-config-global-defaults","title":"User-Level Config (Global Defaults)","text":"<p>Create <code>~/.holodeck/config.yaml</code> in your home directory:</p> <pre><code>mkdir -p ~/.holodeck\n\ncat &gt; ~/.holodeck/config.yaml &lt;&lt; 'EOF'\nproviders:\n  azure_openai:\n    provider: azure_openai\n    name: gpt-4o\n    api_key: ${AZURE_OPENAI_API_KEY}\n    endpoint: ${AZURE_OPENAI_ENDPOINT}\nEOF\n</code></pre>"},{"location":"guides/global-config/#setting-environment-variables_1","title":"Setting Environment Variables","text":"<pre><code>export AZURE_OPENAI_API_KEY=\"...\"\nexport AZURE_OPENAI_ENDPOINT=\"https://...\"\n</code></pre> <p>Or in <code>.env</code> file at project root:</p> <pre><code>AZURE_OPENAI_API_KEY=...\nAZURE_OPENAI_ENDPOINT=...\n</code></pre>"},{"location":"guides/global-config/#running-an-agent","title":"Running an Agent","text":"<pre><code>holodeck test agent.yaml\n</code></pre> <p>The agent will automatically load config from project root or <code>~/.holodeck/</code>.</p>"},{"location":"guides/global-config/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/global-config/#error-api_key-not-found","title":"Error: \"api_key not found\"","text":"<ul> <li>Check global config exists at <code>~/.holodeck/config.yaml</code></li> <li>Verify environment variable is set: <code>echo $OPENAI_API_KEY</code></li> <li>Check variable name matches in config</li> </ul>"},{"location":"guides/global-config/#error-invalid-provider","title":"Error: \"invalid provider\"","text":"<ul> <li>Check spelling of provider in agent.yaml</li> <li>Valid providers: <code>openai</code>, <code>azure_openai</code>, <code>anthropic</code></li> </ul>"},{"location":"guides/global-config/#agent-ignoring-global-config","title":"Agent ignoring global config","text":"<ul> <li>Verify global config file exists</li> <li>Check file permissions: <code>ls -la ~/.holodeck/</code></li> <li>Verify YAML syntax: <code>cat ~/.holodeck/config.yaml</code></li> </ul>"},{"location":"guides/global-config/#environment-variable-not-expanding","title":"Environment variable not expanding","text":"<ul> <li>Check syntax: <code>${VAR_NAME}</code> (with braces)</li> <li>Verify variable exists: <code>env | grep VAR_NAME</code></li> <li>Note: <code>$VAR_NAME</code> (without braces) is not expanded</li> </ul>"},{"location":"guides/global-config/#best-practices","title":"Best Practices","text":"<ol> <li>Keep Secrets Secure: Never commit API keys to version control</li> <li>Use Environment Variables: Store keys in env, not YAML</li> <li>Global Defaults: Use global config for shared organization settings</li> <li>Per-Agent Overrides: Use agent.yaml for agent-specific settings</li> <li>Don't Over-Configure: Keep global config minimal and focused</li> <li>Document Settings: Add comments to explain why settings exist</li> <li>Version Control: Commit <code>config.yaml.example</code> with placeholders, not real keys</li> </ol>"},{"location":"guides/global-config/#example-secure-setup","title":"Example: Secure Setup","text":"<pre><code># 1. Create project-level config\ncat &gt; config.yaml &lt;&lt; 'EOF'\nproviders:\n  azure_openai:\n    provider: azure_openai\n    name: gpt-4o\n    api_key: ${AZURE_OPENAI_API_KEY}\n    endpoint: ${AZURE_OPENAI_ENDPOINT}\n\nexecution:\n  llm_timeout: 60\n  file_timeout: 30\nEOF\n\n# 2. Create agent config\ncat &gt; agent.yaml &lt;&lt; 'EOF'\nname: my-agent\n\nmodel:\n  provider: azure_openai\n\ninstructions:\n  inline: \"You are a helpful assistant.\"\n\ntest_cases:\n  - input: \"Hello!\"\n    ground_truth: \"Hi there! How can I help?\"\n    evaluations:\n      - f1_score\n\nevaluations:\n  model:\n    provider: azure_openai\n  metrics:\n    - metric: f1_score\n      threshold: 0.7\nEOF\n\n# 3. Create .env file with secrets (DO NOT commit)\ncat &gt; .env &lt;&lt; 'EOF'\nAZURE_OPENAI_API_KEY=your-key-here\nAZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com/\nEOF\n\n# 4. Run agent (config and env automatically loaded)\nholodeck test agent.yaml\n</code></pre>"},{"location":"guides/global-config/#next-steps","title":"Next Steps","text":"<ul> <li>See Agent Configuration Guide for agent-specific settings</li> <li>See File References Guide for path resolution</li> <li>See Environment Variables Documentation (future)</li> </ul>"},{"location":"guides/llm-providers/","title":"LLM Providers Guide","text":"<p>This guide explains how to configure LLM providers in HoloDeck for your AI agents.</p>"},{"location":"guides/llm-providers/#overview","title":"Overview","text":"<p>HoloDeck supports multiple LLM providers, allowing you to choose the best model for your use case. Provider configuration can be defined at two levels:</p> <ul> <li>Global Configuration (<code>config.yaml</code>): Shared settings and API credentials</li> <li>Agent Configuration (<code>agent.yaml</code>): Per-agent model selection and overrides</li> </ul>"},{"location":"guides/llm-providers/#supported-providers","title":"Supported Providers","text":"Provider Description API Key Required <code>openai</code> OpenAI API (GPT-4o, GPT-4o-mini, etc.) Yes <code>azure_openai</code> Azure OpenAI Service Yes + Endpoint <code>anthropic</code> Anthropic Claude models Yes <code>ollama</code> Local models via Ollama No (Endpoint required)"},{"location":"guides/llm-providers/#quick-start","title":"Quick Start","text":""},{"location":"guides/llm-providers/#minimal-agent-configuration","title":"Minimal Agent Configuration","text":"<pre><code># agent.yaml\nname: my-agent\n\nmodel:\n  provider: openai\n  name: gpt-4o\n\ninstructions:\n  inline: \"You are a helpful assistant.\"\n</code></pre>"},{"location":"guides/llm-providers/#with-global-configuration","title":"With Global Configuration","text":"<pre><code># config.yaml\nproviders:\n  openai:\n    provider: openai\n    name: gpt-4o\n    api_key: ${OPENAI_API_KEY}\n</code></pre> <pre><code># agent.yaml\nname: my-agent\n\nmodel:\n  provider: openai\n  # Inherits name, api_key from config.yaml\n\ninstructions:\n  inline: \"You are a helpful assistant.\"\n</code></pre>"},{"location":"guides/llm-providers/#configuration-fields","title":"Configuration Fields","text":"<p>All providers share these common fields:</p> Field Type Required Default Description <code>provider</code> string Yes - Provider identifier <code>name</code> string Yes - Model name/identifier <code>temperature</code> float No 0.3 Randomness (0.0-2.0) <code>max_tokens</code> integer No 1000 Maximum response tokens <code>top_p</code> float No - Nucleus sampling (0.0-1.0) <code>api_key</code> string No - API authentication key <code>endpoint</code> string Varies - API endpoint URL"},{"location":"guides/llm-providers/#temperature","title":"Temperature","text":"<p>Controls response randomness:</p> <ul> <li>0.0: Deterministic, focused responses</li> <li>0.3: Default, balanced</li> <li>0.7: More creative</li> <li>1.0+: Highly creative/random</li> </ul> <pre><code>model:\n  temperature: 0.5  # Moderately creative\n</code></pre>"},{"location":"guides/llm-providers/#max-tokens","title":"Max Tokens","text":"<p>Limits response length. Set based on your use case:</p> <pre><code>model:\n  max_tokens: 2000  # Allow longer responses\n</code></pre>"},{"location":"guides/llm-providers/#top-p-nucleus-sampling","title":"Top P (Nucleus Sampling)","text":"<p>Alternative to temperature for controlling randomness. While both can be used simultaneously, it's recommended to adjust one or the other for more predictable results:</p> <pre><code>model:\n  top_p: 0.9  # Consider top 90% probability tokens\n</code></pre>"},{"location":"guides/llm-providers/#openai","title":"OpenAI","text":"<p>OpenAI provides GPT-4o, GPT-4o-mini, and other models through their API.</p>"},{"location":"guides/llm-providers/#prerequisites","title":"Prerequisites","text":"<ol> <li>Create an account at platform.openai.com</li> <li>Generate an API key in the API Keys section</li> <li>Set up billing in your account</li> </ol>"},{"location":"guides/llm-providers/#configuration","title":"Configuration","text":"<p>Global Configuration (Recommended):</p> <pre><code># config.yaml\nproviders:\n  openai:\n    provider: openai\n    name: gpt-4o\n    temperature: 0.3\n    max_tokens: 2000\n    api_key: ${OPENAI_API_KEY}\n</code></pre> <p>Agent Configuration:</p> <pre><code># agent.yaml\nname: my-agent\n\nmodel:\n  provider: openai\n  name: gpt-4o\n  temperature: 0.7\n  max_tokens: 4000\n\ninstructions:\n  inline: \"You are a helpful assistant.\"\n</code></pre>"},{"location":"guides/llm-providers/#environment-variables","title":"Environment Variables","text":"<pre><code># .env\nOPENAI_API_KEY=sk-...\n</code></pre>"},{"location":"guides/llm-providers/#available-models","title":"Available Models","text":"Model Description Context Window <code>gpt-4o</code> Most capable, multimodal 128K tokens <code>gpt-4o-mini</code> Fast and cost-effective 128K tokens <code>gpt-4-turbo</code> Previous generation flagship 128K tokens <code>gpt-3.5-turbo</code> Fast, lower cost 16K tokens"},{"location":"guides/llm-providers/#complete-example","title":"Complete Example","text":"<pre><code># config.yaml\nproviders:\n  openai:\n    provider: openai\n    name: gpt-4o\n    api_key: ${OPENAI_API_KEY}\n\n  openai-fast:\n    provider: openai\n    name: gpt-4o-mini\n    api_key: ${OPENAI_API_KEY}\n</code></pre> <pre><code># agent.yaml\nname: support-agent\ndescription: Customer support with GPT-4o\n\nmodel:\n  provider: openai\n  name: gpt-4o\n  temperature: 0.5\n  max_tokens: 2000\n\ninstructions:\n  inline: |\n    You are a customer support specialist.\n    Be helpful, accurate, and professional.\n</code></pre>"},{"location":"guides/llm-providers/#azure-openai","title":"Azure OpenAI","text":"<p>Azure OpenAI Service provides OpenAI models through Microsoft Azure with enterprise features.</p>"},{"location":"guides/llm-providers/#prerequisites_1","title":"Prerequisites","text":"<ol> <li>Azure subscription with Azure OpenAI access</li> <li>Create an Azure OpenAI resource in the Azure Portal</li> <li>Deploy a model in Azure OpenAI Studio</li> <li>Note your endpoint URL and API key</li> </ol>"},{"location":"guides/llm-providers/#configuration_1","title":"Configuration","text":"<p>Azure OpenAI requires both an <code>endpoint</code> and <code>api_key</code>:</p> <p>Global Configuration (Recommended):</p> <pre><code># config.yaml\nproviders:\n  azure_openai:\n    provider: azure_openai\n    name: gpt-4o\n    endpoint: ${AZURE_OPENAI_ENDPOINT}\n    api_key: ${AZURE_OPENAI_API_KEY}\n    temperature: 0.3\n    max_tokens: 2000\n</code></pre> <p>Agent Configuration:</p> <pre><code># agent.yaml\nname: enterprise-agent\n\nmodel:\n  provider: azure_openai\n  name: gpt-4o  # Must match your Azure deployment name\n  endpoint: https://my-resource.openai.azure.com/\n  temperature: 0.5\n\ninstructions:\n  inline: \"You are an enterprise assistant.\"\n</code></pre>"},{"location":"guides/llm-providers/#environment-variables_1","title":"Environment Variables","text":"<pre><code># .env\nAZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/\nAZURE_OPENAI_API_KEY=your-api-key-here\n</code></pre>"},{"location":"guides/llm-providers/#endpoint-format","title":"Endpoint Format","text":"<p>The endpoint URL follows this pattern:</p> <pre><code>https://{resource-name}.openai.azure.com/\n</code></pre> <p>Find your endpoint in:</p> <ol> <li>Azure Portal &gt; Your OpenAI Resource &gt; Keys and Endpoint</li> <li>Azure OpenAI Studio &gt; Deployments &gt; Your Deployment</li> </ol>"},{"location":"guides/llm-providers/#understanding-azure-deployment-names","title":"Understanding Azure Deployment Names","text":"<p>Important: In Azure OpenAI, the <code>name</code> field refers to your deployment name, not the base model name. This is different from OpenAI's API.</p> <p>When you deploy a model in Azure OpenAI Studio, you create a deployment with a custom name:</p> <ol> <li>Base Model: The underlying model (e.g., <code>gpt-4o</code>, <code>gpt-4o-mini</code>)</li> <li>Deployment Name: Your custom identifier (e.g., <code>my-gpt4o</code>, <code>prod-gpt4</code>)</li> </ol> <p>The <code>name</code> field in HoloDeck must match your deployment name:</p> <pre><code># If your Azure deployment is named \"my-gpt4o-production\"\n# backed by the gpt-4o base model:\n\nmodel:\n  provider: azure_openai\n  name: my-gpt4o-production  # Must match deployment name exactly\n  endpoint: https://my-resource.openai.azure.com/\n</code></pre> <p>Common Mistake:</p> <pre><code># WRONG - Using base model name\nmodel:\n  provider: azure_openai\n  name: gpt-4o  # This won't work unless your deployment is literally named \"gpt-4o\"\n\n# CORRECT - Using your deployment name\nmodel:\n  provider: azure_openai\n  name: my-gpt4o-deployment  # Your actual deployment name\n</code></pre> <p>Tip for OpenAI Users: If you're transitioning from OpenAI to Azure, remember that Azure adds this extra layer of indirection. Your deployment name can be anything, but it must be specified exactly in the configuration.</p>"},{"location":"guides/llm-providers/#available-models_1","title":"Available Models","text":"<p>Azure OpenAI offers the same models as OpenAI, deployed to your resource:</p> Model Azure Deployment Description GPT-4o Deploy in Azure Most capable GPT-4o-mini Deploy in Azure Cost-effective GPT-4 Deploy in Azure Previous flagship GPT-3.5-Turbo Deploy in Azure Fast, lower cost"},{"location":"guides/llm-providers/#complete-example_1","title":"Complete Example","text":"<pre><code># config.yaml\nproviders:\n  azure_openai:\n    provider: azure_openai\n    name: gpt-4o-deployment\n    endpoint: ${AZURE_OPENAI_ENDPOINT}\n    api_key: ${AZURE_OPENAI_API_KEY}\n    temperature: 0.3\n    max_tokens: 2000\n</code></pre> <pre><code># agent.yaml\nname: enterprise-support\ndescription: Enterprise support agent on Azure\n\nmodel:\n  provider: azure_openai\n  name: gpt-4o-deployment\n  temperature: 0.5\n  max_tokens: 4000\n\ninstructions:\n  file: prompts/enterprise-support.txt\n\nevaluations:\n  model:\n    provider: azure_openai\n    name: gpt-4o-deployment\n  metrics:\n    - metric: f1_score\n      threshold: 0.8\n</code></pre>"},{"location":"guides/llm-providers/#anthropic","title":"Anthropic","text":"<p>Anthropic provides the Claude family of models known for safety and helpfulness.</p>"},{"location":"guides/llm-providers/#prerequisites_2","title":"Prerequisites","text":"<ol> <li>Create an account at console.anthropic.com</li> <li>Generate an API key in the Console</li> <li>Set up billing</li> </ol>"},{"location":"guides/llm-providers/#configuration_2","title":"Configuration","text":"<p>Global Configuration (Recommended):</p> <pre><code># config.yaml\nproviders:\n  anthropic:\n    provider: anthropic\n    name: claude-sonnet-4-20250514\n    temperature: 0.3\n    max_tokens: 4000\n    api_key: ${ANTHROPIC_API_KEY}\n</code></pre> <p>Agent Configuration:</p> <pre><code># agent.yaml\nname: claude-agent\n\nmodel:\n  provider: anthropic\n  name: claude-sonnet-4-20250514\n  temperature: 0.5\n  max_tokens: 4000\n\ninstructions:\n  inline: \"You are Claude, a helpful AI assistant.\"\n</code></pre>"},{"location":"guides/llm-providers/#environment-variables_2","title":"Environment Variables","text":"<pre><code># .env\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre>"},{"location":"guides/llm-providers/#available-models_2","title":"Available Models","text":"Model Description Context Window <code>claude-sonnet-4-20250514</code> Best balance of speed and capability 200K tokens <code>claude-opus-4-20250514</code> Most capable, best for complex tasks 200K tokens <code>claude-3-5-sonnet-20241022</code> Previous generation Sonnet 200K tokens <code>claude-3-5-haiku-20241022</code> Fast and cost-effective 200K tokens <p>Note: Model identifiers include version dates (e.g., <code>20250514</code>). Check Anthropic's documentation for the latest available models and their capabilities.</p>"},{"location":"guides/llm-providers/#complete-example_2","title":"Complete Example","text":"<pre><code># config.yaml\nproviders:\n  anthropic:\n    provider: anthropic\n    name: claude-sonnet-4-20250514\n    api_key: ${ANTHROPIC_API_KEY}\n\n  anthropic-fast:\n    provider: anthropic\n    name: claude-3-5-haiku-20241022\n    api_key: ${ANTHROPIC_API_KEY}\n</code></pre> <pre><code># agent.yaml\nname: research-assistant\ndescription: Research assistant powered by Claude\n\nmodel:\n  provider: anthropic\n  name: claude-sonnet-4-20250514\n  temperature: 0.3\n  max_tokens: 8000\n\ninstructions:\n  inline: |\n    You are a research assistant.\n    Provide thorough, well-sourced answers.\n    Be accurate and cite relevant information.\n</code></pre>"},{"location":"guides/llm-providers/#ollama","title":"Ollama","text":"<p>Ollama enables running open-source LLMs locally on your machine. This is ideal for privacy-sensitive applications, offline deployments, and avoiding API costs.</p>"},{"location":"guides/llm-providers/#benefits-of-ollama","title":"Benefits of Ollama","text":"<ul> <li>Privacy: Data never leaves your machine</li> <li>No API Costs: Run unlimited queries without usage fees</li> <li>Offline Support: Works without internet connection</li> <li>Open-Source Models: Access to Llama, Mistral, CodeLlama, and more</li> </ul>"},{"location":"guides/llm-providers/#prerequisites_3","title":"Prerequisites","text":"<ol> <li>Install Ollama from ollama.com</li> </ol> <p>macOS/Linux: <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre></p> <p>Windows:    Download from ollama.com/download</p> <ol> <li> <p>Pull a model:    <pre><code>ollama pull llama3.2\n</code></pre></p> </li> <li> <p>Verify Ollama is running:    <pre><code>ollama list\n</code></pre></p> </li> </ol>"},{"location":"guides/llm-providers/#configuration_3","title":"Configuration","text":"<p>Ollama requires an <code>endpoint</code> pointing to your local Ollama server:</p> <p>Global Configuration (Recommended):</p> <pre><code># config.yaml\nproviders:\n  ollama:\n    provider: ollama\n    name: llama3.2\n    endpoint: http://localhost:11434\n    temperature: 0.7\n    max_tokens: 2000\n</code></pre> <p>Agent Configuration:</p> <pre><code># agent.yaml\nname: local-agent\n\nmodel:\n  provider: ollama\n  name: llama3.2\n  endpoint: http://localhost:11434\n  temperature: 0.5\n\ninstructions:\n  inline: \"You are a helpful local assistant.\"\n</code></pre>"},{"location":"guides/llm-providers/#environment-variables_3","title":"Environment Variables","text":"<pre><code># .env (optional - for custom endpoint)\nOLLAMA_ENDPOINT=http://localhost:11434\n</code></pre>"},{"location":"guides/llm-providers/#available-models_3","title":"Available Models","text":"<p>Pull models with <code>ollama pull &lt;model-name&gt;</code>:</p> Model Command Description Size GPT-OSS (20B) <code>ollama pull gpt-oss:20b</code> Recommended completion model 40GB Nomic Embed Text <code>ollama pull nomic-embed-text:latest</code> Recommended embedding model 274MB Llama 3.2 <code>ollama pull llama3.2</code> Meta's latest, general purpose 2GB Llama 3.2 (3B) <code>ollama pull llama3.2:3b</code> Larger Llama variant 5GB Mistral <code>ollama pull mistral</code> Fast and capable 4GB CodeLlama <code>ollama pull codellama</code> Optimized for code 4GB Phi-3 <code>ollama pull phi3</code> Microsoft's compact model 2GB Gemma 2 <code>ollama pull gemma2</code> Google's open model 5GB <p>Tip: Run <code>ollama list</code> to see your installed models.</p>"},{"location":"guides/llm-providers/#running-ollama-as-a-service","title":"Running Ollama as a Service","text":"<p>For production use, run Ollama as a background service:</p> <p>Start Ollama server: <pre><code>ollama serve\n</code></pre></p> <p>Or with Docker: <pre><code>docker run -d \\\n  --name ollama \\\n  -p 11434:11434 \\\n  -v ollama-data:/root/.ollama \\\n  ollama/ollama\n</code></pre></p>"},{"location":"guides/llm-providers/#context-size-configuration","title":"Context Size Configuration","text":"<p>For agent workloads, we recommend configuring a context size of at least 16k tokens. By default, Ollama models may use smaller context windows which can limit agent capabilities.</p> <p>Create a custom model with extended context:</p> <pre><code># Create a Modelfile with extended context\ncat &lt;&lt;EOF &gt; Modelfile\nFROM gpt-oss:20b\nPARAMETER num_ctx 16384\nEOF\n\n# Create the custom model\nollama create gpt-oss:20b-16k -f Modelfile\n</code></pre> <p>For larger context needs (32k):</p> <pre><code>cat &lt;&lt;EOF &gt; Modelfile\nFROM gpt-oss:20b\nPARAMETER num_ctx 32768\nEOF\n\nollama create gpt-oss:20b-32k -f Modelfile\n</code></pre> <p>Use the custom model in your configuration:</p> <pre><code>model:\n  provider: ollama\n  name: gpt-oss:20b-16k  # or gpt-oss:20b-32k for larger context\n  endpoint: http://localhost:11434\n</code></pre> <p>Note: Larger context sizes require more memory. A 32k context with a 20B parameter model may require 48GB+ RAM or a GPU with 16GB+ VRAM.</p>"},{"location":"guides/llm-providers/#complete-example_3","title":"Complete Example","text":"<pre><code># config.yaml\nproviders:\n  ollama:\n    provider: ollama\n    name: llama3.2\n    endpoint: ${OLLAMA_ENDPOINT}\n    temperature: 0.7\n\n  ollama-code:\n    provider: ollama\n    name: codellama\n    endpoint: ${OLLAMA_ENDPOINT}\n    temperature: 0.2\n</code></pre> <pre><code># agent.yaml\nname: local-assistant\ndescription: Privacy-focused local assistant\n\nmodel:\n  provider: ollama\n  name: llama3.2\n  temperature: 0.5\n  max_tokens: 4000\n\ninstructions:\n  inline: |\n    You are a helpful assistant running locally.\n    All data stays on this machine for privacy.\n</code></pre>"},{"location":"guides/llm-providers/#troubleshooting-ollama","title":"Troubleshooting Ollama","text":"<p>Error: <code>endpoint is required for ollama provider</code></p> <p>Solution: Always include the endpoint: <pre><code>model:\n  provider: ollama\n  name: llama3.2\n  endpoint: http://localhost:11434\n</code></pre></p> <p>Error: <code>Connection refused</code></p> <p>Solutions: 1. Verify Ollama is running: <code>ollama list</code> 2. Start the server: <code>ollama serve</code> 3. Check the endpoint URL matches your setup</p> <p>Error: <code>Model not found</code></p> <p>Solution: Pull the model first: <pre><code>ollama pull llama3.2\n</code></pre></p>"},{"location":"guides/llm-providers/#multi-provider-setup","title":"Multi-Provider Setup","text":"<p>Configure multiple providers to use different models for different purposes:</p> <pre><code># config.yaml\nproviders:\n  # Primary provider for agents\n  openai:\n    provider: openai\n    name: gpt-4o\n    api_key: ${OPENAI_API_KEY}\n    temperature: 0.3\n\n  # Fast provider for evaluations\n  openai-fast:\n    provider: openai\n    name: gpt-4o-mini\n    api_key: ${OPENAI_API_KEY}\n    temperature: 0.0\n\n  # Enterprise provider\n  azure:\n    provider: azure_openai\n    name: gpt-4o-deployment\n    endpoint: ${AZURE_OPENAI_ENDPOINT}\n    api_key: ${AZURE_OPENAI_API_KEY}\n\n  # Alternative provider\n  anthropic:\n    provider: anthropic\n    name: claude-sonnet-4-20250514\n    api_key: ${ANTHROPIC_API_KEY}\n\n  # Local provider (no API costs, privacy-focused)\n  ollama:\n    provider: ollama\n    name: llama3.2\n    endpoint: ${OLLAMA_ENDPOINT}\n</code></pre> <p>Use different providers in your agent:</p> <pre><code># agent.yaml\nname: multi-model-agent\n\nmodel:\n  provider: openai\n  name: gpt-4o\n\nevaluations:\n  model:\n    provider: openai\n    name: gpt-4o-mini  # Use faster model for evaluations\n  metrics:\n    - metric: f1_score\n      threshold: 0.8\n</code></pre>"},{"location":"guides/llm-providers/#security-best-practices","title":"Security Best Practices","text":""},{"location":"guides/llm-providers/#never-commit-api-keys","title":"Never Commit API Keys","text":"<pre><code># WRONG - Never do this\nproviders:\n  openai:\n    api_key: sk-abc123...  # Exposed secret!\n\n# CORRECT - Use environment variables\nproviders:\n  openai:\n    api_key: ${OPENAI_API_KEY}\n</code></pre>"},{"location":"guides/llm-providers/#use-env-files","title":"Use .env Files","text":"<p>Create a <code>.env</code> file (add to <code>.gitignore</code>):</p> <pre><code># .env - DO NOT COMMIT\nOPENAI_API_KEY=sk-...\nAZURE_OPENAI_ENDPOINT=https://...\nAZURE_OPENAI_API_KEY=...\nANTHROPIC_API_KEY=sk-ant-...\nOLLAMA_ENDPOINT=http://localhost:11434\n</code></pre>"},{"location":"guides/llm-providers/#create-example-files","title":"Create Example Files","text":"<p>Commit a template for other developers:</p> <pre><code># .env.example - Safe to commit\nOPENAI_API_KEY=your-openai-api-key-here\nAZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/\nAZURE_OPENAI_API_KEY=your-azure-api-key-here\nANTHROPIC_API_KEY=your-anthropic-api-key-here\nOLLAMA_ENDPOINT=http://localhost:11434\n</code></pre>"},{"location":"guides/llm-providers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/llm-providers/#invalid-api-key","title":"Invalid API Key","text":"<p>Error: <code>AuthenticationError</code> or <code>Invalid API key</code></p> <p>Solutions:</p> <ol> <li>Verify your API key is correct</li> <li>Check environment variable is set: <code>echo $OPENAI_API_KEY</code></li> <li>Ensure no extra whitespace in the key</li> <li>Regenerate the API key if needed</li> </ol>"},{"location":"guides/llm-providers/#azure-endpoint-issues","title":"Azure Endpoint Issues","text":"<p>Error: <code>endpoint is required for azure_openai provider</code></p> <p>Solution: Include the endpoint in your configuration:</p> <pre><code>model:\n  provider: azure_openai\n  name: my-deployment\n  endpoint: https://my-resource.openai.azure.com/\n</code></pre>"},{"location":"guides/llm-providers/#model-not-found","title":"Model Not Found","text":"<p>Error: <code>Model not found</code> or <code>Deployment not found</code></p> <p>Solutions:</p> <ul> <li>OpenAI: Check the model name is valid (e.g., <code>gpt-4o</code>, not <code>gpt4o</code>)</li> <li>Azure: Ensure <code>name</code> matches your deployment name exactly</li> <li>Anthropic: Use full model identifier (e.g., <code>claude-sonnet-4-20250514</code>)</li> </ul>"},{"location":"guides/llm-providers/#rate-limits","title":"Rate Limits","text":"<p>Error: <code>Rate limit exceeded</code></p> <p>Solutions:</p> <ol> <li>Implement retry logic with exponential backoff</li> <li>Reduce <code>max_tokens</code> to use fewer tokens</li> <li>Use a faster/cheaper model for testing</li> <li>Upgrade your API plan</li> </ol>"},{"location":"guides/llm-providers/#temperature-out-of-range","title":"Temperature Out of Range","text":"<p>Error: <code>temperature must be between 0.0 and 2.0</code></p> <p>Solution: Use a value between 0.0 and 2.0:</p> <pre><code>model:\n  temperature: 0.7  # Valid\n</code></pre>"},{"location":"guides/llm-providers/#environment-variable-reference","title":"Environment Variable Reference","text":"Variable Provider Description <code>OPENAI_API_KEY</code> OpenAI API authentication key <code>AZURE_OPENAI_ENDPOINT</code> Azure OpenAI Resource endpoint URL <code>AZURE_OPENAI_API_KEY</code> Azure OpenAI API authentication key <code>ANTHROPIC_API_KEY</code> Anthropic API authentication key <code>OLLAMA_ENDPOINT</code> Ollama Server endpoint (default: <code>http://localhost:11434</code>)"},{"location":"guides/llm-providers/#next-steps","title":"Next Steps","text":"<ul> <li>See Agent Configuration for complete agent setup</li> <li>See Global Configuration for shared provider settings and credentials</li> <li>See Evaluations Guide for configuring evaluation models (consider using faster models like <code>gpt-4o-mini</code> for cost-effective evaluations)</li> <li>See Tools Guide for extending agent capabilities</li> <li>See Vector Stores Guide for semantic search configuration</li> </ul>"},{"location":"guides/tools/","title":"Tools Reference Guide","text":"<p>This guide explains HoloDeck's four tool types that extend agent capabilities.</p>"},{"location":"guides/tools/#overview","title":"Overview","text":"<p>Tools are agent capabilities defined in <code>agent.yaml</code>. HoloDeck supports four tool types:</p> Tool Type Description Status Vectorstore Tools Semantic search over data \u2705 Implemented MCP Tools Model Context Protocol servers \u2705 Implemented Function Tools Custom Python functions \ud83d\udea7 Planned Prompt Tools LLM-powered semantic functions \ud83d\udea7 Planned <p>Note: Vectorstore Tools and MCP Tools are fully implemented. Function and Prompt tools are defined in the configuration schema but not yet functional.</p>"},{"location":"guides/tools/#common-tool-fields","title":"Common Tool Fields","text":"<p>All tools share these fields:</p> <pre><code>tools:\n  - name: tool-id # Required: Tool identifier (unique)\n    description: What it does # Required: Human-readable description\n    type: vectorstore|function|mcp|prompt # Required: Tool type\n</code></pre>"},{"location":"guides/tools/#name","title":"Name","text":"<ul> <li>Required: Yes</li> <li>Type: String</li> <li>Format: 1-100 characters, alphanumeric + underscores</li> <li>Uniqueness: Must be unique within agent</li> <li>Purpose: Used to reference tool in test cases, execution logs</li> </ul> <pre><code>- name: search_kb\n</code></pre>"},{"location":"guides/tools/#description","title":"Description","text":"<ul> <li>Required: Yes</li> <li>Type: String</li> <li>Max Length: 500 characters</li> <li>Purpose: Helps agent understand when to use this tool</li> </ul> <pre><code>- description: Search company knowledge base for answers\n</code></pre>"},{"location":"guides/tools/#type","title":"Type","text":"<ul> <li>Required: Yes</li> <li>Type: String (Enum)</li> <li>Options: <code>vectorstore</code>, <code>function</code>, <code>mcp</code>, <code>prompt</code></li> <li>Purpose: Determines which additional fields are required</li> </ul> <pre><code>- type: vectorstore\n</code></pre>"},{"location":"guides/tools/#vectorstore-tools","title":"Vectorstore Tools \u2705","text":"<p>Status: Fully implemented</p> <p>Semantic search over unstructured or structured data.</p>"},{"location":"guides/tools/#when-to-use","title":"When to Use","text":"<ul> <li>Searching documents, knowledge bases, FAQs</li> <li>Semantic similarity matching</li> <li>Context retrieval for RAG (Retrieval-Augmented Generation)</li> </ul>"},{"location":"guides/tools/#basic-example","title":"Basic Example","text":"<pre><code>- name: search-kb\n  description: Search knowledge base for answers\n  type: vectorstore\n  source: knowledge_base/\n</code></pre>"},{"location":"guides/tools/#supported-vector-database-providers","title":"Supported Vector Database Providers","text":"<p>HoloDeck supports multiple vector database backends through Semantic Kernel's VectorStoreCollection abstractions. You can switch providers via configuration without changing your agent code.</p> Provider Description Connection Install Command <code>postgres</code> PostgreSQL with pgvector extension <code>postgresql://user:pass@host/db</code> <code>uv add holodeck-ai[postgres]</code> <code>qdrant</code> Qdrant vector database <code>http://localhost:6333</code> <code>uv add holodeck-ai[qdrant]</code> <code>chromadb</code> ChromaDB (local or server) Local path or host URL <code>uv add holodeck-ai[chromadb]</code> <code>pinecone</code> Pinecone serverless vector database API key + index name <code>uv add holodeck-ai[pinecone]</code> <code>in-memory</code> Simple in-memory storage None required Built-in <p>Tip: Install all vector store providers at once with <code>uv add holodeck-ai[vectorstores]</code>. Use <code>in-memory</code> for development and testing without installing any dependencies. Switch to a persistent provider like <code>postgres</code>, <code>qdrant</code>, or <code>chromadb</code> for production.</p>"},{"location":"guides/tools/#database-configuration-examples","title":"Database Configuration Examples","text":"<p>PostgreSQL with pgvector</p> <pre><code>- name: search-kb\n  type: vectorstore\n  source: knowledge_base/\n  database:\n    provider: postgres\n    connection_string: postgresql://user:password@localhost:5432/mydb\n</code></pre> <p>Azure AI Search</p> <pre><code>- name: search-kb\n  type: vectorstore\n  source: knowledge_base/\n  database:\n    provider: azure-ai-search\n    connection_string: ${AZURE_SEARCH_ENDPOINT}\n    api_key: ${AZURE_SEARCH_API_KEY}\n</code></pre> <p>Qdrant</p> <pre><code>- name: search-kb\n  type: vectorstore\n  source: knowledge_base/\n  database:\n    provider: qdrant\n    url: http://localhost:6333\n    # api_key: optional-api-key\n</code></pre> <p>In-Memory (development only)</p> <pre><code>- name: search-kb\n  type: vectorstore\n  source: knowledge_base/\n  database:\n    provider: in-memory\n</code></pre> <p>Reference to Global Config</p> <p>You can also reference a named vectorstore from your global <code>config.yaml</code>:</p> <pre><code># In agent.yaml\n- name: search-kb\n  type: vectorstore\n  source: knowledge_base/\n  database: my-postgres-store # Reference to config.yaml vectorstores section\n</code></pre> <pre><code># In config.yaml\nvectorstores:\n  my-postgres-store:\n    provider: postgres\n    connection_string: ${DATABASE_URL}\n</code></pre>"},{"location":"guides/tools/#required-fields","title":"Required Fields","text":""},{"location":"guides/tools/#source","title":"Source","text":"<ul> <li>Type: String (path)</li> <li>Purpose: Data file or directory to index</li> <li>Formats Supported:</li> <li>Single files: <code>.txt</code>, <code>.md</code>, <code>.pdf</code>, <code>.json</code>, <code>.csv</code></li> <li>Directories: Recursively indexes supported formats</li> <li>Remote URLs: File auto-cached locally</li> </ul> <pre><code>source: knowledge_base/\n# OR\nsource: docs.json\n# OR\nsource: https://example.com/data.pdf\n</code></pre>"},{"location":"guides/tools/#optional-fields","title":"Optional Fields","text":""},{"location":"guides/tools/#embedding-model","title":"Embedding Model","text":"<ul> <li>Type: String</li> <li>Purpose: Which embedding model to use</li> <li>Default: Provider-specific default</li> <li>Examples: <code>text-embedding-3-small</code>, <code>text-embedding-ada-002</code></li> </ul> <pre><code>embedding_model: text-embedding-3-small\n</code></pre>"},{"location":"guides/tools/#vector-field","title":"Vector Field","text":"<ul> <li>Type: String or List of strings</li> <li>Purpose: Which field(s) to vectorize (for JSON/CSV)</li> <li>Default: Auto-detect text fields</li> <li>Note: XOR with <code>vector_fields</code> (use one or the other)</li> </ul> <pre><code>vector_field: content\n# OR\nvector_field: [title, description]\n</code></pre>"},{"location":"guides/tools/#meta-fields","title":"Meta Fields","text":"<ul> <li>Type: List of strings</li> <li>Purpose: Metadata fields to include in results</li> <li>Default: All fields included</li> </ul> <pre><code>meta_fields: [title, source, date]\n</code></pre>"},{"location":"guides/tools/#chunk-size","title":"Chunk Size","text":"<ul> <li>Type: Integer</li> <li>Purpose: Characters per chunk for text splitting</li> <li>Default: 512</li> <li>Constraint: Must be &gt; 0</li> </ul> <pre><code>chunk_size: 1024\n</code></pre>"},{"location":"guides/tools/#chunk-overlap","title":"Chunk Overlap","text":"<ul> <li>Type: Integer</li> <li>Purpose: Characters to overlap between chunks</li> <li>Default: 0</li> <li>Constraint: Must be &gt;= 0</li> </ul> <pre><code>chunk_overlap: 100\n</code></pre>"},{"location":"guides/tools/#record-path","title":"Record Path","text":"<ul> <li>Type: String</li> <li>Purpose: Path to array in nested JSON (dot notation)</li> <li>Example: For <code>{data: {items: [{...}]}}</code>, use <code>data.items</code></li> </ul> <pre><code>record_path: data.records\n</code></pre>"},{"location":"guides/tools/#record-prefix","title":"Record Prefix","text":"<ul> <li>Type: String</li> <li>Purpose: Prefix added to record fields</li> <li>Default: None</li> </ul> <pre><code>record_prefix: record_\n</code></pre>"},{"location":"guides/tools/#meta-prefix","title":"Meta Prefix","text":"<ul> <li>Type: String</li> <li>Purpose: Prefix added to metadata fields</li> <li>Default: None</li> </ul> <pre><code>meta_prefix: meta_\n</code></pre>"},{"location":"guides/tools/#complete-example","title":"Complete Example","text":"<pre><code>- name: search-docs\n  description: Search technical documentation\n  type: vectorstore\n  source: docs/\n  embedding_model: text-embedding-3-small\n  vector_field: [title, content]\n  meta_fields: [source, date, url]\n  chunk_size: 1024\n  chunk_overlap: 128\n</code></pre>"},{"location":"guides/tools/#data-format-examples","title":"Data Format Examples","text":"<p>Text Files (<code>.txt</code>, <code>.md</code>)</p> <pre><code># Document Title\n\nThis is the document content that will be\nvectorized for semantic search.\n</code></pre> <p>JSON (Array of objects)</p> <pre><code>[\n  {\n    \"title\": \"Getting Started\",\n    \"content\": \"How to get started with the platform...\",\n    \"source\": \"docs/intro.md\"\n  }\n]\n</code></pre> <p>JSON (Nested structure)</p> <pre><code>{\n  \"data\": {\n    \"records\": [\n      {\n        \"id\": 1,\n        \"title\": \"Article 1\",\n        \"content\": \"...\"\n      }\n    ]\n  }\n}\n</code></pre> <p>Use <code>record_path: data.records</code> to access records.</p> <p>CSV</p> <pre><code>title,content,source\n\"Getting Started\",\"How to get started...\",\"docs/intro\"\n\"API Reference\",\"API documentation...\",\"docs/api\"\n</code></pre>"},{"location":"guides/tools/#mcp-tools","title":"MCP Tools \u2705","text":"<p>Status: Fully implemented (stdio transport)</p> <p>Model Context Protocol (MCP) server integrations enable agents to interact with external systems through a standardized protocol. HoloDeck uses Semantic Kernel's MCP plugins for seamless integration.</p> <p>Finding MCP Servers: Browse the official MCP server registry at github.com/modelcontextprotocol/servers for a curated list of available servers including filesystem, GitHub, Slack, Google Drive, PostgreSQL, and many more community-contributed integrations.</p>"},{"location":"guides/tools/#when-to-use_1","title":"When to Use","text":"<ul> <li>File system operations (read, write, list files)</li> <li>GitHub/GitLab operations (issues, PRs, code)</li> <li>Database access (SQLite, PostgreSQL)</li> <li>Web browsing and search</li> <li>Any standardized MCP server</li> </ul>"},{"location":"guides/tools/#basic-example_1","title":"Basic Example","text":"<pre><code>- name: filesystem\n  description: Read and write files in the workspace\n  type: mcp\n  command: npx\n  args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"./data\"]\n</code></pre>"},{"location":"guides/tools/#complete-example_1","title":"Complete Example","text":"<pre><code>tools:\n  # MCP filesystem tool for reading/writing files\n  - type: mcp\n    name: filesystem\n    description: Read and write files in the workspace data directory\n    command: npx\n    args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"./sample/data\"]\n    config:\n      allowed_directories: [\"./sample/data\"]\n    request_timeout: 30\n</code></pre>"},{"location":"guides/tools/#required-fields_1","title":"Required Fields","text":""},{"location":"guides/tools/#command","title":"Command","text":"<ul> <li>Type: String (enum: <code>npx</code>, <code>node</code>, <code>uvx</code>, <code>docker</code>)</li> <li>Purpose: How to launch the MCP server</li> <li>Required: Yes (for stdio transport)</li> </ul> <pre><code>command: npx     # For npm packages (auto-installs if needed)\n# OR\ncommand: node    # For local .js files or installed packages\n# OR\ncommand: uvx     # For Python packages via uv\n# OR\ncommand: docker  # For containerized servers\n</code></pre> <p>When to use each:</p> <ul> <li><code>npx</code> - Run npm packages directly (e.g., <code>@modelcontextprotocol/server-filesystem</code>)</li> <li><code>node</code> - Run local JavaScript files (e.g., <code>./tools/my-server.js</code>)</li> <li><code>uvx</code> - Run Python packages via uv (e.g., <code>mcp-server-fetch</code>)</li> <li><code>docker</code> - Run containerized MCP servers</li> </ul>"},{"location":"guides/tools/#args","title":"Args","text":"<ul> <li>Type: List of strings</li> <li>Purpose: Command-line arguments for the server</li> <li>Note: Often includes the server package name and configuration</li> </ul> <pre><code>args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"./data\"]\n</code></pre>"},{"location":"guides/tools/#optional-fields_1","title":"Optional Fields","text":""},{"location":"guides/tools/#transport","title":"Transport","text":"<ul> <li>Type: String (enum: <code>stdio</code>, <code>sse</code>, <code>websocket</code>, <code>http</code>)</li> <li>Default: <code>stdio</code></li> <li>Purpose: Communication protocol with the server</li> <li>Note: Currently only <code>stdio</code> is implemented</li> </ul> <pre><code>transport: stdio # Default, works with most servers\n</code></pre>"},{"location":"guides/tools/#config","title":"Config","text":"<ul> <li>Type: Object (free-form)</li> <li>Purpose: Server-specific configuration passed via MCP_CONFIG env var</li> <li>Validation: Server validates at runtime</li> </ul> <pre><code>config:\n  allowed_directories: [\"./data\", \"/tmp\"]\n  max_file_size: 1048576\n</code></pre>"},{"location":"guides/tools/#env","title":"Env","text":"<ul> <li>Type: Object (string key-value pairs)</li> <li>Purpose: Environment variables for the server process</li> <li>Supports: Variable substitution with <code>${VAR_NAME}</code></li> </ul> <pre><code>env:\n  GITHUB_TOKEN: \"${GITHUB_TOKEN}\"\n  API_KEY: \"static-value\"\n</code></pre>"},{"location":"guides/tools/#env-file","title":"Env File","text":"<ul> <li>Type: String (path)</li> <li>Purpose: Load environment variables from a file</li> <li>Format: Standard <code>.env</code> file format</li> </ul> <pre><code>env_file: .env.mcp\n</code></pre>"},{"location":"guides/tools/#request-timeout","title":"Request Timeout","text":"<ul> <li>Type: Integer (seconds)</li> <li>Default: 30</li> <li>Purpose: Timeout for individual MCP requests</li> </ul> <pre><code>request_timeout: 60\n</code></pre>"},{"location":"guides/tools/#encoding","title":"Encoding","text":"<ul> <li>Type: String</li> <li>Default: <code>utf-8</code></li> <li>Purpose: Character encoding for stdio communication</li> </ul> <pre><code>encoding: utf-8\n</code></pre>"},{"location":"guides/tools/#sample-mcp-servers","title":"Sample MCP Servers","text":""},{"location":"guides/tools/#filesystem-stdio","title":"Filesystem (stdio)","text":"<p>Read, write, and manage files:</p> <pre><code>- name: filesystem\n  type: mcp\n  description: File system operations\n  command: npx\n  args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"./data\"]\n  config:\n    allowed_directories: [\"./data\"]\n</code></pre> <p>Tools provided: <code>read_file</code>, <code>write_file</code>, <code>list_directory</code>, <code>create_directory</code>, <code>move_file</code>, <code>search_files</code>, <code>get_file_info</code></p>"},{"location":"guides/tools/#github","title":"GitHub","text":"<p>Interact with GitHub repositories:</p> <pre><code>- name: github\n  type: mcp\n  description: GitHub repository operations\n  command: npx\n  args: [\"-y\", \"@modelcontextprotocol/server-github\"]\n  env:\n    GITHUB_PERSONAL_ACCESS_TOKEN: \"${GITHUB_TOKEN}\"\n</code></pre> <p>Tools provided: <code>search_repositories</code>, <code>create_issue</code>, <code>list_issues</code>, <code>get_file_contents</code>, <code>create_pull_request</code>, <code>fork_repository</code></p>"},{"location":"guides/tools/#sqlite","title":"SQLite","text":"<p>Query SQLite databases:</p> <pre><code>- name: sqlite\n  type: mcp\n  description: SQLite database queries\n  command: npx\n  args:\n    [\n      \"-y\",\n      \"@modelcontextprotocol/server-sqlite\",\n      \"--db-path\",\n      \"./data/database.db\",\n    ]\n</code></pre> <p>Tools provided: <code>read_query</code>, <code>write_query</code>, <code>create_table</code>, <code>list_tables</code>, <code>describe_table</code></p>"},{"location":"guides/tools/#brave-search","title":"Brave Search","text":"<p>Web search capabilities:</p> <pre><code>- name: brave-search\n  type: mcp\n  description: Web search via Brave\n  command: npx\n  args: [\"-y\", \"@modelcontextprotocol/server-brave-search\"]\n  env:\n    BRAVE_API_KEY: \"${BRAVE_API_KEY}\"\n</code></pre> <p>Tools provided: <code>brave_web_search</code>, <code>brave_local_search</code></p>"},{"location":"guides/tools/#puppeteer-browser-automation","title":"Puppeteer (Browser Automation)","text":"<p>Browser automation and web scraping:</p> <pre><code>- name: puppeteer\n  type: mcp\n  description: Browser automation\n  command: npx\n  args: [\"-y\", \"@modelcontextprotocol/server-puppeteer\"]\n</code></pre> <p>Tools provided: <code>puppeteer_navigate</code>, <code>puppeteer_screenshot</code>, <code>puppeteer_click</code>, <code>puppeteer_fill</code>, <code>puppeteer_evaluate</code></p>"},{"location":"guides/tools/#local-nodejs-servers-node","title":"Local Node.js Servers (node)","text":"<p>For local JavaScript MCP server files, use <code>node</code>:</p> <pre><code>- name: my-custom-server\n  type: mcp\n  description: Custom local MCP server\n  command: node\n  args: [\"./tools/my-mcp-server.js\", \"--config\", \"./config.json\"]\n</code></pre> <p>Note: Use <code>node</code> for local <code>.js</code> files. Use <code>npx</code> for npm packages.</p>"},{"location":"guides/tools/#python-mcp-servers-uvx","title":"Python MCP Servers (uvx)","text":"<p>For Python-based MCP servers, use <code>uvx</code>:</p> <pre><code>- name: mcp-server-fetch\n  type: mcp\n  description: Fetch web content\n  command: uvx\n  args: [\"mcp-server-fetch\"]\n</code></pre>"},{"location":"guides/tools/#memory-short-term-storage","title":"Memory (Short-Term Storage)","text":"<p>Scratchpad for agent short-term memory storage:</p> <pre><code>- name: memory\n  type: mcp\n  description: Scratchpad for short term memory storage\n  command: uvx\n  args: [\"basic-memory\", \"mcp\"]\n  request_timeout: 30\n</code></pre> <p>Tools provided: <code>write_note</code>, <code>read_note</code>, <code>search_notes</code>, <code>delete_note</code></p> <p>Use case: Enable agents to persist information across conversation turns, store intermediate results, or maintain context during multi-step tasks, and especially between chat sessions.</p>"},{"location":"guides/tools/#docker-mcp-servers","title":"Docker MCP Servers","text":"<p>For containerized servers:</p> <pre><code>- name: custom-server\n  type: mcp\n  description: Custom containerized server\n  command: docker\n  args: [\"run\", \"-i\", \"--rm\", \"my-mcp-server:latest\"]\n</code></pre>"},{"location":"guides/tools/#environment-variable-patterns","title":"Environment Variable Patterns","text":"<p>Static values:</p> <pre><code>env:\n  API_KEY: \"sk-1234567890\"\n</code></pre> <p>Environment substitution:</p> <pre><code>env:\n  GITHUB_TOKEN: \"${GITHUB_TOKEN}\" # From process environment\n</code></pre> <p>From env file:</p> <pre><code>env_file: .env.mcp\nenv:\n  OVERRIDE_VAR: \"override-value\" # Overrides env_file\n</code></pre>"},{"location":"guides/tools/#error-handling","title":"Error Handling","text":"<ul> <li>Server unavailable: Error during agent startup</li> <li>Connection timeout: Configurable via <code>request_timeout</code></li> <li>Invalid config: Error during agent startup (validation)</li> <li>Runtime errors: Logged and returned as tool error responses</li> </ul>"},{"location":"guides/tools/#prerequisites","title":"Prerequisites","text":"<p>MCP tools require the appropriate runtime to be installed on your machine based on the <code>command</code> you use:</p> Command Required Software Installation <code>npx</code> Node.js + npm nodejs.org or <code>brew install node</code> <code>node</code> Node.js nodejs.org or <code>brew install node</code> <code>uvx</code> uv (Python) <code>curl -LsSf https://astral.sh/uv/install.sh \\| sh</code> or <code>brew install uv</code> <code>docker</code> Docker docker.com or <code>brew install --cask docker</code> <p>Verify installation:</p> <pre><code># For npm-based MCP servers\nnode --version    # Should show v18+ recommended\nnpx --version\n\n# For Python-based MCP servers\nuv --version\nuvx --version\n\n# For containerized servers\ndocker --version\n</code></pre> <p>Tip: Most MCP servers use <code>npx</code> with npm packages. Ensure Node.js 18+ is installed for best compatibility.</p>"},{"location":"guides/tools/#lifecycle-management","title":"Lifecycle Management","text":"<p>MCP plugins are automatically managed:</p> <ol> <li>Startup: Plugin initialized and connected when agent starts</li> <li>Execution: Tools discovered and registered on the kernel</li> <li>Shutdown: Plugin properly closed when session ends</li> </ol> <p>Important: Always terminate chat sessions properly (<code>exit</code> or <code>quit</code>) to ensure MCP servers are cleanly shut down.</p>"},{"location":"guides/tools/#function-tools","title":"Function Tools \ud83d\udea7","text":"<p>Status: Planned - Configuration schema defined, execution not yet implemented</p> <p>Execute custom Python functions.</p>"},{"location":"guides/tools/#when-to-use_2","title":"When to Use","text":"<ul> <li>Custom business logic</li> <li>Database queries</li> <li>System operations</li> <li>Complex calculations</li> </ul>"},{"location":"guides/tools/#basic-example_2","title":"Basic Example","text":"<pre><code>- name: get-user\n  description: Look up user information\n  type: function\n  file: tools/users.py\n  function: get_user\n</code></pre>"},{"location":"guides/tools/#required-fields_2","title":"Required Fields","text":""},{"location":"guides/tools/#file","title":"File","text":"<ul> <li>Type: String (path)</li> <li>Purpose: Python file containing the function</li> <li>Path: Relative to <code>agent.yaml</code> directory</li> <li>Format: Standard Python module</li> </ul> <pre><code>file: tools/users.py\n</code></pre>"},{"location":"guides/tools/#function","title":"Function","text":"<ul> <li>Type: String</li> <li>Purpose: Function name to call</li> <li>Format: Valid Python identifier</li> </ul> <pre><code>function: get_user\n</code></pre>"},{"location":"guides/tools/#optional-fields_2","title":"Optional Fields","text":""},{"location":"guides/tools/#parameters","title":"Parameters","text":"<ul> <li>Type: Object mapping parameter names to schemas</li> <li>Purpose: Define function parameters the agent can pass</li> <li>Default: No parameters (function takes no args)</li> </ul> <pre><code>parameters:\n  user_id:\n    type: string\n    description: User identifier\n  include_details:\n    type: boolean\n    description: Include detailed information\n</code></pre> <p>Parameter schema fields:</p> <ul> <li><code>type</code>: <code>string</code>, <code>integer</code>, <code>float</code>, <code>boolean</code>, <code>array</code>, <code>object</code></li> <li><code>description</code>: What the parameter is for</li> <li><code>enum</code>: Optional list of allowed values</li> <li><code>default</code>: Optional default value</li> </ul>"},{"location":"guides/tools/#complete-example_2","title":"Complete Example","text":"<pre><code>- name: create-ticket\n  description: Create a support ticket\n  type: function\n  file: tools/support.py\n  function: create_ticket\n  parameters:\n    title:\n      type: string\n      description: Ticket title (required)\n    priority:\n      type: string\n      description: Ticket priority\n      enum: [low, medium, high]\n    description:\n      type: string\n      description: Detailed description\n</code></pre>"},{"location":"guides/tools/#python-function-format","title":"Python Function Format","text":"<pre><code># tools/support.py\n\ndef create_ticket(title: str, priority: str = \"medium\", description: str = \"\") -&gt; dict:\n    \"\"\"\n    Create a support ticket.\n\n    Args:\n        title: Ticket title\n        priority: low|medium|high\n        description: Detailed description\n\n    Returns:\n        Created ticket data\n    \"\"\"\n    return {\n        \"id\": \"TICKET-123\",\n        \"status\": \"open\",\n        \"title\": title,\n        \"priority\": priority,\n    }\n</code></pre>"},{"location":"guides/tools/#best-practices","title":"Best Practices","text":"<ul> <li>Keep functions focused on single tasks</li> <li>Use clear parameter names</li> <li>Add type hints and docstrings</li> <li>Handle errors gracefully (return error messages)</li> <li>Return JSON-serializable data</li> <li>Avoid long-running operations (prefer async tools in future versions)</li> </ul>"},{"location":"guides/tools/#prompt-tools","title":"Prompt Tools \ud83d\udea7","text":"<p>Status: Planned - Configuration schema defined, execution not yet implemented</p> <p>LLM-powered semantic functions with template substitution.</p>"},{"location":"guides/tools/#when-to-use_3","title":"When to Use","text":"<ul> <li>Text generation with templates</li> <li>Specialized prompts for specific tasks</li> <li>Reusable prompt chains</li> <li>A/B testing different prompts</li> </ul>"},{"location":"guides/tools/#basic-example_3","title":"Basic Example","text":"<pre><code>- name: summarize\n  description: Summarize text into key points\n  type: prompt\n  template: \"Summarize this in 3 bullet points: {{text}}\"\n  parameters:\n    text:\n      type: string\n      description: Text to summarize\n</code></pre>"},{"location":"guides/tools/#required-fields_3","title":"Required Fields","text":""},{"location":"guides/tools/#template-or-file","title":"Template or File","text":"<p>Either <code>template</code> (inline) or <code>file</code> (external), not both:</p> <p>Inline Template</p> <ul> <li>Type: String</li> <li>Max Length: 5000 characters</li> <li>Syntax: Mustache-style <code>{{variable}}</code></li> </ul> <pre><code>template: \"Summarize: {{content}}\"\n</code></pre> <p>Template File</p> <ul> <li>Type: String (path)</li> <li>Path: Relative to <code>agent.yaml</code></li> </ul> <pre><code>file: prompts/summarize.txt\n</code></pre> <p>File contents:</p> <pre><code>Summarize this text in 3 bullet points:\n\n{{text}}\n\nFocus on key takeaways.\n</code></pre>"},{"location":"guides/tools/#parameters_1","title":"Parameters","text":"<ul> <li>Type: Object mapping parameter names to schemas</li> <li>Purpose: Template variables the agent can fill</li> <li>Required: Yes (at least one)</li> </ul> <pre><code>parameters:\n  text:\n    type: string\n    description: Text to process\n</code></pre>"},{"location":"guides/tools/#optional-fields_3","title":"Optional Fields","text":""},{"location":"guides/tools/#model-override","title":"Model Override","text":"<ul> <li>Type: Model configuration object</li> <li>Purpose: Use different model for this tool</li> <li>Default: Uses agent's model</li> </ul> <pre><code>model:\n  provider: openai\n  name: gpt-4 # Different from agent's model\n  temperature: 0.2\n</code></pre>"},{"location":"guides/tools/#complete-example_3","title":"Complete Example","text":"<pre><code>- name: code-reviewer\n  description: Review code for best practices\n  type: prompt\n  file: prompts/code_review.txt\n  model:\n    provider: openai\n    name: gpt-4\n    temperature: 0.3\n  parameters:\n    code:\n      type: string\n      description: Code to review\n    language:\n      type: string\n      description: Programming language\n      enum: [python, javascript, go, java]\n</code></pre> <p>Template file (<code>prompts/code_review.txt</code>):</p> <pre><code>Review this {{language}} code for best practices.\n\nCode:\n{{code}}\n\nProvide:\n1. Issues found\n2. Suggestions for improvement\n3. Security considerations\n</code></pre>"},{"location":"guides/tools/#template-syntax","title":"Template Syntax","text":"<p>Variables use Mustache-style syntax:</p> <pre><code>Simple variable: {{name}}\n\nConditionals (if parameter provided):\n{{#if description}}\nDescription: {{description}}\n{{/if}}\n\nLoops (if parameter is array):\n{{#each items}}\n- {{this}}\n{{/each}}\n</code></pre>"},{"location":"guides/tools/#tool-comparison","title":"Tool Comparison","text":"Feature Vectorstore MCP Function Prompt Status \u2705 Implemented \u2705 Implemented \ud83d\udea7 Planned \ud83d\udea7 Planned Use Case Search data External integrations Custom logic Template-based Execution Vector similarity MCP protocol (stdio) Python function LLM generation Setup Data files Server config + runtime Python files Template text Parameters Implicit (search query) Server-specific tools Defined in code Defined in YAML Latency Medium (~100ms) Medium (~50-500ms) Low (&lt;10ms) High (LLM call) Cost Embedding API Server resource Internal LLM tokens"},{"location":"guides/tools/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/tools/#knowledge-base-search","title":"Knowledge Base Search","text":"<pre><code>- name: search-kb\n  type: vectorstore\n  source: kb/\n  chunk_size: 512\n  embedding_model: text-embedding-3-small\n</code></pre>"},{"location":"guides/tools/#database-query","title":"Database Query","text":"<pre><code>- name: query-db\n  type: function\n  file: tools/db.py\n  function: query\n  parameters:\n    sql:\n      type: string\n</code></pre>"},{"location":"guides/tools/#file-operations-mcp","title":"File Operations (MCP)","text":"<pre><code>- name: filesystem\n  type: mcp\n  description: Read and write files\n  command: npx\n  args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"./data\"]\n  config:\n    allowed_directories: [\"./data\"]\n</code></pre>"},{"location":"guides/tools/#github-integration-mcp","title":"GitHub Integration (MCP)","text":"<pre><code>- name: github\n  type: mcp\n  description: GitHub repository operations\n  command: npx\n  args: [\"-y\", \"@modelcontextprotocol/server-github\"]\n  env:\n    GITHUB_PERSONAL_ACCESS_TOKEN: \"${GITHUB_TOKEN}\"\n</code></pre>"},{"location":"guides/tools/#text-transformation","title":"Text Transformation","text":"<pre><code>- name: translate\n  type: prompt\n  template: \"Translate to {{language}}: {{text}}\"\n  parameters:\n    text:\n      type: string\n    language:\n      type: string\n</code></pre>"},{"location":"guides/tools/#error-handling_1","title":"Error Handling","text":""},{"location":"guides/tools/#vectorstore-tool-errors","title":"Vectorstore Tool Errors","text":"<ul> <li>No data found: Returns empty results</li> <li>Invalid path: Error during agent startup (config validation)</li> <li>Unsupported format: Error during agent startup</li> </ul>"},{"location":"guides/tools/#function-tool-errors","title":"Function Tool Errors","text":"<ul> <li>Function not found: Error during agent startup</li> <li>Runtime error: Caught and returned as error message</li> <li>Type mismatch: Type checking during agent startup</li> </ul>"},{"location":"guides/tools/#mcp-tool-errors","title":"MCP Tool Errors","text":"<ul> <li>Server unavailable: Error during agent startup (fails fast)</li> <li>Command not found: Error if runtime (npx, uvx, docker) not installed</li> <li>Connection timeout: Configurable via <code>request_timeout</code>, returns error</li> <li>Invalid config: Error during agent startup (validation)</li> <li>Runtime errors: Returned as tool error responses to the LLM</li> </ul>"},{"location":"guides/tools/#prompt-tool-errors","title":"Prompt Tool Errors","text":"<ul> <li>Invalid template: Error during agent startup</li> <li>LLM failure: Soft failure (logged, error message returned)</li> <li>Template rendering: Error during execution</li> </ul>"},{"location":"guides/tools/#performance-tips","title":"Performance Tips","text":""},{"location":"guides/tools/#vectorstore-tools_1","title":"Vectorstore Tools","text":"<ul> <li>Use appropriate chunk size (larger = fewer embeddings)</li> <li>Enable caching for remote files</li> <li>Reduce <code>vector_field</code> count if possible</li> <li>Index only necessary fields</li> </ul>"},{"location":"guides/tools/#function-tools_1","title":"Function Tools","text":"<ul> <li>Keep functions fast (&lt;1 second)</li> <li>Use connection pooling for databases</li> <li>Cache results when possible</li> </ul>"},{"location":"guides/tools/#mcp-tools_1","title":"MCP Tools","text":"<ul> <li>Use server-side filtering when available</li> <li>Limit result sets</li> <li>Cache responses locally</li> </ul>"},{"location":"guides/tools/#prompt-tools_1","title":"Prompt Tools","text":"<ul> <li>Use simpler models for repeated operations</li> <li>Batch processing when possible</li> <li>Limit template complexity</li> </ul>"},{"location":"guides/tools/#best-practices_1","title":"Best Practices","text":"<ol> <li>Clear Names: Use descriptive tool names</li> <li>Clear Descriptions: Agent uses description to decide when to call tool</li> <li>Parameters: Define expected parameters clearly</li> <li>Error Handling: Handle errors gracefully</li> <li>Performance: Test with realistic data</li> <li>Versioning: Manage tool file versions in source control</li> <li>Testing: Include test cases that exercise each tool</li> </ol>"},{"location":"guides/tools/#next-steps","title":"Next Steps","text":"<ul> <li>See Agent Configuration Guide for tool usage</li> <li>See File References Guide for path resolution</li> <li>See Examples for complete tool usage</li> </ul>"},{"location":"guides/vector-stores/","title":"Vector Stores Guide","text":"<p>This guide explains how to set up and configure vector stores for semantic search in HoloDeck agents.</p>"},{"location":"guides/vector-stores/#overview","title":"Overview","text":"<p>Vector stores enable semantic search capabilities for your agents, allowing them to search through documents, knowledge bases, and structured data using natural language queries. HoloDeck uses vector embeddings to find semantically similar content.</p>"},{"location":"guides/vector-stores/#why-use-vector-stores","title":"Why Use Vector Stores?","text":"<ul> <li>Semantic Search: Find relevant information based on meaning, not just keywords</li> <li>RAG (Retrieval-Augmented Generation): Ground agent responses in your data</li> <li>Knowledge Bases: Build searchable document repositories</li> <li>FAQ Systems: Match user questions to relevant answers</li> </ul>"},{"location":"guides/vector-stores/#installing-vector-store-providers","title":"Installing Vector Store Providers","text":"<p>HoloDeck uses optional dependencies for vector database providers. Install only what you need:</p>"},{"location":"guides/vector-stores/#individual-providers","title":"Individual Providers","text":"<pre><code># PostgreSQL with pgvector\nuv add holodeck-ai[postgres]\n\n# Qdrant\nuv add holodeck-ai[qdrant]\n\n# Pinecone\nuv add holodeck-ai[pinecone]\n\n# ChromaDB\nuv add holodeck-ai[chromadb]\n</code></pre>"},{"location":"guides/vector-stores/#all-vector-stores","title":"All Vector Stores","text":"<pre><code># Install all vector store providers at once\nuv add holodeck-ai[vectorstores]\n</code></pre>"},{"location":"guides/vector-stores/#with-pip","title":"With pip","text":"<pre><code>pip install holodeck-ai[postgres]\npip install holodeck-ai[qdrant]\npip install holodeck-ai[pinecone]\npip install holodeck-ai[chromadb]\n# Or all at once\npip install holodeck-ai[vectorstores]\n</code></pre>"},{"location":"guides/vector-stores/#prerequisites","title":"Prerequisites","text":"<p>Before setting up a vector store, you need a container runtime:</p>"},{"location":"guides/vector-stores/#docker-recommended","title":"Docker (Recommended)","text":"<p>Docker is the most common container runtime. Install it from docker.com.</p> <p>Verify installation:</p> <pre><code>docker --version\n# Docker version 24.0.0, build ...\n</code></pre>"},{"location":"guides/vector-stores/#podman-alternative","title":"Podman (Alternative)","text":"<p>Podman is a daemonless container engine, useful in environments where Docker isn't available.</p> <p>Install on Linux:</p> <pre><code># Ubuntu/Debian\nsudo apt-get install podman\n\n# Fedora/RHEL\nsudo dnf install podman\n</code></pre> <p>Install on macOS:</p> <pre><code>brew install podman\npodman machine init\npodman machine start\n</code></pre> <p>Verify installation:</p> <pre><code>podman --version\n# podman version 4.0.0\n</code></pre> <p>Note: Podman commands are compatible with Docker. Replace <code>docker</code> with <code>podman</code> in the examples below.</p>"},{"location":"guides/vector-stores/#setting-up-chromadb","title":"Setting Up ChromaDB","text":"<p>ChromaDB is an open-source embedding database that's simple to set up and ideal for development. It provides a lightweight vector database with native Python support.</p>"},{"location":"guides/vector-stores/#quick-start-with-docker","title":"Quick Start with Docker","text":"<p>Run ChromaDB:</p> <pre><code>docker run -d \\\n  --name chromadb \\\n  -p 8000:8000 \\\n  -v ./chroma-data:/chroma/chroma \\\n  -e IS_PERSISTENT=TRUE \\\n  -e ANONYMIZED_TELEMETRY=FALSE \\\n  chromadb/chroma:latest\n</code></pre> <p>This exposes:</p> <ul> <li>Port 8000: ChromaDB HTTP API (for HoloDeck connections)</li> </ul> <p>Verify ChromaDB is running:</p> <pre><code>curl http://localhost:8000/api/v2/heartbeat\n# {\"nanosecond heartbeat\":1234567890}\n</code></pre>"},{"location":"guides/vector-stores/#docker-compose-recommended-for-projects","title":"Docker Compose (Recommended for Projects)","text":"<p>Create a <code>docker-compose.yml</code> file in your project root:</p> <pre><code>version: \"3.9\"\n\nservices:\n  chromadb:\n    image: chromadb/chroma:latest\n    container_name: holodeck-chromadb\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - chroma-data:/chroma/chroma\n    environment:\n      - IS_PERSISTENT=TRUE\n      - PERSIST_DIRECTORY=/chroma/chroma\n      - ANONYMIZED_TELEMETRY=FALSE\n    restart: unless-stopped\n\nvolumes:\n  chroma-data:\n</code></pre> <p>Start the service:</p> <pre><code>docker compose up -d\n</code></pre> <p>Stop the service:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"guides/vector-stores/#podman-equivalent","title":"Podman Equivalent","text":"<pre><code>podman run -d \\\n  --name chromadb \\\n  -p 8000:8000 \\\n  -v ./chroma-data:/chroma/chroma \\\n  -e IS_PERSISTENT=TRUE \\\n  -e ANONYMIZED_TELEMETRY=FALSE \\\n  docker.io/chromadb/chroma:latest\n</code></pre>"},{"location":"guides/vector-stores/#chromadb-environment-variables","title":"ChromaDB Environment Variables","text":"Variable Description Default <code>IS_PERSISTENT</code> Enable data persistence <code>FALSE</code> <code>PERSIST_DIRECTORY</code> Path for persistent storage <code>/chroma/chroma</code> <code>ANONYMIZED_TELEMETRY</code> Send anonymous usage data <code>TRUE</code> <p>Version Pinning: For stability, pin to a specific version (e.g., <code>chromadb/chroma:0.6.3</code>) instead of <code>latest</code> to avoid unexpected changes during upgrades.</p>"},{"location":"guides/vector-stores/#setting-up-postgresql-with-pgvector","title":"Setting Up PostgreSQL with pgvector","text":"<p>PostgreSQL with pgvector provides production-grade vector storage with full SQL capabilities. It's ideal if you already have PostgreSQL infrastructure.</p>"},{"location":"guides/vector-stores/#quick-start-with-docker_1","title":"Quick Start with Docker","text":"<p>Run PostgreSQL with pgvector:</p> <pre><code>docker run -d \\\n  --name postgres \\\n  -p 5432:5432 \\\n  -e POSTGRES_PASSWORD=your_password \\\n  -v pgdata:/var/lib/postgresql/data \\\n  pgvector/pgvector:pg17\n</code></pre> <p>This exposes:</p> <ul> <li>Port 5432: PostgreSQL database (for HoloDeck connections)</li> </ul> <p>Verify PostgreSQL is running:</p> <pre><code>docker exec -it postgres psql -U postgres -c \"SELECT 1;\"\n</code></pre>"},{"location":"guides/vector-stores/#docker-compose-recommended-for-projects_1","title":"Docker Compose (Recommended for Projects)","text":"<pre><code>version: \"3.9\"\n\nservices:\n  postgres:\n    image: pgvector/pgvector:pg17\n    container_name: holodeck-postgres\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n    environment:\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n      - POSTGRES_DB=holodeck\n    restart: unless-stopped\n\nvolumes:\n  pgdata:\n</code></pre>"},{"location":"guides/vector-stores/#postgresql-environment-variables","title":"PostgreSQL Environment Variables","text":"Variable Description Default <code>POSTGRES_PASSWORD</code> Database password (required) - <code>POSTGRES_USER</code> Database user <code>postgres</code> <code>POSTGRES_DB</code> Default database name <code>postgres</code> <p>Security: Always use environment variables for passwords. Never commit plaintext passwords to version control.</p>"},{"location":"guides/vector-stores/#setting-up-qdrant","title":"Setting Up Qdrant","text":"<p>Qdrant is a high-performance vector database designed for production workloads. It supports both HTTP and gRPC protocols.</p>"},{"location":"guides/vector-stores/#quick-start-with-docker_2","title":"Quick Start with Docker","text":"<p>Run Qdrant:</p> <pre><code>docker run -d \\\n  --name qdrant \\\n  -p 6333:6333 \\\n  -p 6334:6334 \\\n  -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n  qdrant/qdrant\n</code></pre> <p>This exposes:</p> <ul> <li>Port 6333: HTTP API (for HoloDeck connections)</li> <li>Port 6334: gRPC API (for high-performance connections)</li> </ul> <p>Verify Qdrant is running:</p> <pre><code>curl http://localhost:6333/healthz\n# {\"title\":\"qdrant - vector search engine\",\"version\":\"...\"}\n</code></pre>"},{"location":"guides/vector-stores/#docker-compose-recommended-for-projects_2","title":"Docker Compose (Recommended for Projects)","text":"<pre><code>version: \"3.9\"\n\nservices:\n  qdrant:\n    image: qdrant/qdrant:latest\n    container_name: holodeck-qdrant\n    ports:\n      - \"6333:6333\"\n      - \"6334:6334\"\n    volumes:\n      - qdrant-data:/qdrant/storage\n    restart: unless-stopped\n\nvolumes:\n  qdrant-data:\n</code></pre>"},{"location":"guides/vector-stores/#qdrant-with-api-key-production","title":"Qdrant with API Key (Production)","text":"<p>For production deployments, enable API key authentication:</p> <pre><code>services:\n  qdrant:\n    image: qdrant/qdrant:latest\n    ports:\n      - \"6333:6333\"\n    volumes:\n      - qdrant-data:/qdrant/storage\n    environment:\n      - QDRANT__SERVICE__API_KEY=${QDRANT_API_KEY}\n</code></pre> <p>Version Pinning: For stability, pin to a specific version (e.g., <code>qdrant/qdrant:v1.9.0</code>) instead of <code>latest</code>.</p>"},{"location":"guides/vector-stores/#setting-up-pinecone","title":"Setting Up Pinecone","text":"<p>Pinecone is a fully managed, serverless vector database. For production, use the cloud service with an API key. For local development and testing, use Pinecone Local.</p>"},{"location":"guides/vector-stores/#cloud-setup-production","title":"Cloud Setup (Production)","text":"<ol> <li>Create an account at pinecone.io</li> <li>Create an index in the Pinecone console</li> <li>Copy your API key from the console</li> </ol> <p>Configure in HoloDeck:</p> <pre><code>database:\n  provider: pinecone\n  api_key: ${PINECONE_API_KEY}\n  namespace: my-namespace  # optional\n</code></pre>"},{"location":"guides/vector-stores/#local-development-with-docker","title":"Local Development with Docker","text":"<p>For local development without a Pinecone account, use Pinecone Local:</p> <p>Pull the image:</p> <pre><code>docker pull ghcr.io/pinecone-io/pinecone-index:latest\n</code></pre> <p>Run Pinecone Local:</p> <pre><code>docker run -d \\\n  --name pinecone-local \\\n  -e PORT=5081 \\\n  -e INDEX_TYPE=serverless \\\n  -e DIMENSION=1536 \\\n  -e METRIC=cosine \\\n  -p 5081:5081 \\\n  --platform linux/amd64 \\\n  ghcr.io/pinecone-io/pinecone-index:latest\n</code></pre> <p>This exposes:</p> <ul> <li>Port 5081: Pinecone Local API</li> </ul>"},{"location":"guides/vector-stores/#pinecone-local-environment-variables","title":"Pinecone Local Environment Variables","text":"Variable Description Example <code>PORT</code> API port <code>5081</code> <code>INDEX_TYPE</code> Index type <code>serverless</code> <code>DIMENSION</code> Vector dimensions <code>1536</code> <code>METRIC</code> Distance metric <code>cosine</code>, <code>euclidean</code>, <code>dotproduct</code> <p>Note: Pinecone Local is for development and testing only. Use the cloud service for production workloads.</p>"},{"location":"guides/vector-stores/#configuring-vector-stores-in-holodeck","title":"Configuring Vector Stores in HoloDeck","text":""},{"location":"guides/vector-stores/#global-configuration-configyaml","title":"Global Configuration (config.yaml)","text":"<p>Define reusable vector store connections in your global configuration:</p> <pre><code># config.yaml (project root or ~/.holodeck/config.yaml)\n\nvectorstores:\n  # ChromaDB (lightweight, Python-native)\n  my-chroma-store:\n    provider: chromadb\n    connection_string: http://localhost:8000\n\n  # PostgreSQL with pgvector\n  my-postgres-store:\n    provider: postgres\n    connection_string: ${DATABASE_URL}\n\n  # Qdrant (high-performance)\n  my-qdrant-store:\n    provider: qdrant\n    url: http://localhost:6333\n</code></pre>"},{"location":"guides/vector-stores/#environment-variables","title":"Environment Variables","text":"<p>Store sensitive connection details in environment variables:</p> <pre><code># .env file (DO NOT commit to version control)\nDATABASE_URL=postgresql://user:password@localhost:5432/mydb\nQDRANT_API_KEY=your-api-key\n</code></pre>"},{"location":"guides/vector-stores/#agent-configuration-agentyaml","title":"Agent Configuration (agent.yaml)","text":"<p>Reference the global vector store in your agent's tools:</p> <pre><code># agent.yaml\nname: knowledge-agent\ndescription: Agent with semantic search capabilities\n\nmodel:\n  provider: openai\n  name: gpt-4o\n\ninstructions:\n  inline: |\n    You are a helpful assistant with access to a knowledge base.\n    Use the search tool to find relevant information before answering.\n\ntools:\n  - name: search-kb\n    description: Search the knowledge base for relevant information\n    type: vectorstore\n    source: knowledge_base/\n    database: my-chroma-store  # Reference to config.yaml\n</code></pre>"},{"location":"guides/vector-stores/#inline-database-configuration","title":"Inline Database Configuration","text":"<p>Alternatively, configure the database directly in <code>agent.yaml</code>:</p> <pre><code>tools:\n  - name: search-docs\n    description: Search technical documentation\n    type: vectorstore\n    source: docs/\n    database:\n      provider: chromadb\n      connection_string: http://localhost:8000\n</code></pre>"},{"location":"guides/vector-stores/#connection-string-formats","title":"Connection String Formats","text":""},{"location":"guides/vector-stores/#postgresql-connection-strings","title":"PostgreSQL Connection Strings","text":"<p>PostgreSQL uses standard connection string format:</p> Format Example Basic <code>postgresql://localhost:5432/mydb</code> With credentials <code>postgresql://user:password@localhost:5432/mydb</code> With SSL <code>postgresql://user:password@host:5432/mydb?sslmode=require</code> <pre><code>database:\n  provider: postgres\n  connection_string: postgresql://user:password@localhost:5432/mydb\n</code></pre>"},{"location":"guides/vector-stores/#chromadb-connection-strings","title":"ChromaDB Connection Strings","text":"<p>ChromaDB accepts HTTP/HTTPS URLs or local paths:</p> Format Example HTTP <code>http://localhost:8000</code> HTTPS <code>https://chroma.example.com</code> Local persistent (use <code>persist_directory</code> instead) <pre><code># Remote server\ndatabase:\n  provider: chromadb\n  connection_string: http://localhost:8000\n\n# Local persistent storage\ndatabase:\n  provider: chromadb\n  persist_directory: ./data/chromadb\n</code></pre>"},{"location":"guides/vector-stores/#qdrant-connection-strings","title":"Qdrant Connection Strings","text":"<p>Qdrant supports multiple connection formats:</p> Format Example HTTP (local) <code>http://localhost:6333</code> HTTPS (remote) <code>https://qdrant.example.com:6333</code> gRPC (high-performance) <code>qdrant+grpc://localhost:6334</code> In-memory <code>:memory:</code> Local path <code>/path/to/qdrant/data</code> <pre><code># HTTP connection\ndatabase:\n  provider: qdrant\n  connection_string: http://localhost:6333\n\n# With API key (recommended for remote)\ndatabase:\n  provider: qdrant\n  connection_string: https://qdrant.example.com:6333\n  api_key: ${QDRANT_API_KEY}\n\n# gRPC for high-performance\ndatabase:\n  provider: qdrant\n  connection_string: qdrant+grpc://localhost:6334\n</code></pre>"},{"location":"guides/vector-stores/#pinecone-connection","title":"Pinecone Connection","text":"<p>Pinecone uses API key authentication (no connection string):</p> <pre><code>database:\n  provider: pinecone\n  api_key: ${PINECONE_API_KEY}\n  namespace: my-namespace  # optional\n</code></pre> <p>Or use the connection string format:</p> <pre><code>database:\n  provider: pinecone\n  connection_string: pinecone://pc-abc123@my-namespace\n</code></pre> <p>Security: Always use environment variables (<code>${VAR_NAME}</code>) for passwords, API keys, and sensitive connection details. Never commit plaintext credentials to version control.</p>"},{"location":"guides/vector-stores/#complete-example","title":"Complete Example","text":""},{"location":"guides/vector-stores/#project-structure","title":"Project Structure","text":"<pre><code>my-agent-project/\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 agent.yaml\n\u251c\u2500\u2500 .env\n\u251c\u2500\u2500 knowledge_base/\n\u2502   \u251c\u2500\u2500 faq.json\n\u2502   \u2514\u2500\u2500 docs.md\n\u2514\u2500\u2500 docker-compose.yml\n</code></pre>"},{"location":"guides/vector-stores/#configyaml","title":"config.yaml","text":"<pre><code>providers:\n  openai:\n    provider: openai\n    name: gpt-4o\n    api_key: ${OPENAI_API_KEY}\n\nvectorstores:\n  knowledge-store:\n    provider: chromadb\n    connection_string: http://localhost:8000\n\nexecution:\n  cache_enabled: true\n  verbose: false\n</code></pre>"},{"location":"guides/vector-stores/#agentyaml","title":"agent.yaml","text":"<pre><code>name: support-agent\ndescription: Customer support agent with knowledge base search\n\nmodel:\n  provider: openai\n  name: gpt-4o\n  temperature: 0.7\n\ninstructions:\n  inline: |\n    You are a customer support specialist.\n    Always search the knowledge base before answering questions.\n    Provide accurate, helpful responses based on the documentation.\n\ntools:\n  - name: search-kb\n    description: Search knowledge base for answers to customer questions\n    type: vectorstore\n    source: knowledge_base/\n    database: knowledge-store\n    embedding_model: text-embedding-3-small\n    chunk_size: 512\n    chunk_overlap: 50\n\ntest_cases:\n  - name: \"FAQ lookup\"\n    input: \"How do I reset my password?\"\n    expected_tools: [search-kb]\n</code></pre>"},{"location":"guides/vector-stores/#env","title":".env","text":"<pre><code>OPENAI_API_KEY=sk-...\n</code></pre>"},{"location":"guides/vector-stores/#docker-composeyml","title":"docker-compose.yml","text":"<pre><code>version: \"3.9\"\n\nservices:\n  chromadb:\n    image: chromadb/chroma:latest\n    container_name: holodeck-chromadb\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - chroma-data:/chroma/chroma\n    environment:\n      - IS_PERSISTENT=TRUE\n      - ANONYMIZED_TELEMETRY=FALSE\n\nvolumes:\n  chroma-data:\n</code></pre>"},{"location":"guides/vector-stores/#running-the-agent","title":"Running the Agent","text":"<pre><code># 1. Start ChromaDB\ndocker compose up -d\n\n# 2. Verify ChromaDB is running\ncurl http://localhost:8000/api/v2/heartbeat\n\n# 3. Run the agent\nholodeck test agent.yaml\n</code></pre>"},{"location":"guides/vector-stores/#supported-vector-store-providers","title":"Supported Vector Store Providers","text":"<p>HoloDeck supports multiple vector database backends. See the Tools Guide for the complete list.</p> Provider Best For Setup Complexity <code>chromadb</code> Lightweight development, Python-native Low <code>postgres</code> Existing PostgreSQL infrastructure Medium <code>qdrant</code> High-performance production Medium <code>pinecone</code> Serverless, managed cloud Low <code>in-memory</code> Testing and prototyping None"},{"location":"guides/vector-stores/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/vector-stores/#cannot-connect-to-chromadb","title":"Cannot connect to ChromaDB","text":"<p>Error: <code>Connection refused</code> or <code>Cannot connect to http://localhost:8000</code></p> <p>Solutions:</p> <ol> <li> <p>Verify ChromaDB is running:    <pre><code>docker ps | grep chromadb\n</code></pre></p> </li> <li> <p>Check the container logs:    <pre><code>docker logs holodeck-chromadb\n</code></pre></p> </li> <li> <p>Test connectivity:    <pre><code>curl http://localhost:8000/api/v2/heartbeat\n</code></pre></p> </li> <li> <p>Ensure port 8000 is not blocked by firewall</p> </li> </ol>"},{"location":"guides/vector-stores/#data-not-persisting","title":"Data not persisting","text":"<p>Solution: Mount a volume for data persistence:</p> <pre><code>docker run -d \\\n  --name chromadb \\\n  -p 8000:8000 \\\n  -v chroma-data:/chroma/chroma \\\n  -e IS_PERSISTENT=TRUE \\\n  chromadb/chroma:latest\n</code></pre>"},{"location":"guides/vector-stores/#container-already-exists","title":"Container already exists","text":"<p>Error: <code>container name \"chromadb\" is already in use</code></p> <p>Solution: Remove the existing container:</p> <pre><code>docker rm -f chromadb\n</code></pre>"},{"location":"guides/vector-stores/#additional-resources","title":"Additional Resources","text":"<ul> <li>ChromaDB on Docker Hub</li> <li>ChromaDB Documentation</li> <li>Qdrant Documentation</li> <li>PostgreSQL pgvector</li> <li>Tools Reference Guide - Complete vectorstore tool configuration</li> <li>Global Configuration Guide - Shared settings across agents</li> </ul>"},{"location":"guides/vector-stores/#next-steps","title":"Next Steps","text":"<ul> <li>See Tools Reference for vectorstore tool options (chunk size, embedding models, etc.)</li> <li>See Agent Configuration for complete agent setup</li> <li>See LLM Providers Guide for configuring the LLM that powers your agent</li> <li>See Evaluations Guide for testing your agent's search quality</li> <li>See Global Configuration for sharing vectorstore configs across agents</li> </ul>"}]}